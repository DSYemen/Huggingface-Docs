## تحويل النص إلى كلام 

تحويل النص إلى كلام (TTS) هي مهمة إنشاء كلام طبيعي من نص مكتوب، حيث يمكن توليد الكلام بعدة لغات ولعدة متحدثين. هناك العديد من نماذج تحويل النص إلى كلام المتاحة حاليًا في مكتبة 🤗 Transformers، مثل Bark وMMS وVITS وSpeechT5.

يمكنك بسهولة إنشاء صوت باستخدام خط الأنابيب "text-to-audio" (أو "text-to-speech"). يمكن لبعض النماذج، مثل Bark، أيضًا أن تكون مشروطة لتوليد اتصالات غير لفظية مثل الضحك والتنهد والبكاء، أو حتى إضافة الموسيقى.

فيما يلي مثال على كيفية استخدام خط أنابيب "text-to-speech" مع Bark:

```py
>>> from transformers import pipeline

>>> pipe = pipeline("text-to-speech", model="suno/bark-small")
>>> text = "[clears throat] This is a test ... and I just took a long pause."
>>> output = pipe(text)
```

هذه مقتطفات من التعليمات البرمجية التي يمكنك استخدامها للاستماع إلى الصوت الناتج في دفتر الملاحظات:

```python
>>> from IPython.display import Audio
>>> Audio(output["audio"], rate=output["sampling_rate"])
```

للحصول على المزيد من الأمثلة على ما يمكن أن تفعله Bark ونماذج TTS الأخرى المُدربة مسبقًا، راجع [دورة الصوت](https://huggingface.co/learn/audio-course/chapter6/pre-trained_models).

إذا كنت ترغب في ضبط نموذج TTS، فإن النموذجين الوحيدين لتحويل النص إلى كلام المتاحين حاليًا في مكتبة 🤗 Transformers هما SpeechT5 وFastSpeech2Conformer، على الرغم من أنه سيتم إضافة المزيد في المستقبل. تم تدريب SpeechT5 مسبقًا على مزيج من بيانات تحويل الكلام إلى نص وتحويل النص إلى كلام، مما يسمح له بتعلم مساحة موحدة من التمثيلات المخفية المشتركة بين النص والكلام. وهذا يعني أنه يمكن ضبط نفس النموذج المُدرب مسبقًا لمهام مختلفة. علاوة على ذلك، يدعم SpeechT5 متحدثين متعددين من خلال تضمين المتكلم x.

يوضح باقي هذا الدليل كيفية:

1. ضبط نموذج SpeechT5 الذي تم تدريبه في الأصل على الكلام باللغة الإنجليزية على الجزء الفرعي باللغة الهولندية (`nl`) من مجموعة بيانات VoxPopuli.
2. استخدام النموذج المحسن للاستنتاج بطريقتين: باستخدام خط الأنابيب أو مباشرة.

قبل البدء، تأكد من تثبيت جميع المكتبات الضرورية:

```bash
pip install datasets soundfile speechbrain accelerate
```

قم بتثبيت 🤗 Transformers من المصدر حيث لم يتم دمج جميع ميزات SpeechT5 في إصدار رسمي بعد:

```bash
pip install git+https://github.com/huggingface/transformers.git
```

<Tip>

لإكمال هذا الدليل، ستحتاج إلى وحدة معالجة رسومات (GPU). إذا كنت تعمل في دفتر ملاحظات، فشغّل السطر التالي للتحقق من توفر وحدة GPU:

```bash
!nvidia-smi
```

أو بديل لوحدات معالجة الرسومات AMD:

```bash
!rocm-smi
```

</Tip>

نحن نشجعك على تسجيل الدخول إلى حساب Hugging Face الخاص بك لتحميل ومشاركة نموذجك مع المجتمع. عندما يُطلب منك ذلك، أدخل رمزك للتسجيل:

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

## تحميل مجموعة البيانات

VoxPopuli عبارة عن مجموعة بيانات صوتية متعددة اللغات واسعة النطاق تتكون من بيانات مستخرجة من تسجيلات أحداث البرلمان الأوروبي من عام 2009 إلى عام 2020. يحتوي على بيانات صوتية-نصية مُوسمة لـ 15 لغة أوروبية. في هذا الدليل، نستخدم الجزء الفرعي باللغة الهولندية، ولكن يمكنك اختيار جزء فرعي آخر.

لاحظ أن VoxPopuli أو أي مجموعة بيانات أخرى للتعرف التلقائي على الكلام (ASR) قد لا تكون الخيار الأنسب لتدريب نماذج TTS. الميزات التي تجعلها مفيدة لـ ASR، مثل الضوضاء الخلفية المفرطة، غير مرغوب فيها عادةً في TTS. ومع ذلك، يمكن أن يكون العثور على مجموعات بيانات TTS عالية الجودة ومتعددة اللغات ومتعددة المتحدثين أمرًا صعبًا للغاية.

دعنا نحمل البيانات:

```py
>>> from datasets import load_dataset, Audio

>>> dataset = load_dataset("facebook/voxpopuli", "nl", split="train")
>>> len(dataset)
20968
```

20968 مثال يجب أن يكون كافيًا للضبط الدقيق. يتوقع SpeechT5 أن يكون لمجموعة البيانات معدل أخذ عينات يبلغ 16 كيلو هرتز، لذا تأكد من أن الأمثلة في مجموعة البيانات تلبي هذا الشرط:

```py
dataset = dataset.cast_column("audio", Audio(sampling_rate=16000))
```

## معالجة البيانات مسبقًا

دعنا نبدأ بتحديد نقطة تفتيش النموذج لتحميلها وتحميل المعالج المناسب:

```py
>>> from transformers import SpeechT5Processor

>>> checkpoint = "microsoft/speecht5_tts"
>>> processor = SpeechT5Processor.from_pretrained(checkpoint)
```

### تنظيف النص لتوكيده SpeechT5

ابدأ بتنظيف بيانات النص. ستحتاج إلى الجزء المعالج من المعالج لمعالجة النص:

```py
>>> tokenizer = processor.tokenizer
```

تحتوي أمثلة مجموعة البيانات على ميزات `raw_text` و`normalized_text`. عند اتخاذ قرار بشأن الميزة التي سيتم استخدامها كإدخال نصي، ضع في اعتبارك أن معالج SpeechT5 لا يحتوي على أي رموز للأرقام. في `normalized_text`، يتم كتابة الأرقام كنص. وبالتالي، فهو مناسب بشكل أفضل، ونوصي باستخدام `normalized_text` كإدخال نصي.

نظرًا لأن SpeechT5 تم تدريبه على اللغة الإنجليزية، فقد لا يتعرف على أحرف معينة في مجموعة البيانات الهولندية. إذا تُركت كما هي، فسيتم تحويل هذه الأحرف إلى رموز `<unk>`. ومع ذلك، في اللغة الهولندية، تُستخدم أحرف معينة مثل `à` للتشديد على المقاطع. للحفاظ على معنى النص، يمكننا استبدال هذا الحرف بـ "a" عادي.

لتحديد الرموز غير المدعومة، استخرج جميع الأحرف الفريدة في مجموعة البيانات باستخدام `SpeechT5Tokenizer` الذي يعمل مع الأحرف كرموز. للقيام بذلك، اكتب دالة `extract_all_chars` التي تقوم بتوصيل النسخ النصية من جميع الأمثلة في سلسلة واحدة وتحويلها إلى مجموعة من الأحرف.

تأكد من تعيين `batched=True` و`batch_size=-1` في `dataset.map()` بحيث تكون جميع النسخ النصية متاحة مرة واحدة لدالة الخريطة.

```py
>>> def extract_all_chars(batch):
...     all_text = " ".join(batch["normalized_text"])
...     vocab = list(set(all_text))
...     return {"vocab": [vocab], "all_text": [all_text]}


>>> vocabs = dataset.map(
...     extract_all_chars,
...     batched=True,
...     batch_size=-1,
...     keep_in_memory=True,
...     remove_columns=dataset.column_names,
... )

>>> dataset_vocab = set(vocabs["vocab"][0])
>>> tokenizer_vocab = {k for k, _ in tokenizer.get_vocab().items()}
```

الآن لديك مجموعتان من الأحرف: واحدة بمفردات من مجموعة البيانات والأخرى بمفردات من المعالج. لتحديد أي أحرف غير مدعومة في مجموعة البيانات، يمكنك أخذ الفرق بين هاتين المجموعتين. ستتكون المجموعة الناتجة من الأحرف الموجودة في مجموعة البيانات ولكن ليس في المعالج.

```py
>>> dataset_vocab - tokenizer_vocab
{' ', 'à', 'ç', 'è', 'ë', 'í', 'ï', 'ö', 'ü'}
```

للتعامل مع الأحرف غير المدعومة التي تم تحديدها في الخطوة السابقة، حدد دالة تقوم بتعيين هذه الأحرف إلى رموز صالحة. لاحظ أن المسافات يتم استبدالها بالفعل بـ `▁` في المعالج ولا تحتاج إلى التعامل معها بشكل منفصل.

```py
>>> replacements = [
...     ("à", "a"),
...     ("ç", "c"),
...     ("è", "e"),
...     ("ë", "e"),
...     ("í", "i"),
...     ("ï", "i"),
...     ("ö", "o"),
...     ("ü", "u"),
... ]


>>> def cleanup_text(inputs):
...     for src, dst in replacements:
...         inputs["normalized_text"] = inputs["normalized_text"].replace(src, dst)
...     return inputs


>>> dataset = dataset.map(cleanup_text)
```

الآن بعد أن تعاملت مع الأحرف الخاصة في النص، حان الوقت للتركيز على بيانات الصوت.
### المتحدثون

تتضمن مجموعة بيانات VoxPopuli خطابات من متحدثين متعددين، ولكن كم عدد المتحدثين الممثلين في مجموعة البيانات؟ لتحديد ذلك، يمكننا حساب عدد المتحدثين المميزين وعدد الأمثلة التي يساهم بها كل متحدث في مجموعة البيانات. بمعرفة أن إجمالي عدد الأمثلة في مجموعة البيانات يبلغ 20,968، ستمنحنا هذه المعلومات فهمًا أفضل لتوزيع المتحدثين والأمثلة في البيانات.

عن طريق رسم مخطط توزيعي، يمكنك معرفة مقدار البيانات المتوفرة لكل متحدث.

يكشف المخطط التوزيعي أن حوالي ثلث المتحدثين في مجموعة البيانات لديهم أقل من 100 مثال، في حين أن حوالي عشرة متحدثين لديهم أكثر من 500 مثال. لتحسين كفاءة التدريب وتحقيق التوازن في مجموعة البيانات، يمكننا تقييد البيانات بالمتحدثين الذين لديهم ما بين 100 و400 مثال.

دعونا نتحقق من عدد المتحدثين المتبقين:

لنرى كم عدد الأمثلة المتبقية:

تبقى لديك أقل بقليل من 10,000 مثال من حوالي 40 متحدثًا مميزًا، وهو ما يجب أن يكون كافيًا. لاحظ أنه قد يكون لدى بعض المتحدثين الذين لديهم عدد قليل من الأمثلة في الواقع المزيد من الصوت المتاح إذا كانت الأمثلة طويلة. ومع ذلك، فإن تحديد إجمالي كمية الصوت لكل متحدث يتطلب فحص مجموعة البيانات بأكملها، وهي عملية تستغرق وقتًا طويلاً وتنطوي على تحميل وفك تشفير كل ملف صوتي. لذلك، قررنا تخطي هذه الخطوة هنا.

### تضمين المتحدث

لتمكين نموذج TTS من التمييز بين متحدثين متعددين، ستحتاج إلى إنشاء تضمين متحدث لكل مثال. إن تضمين المتحدث هو إدخال إضافي في النموذج الذي يلتقط خصائص صوت متحدث معين.

لإنشاء هذه التضمينات للمتحدثين، استخدم نموذج [spkrec-xvect-voxceleb](https://huggingface.co/speechbrain/spkrec-xvect-voxceleb) المُدرب مسبقًا من SpeechBrain.

أنشئ دالة `create_speaker_embedding()` التي تأخذ موجة صوت إدخال وتخرج متجهًا مكونًا من 512 عنصرًا يحتوي على تضمين المتحدث المقابل.

من المهم ملاحظة أن نموذج `speechbrain/spkrec-xvect-voxceleb` تم تدريبه على الكلام باللغة الإنجليزية من مجموعة بيانات VoxCeleb، في حين أن أمثلة التدريب في هذا الدليل باللغة الهولندية. في حين أننا نعتقد أن هذا النموذج سيظل يولد تضمينات متحدث معقولة لمجموعة البيانات الهولندية الخاصة بنا، فقد لا يكون هذا الافتراض صحيحًا في جميع الحالات.

للحصول على نتائج مثالية، نوصي بتدريب نموذج X-vector على كلام الهدف أولاً. سيضمن ذلك أن النموذج قادر بشكل أفضل على التقاط خصائص الصوت الفريدة الموجودة في اللغة الهولندية.

### معالجة مجموعة البيانات

أخيرًا، دعونا نقوم بمعالجة البيانات إلى التنسيق الذي يتوقعه النموذج. قم بإنشاء دالة `prepare_dataset` تأخذ مثالًا واحدًا وتستخدم كائن `SpeechT5Processor` لتحويل نص الإدخال إلى رموز وتحميل الصوت المستهدف إلى مخطط Mel-spectrogram. يجب أن تضيف أيضًا تضمينات المتحدثين كإدخال إضافي.

تحقق من صحة المعالجة عن طريق النظر في مثال واحد:

ينبغي أن تكون تضمينات المتحدثين عبارة عن متجه مكون من 512 عنصرًا:

ينبغي أن تكون التسميات مخطط Mel-spectrogram مع 80 نافذة Mel.

ملاحظة جانبية: إذا وجدت أن هذا المخطط الطيفي مربكًا، فقد يكون ذلك بسبب معرفتك باتفاقية وضع الترددات المنخفضة في الأسفل والترددات العالية في الجزء العلوي من المخطط. ومع ذلك، عند رسم المخططات الطيفية كصورة باستخدام مكتبة matplotlib، يتم قلب المحور y ويظهر المخطط الطيفي مقلوبًا.

الآن قم بتطبيق دالة المعالجة على مجموعة البيانات بأكملها. سيستغرق ذلك ما بين 5 و10 دقائق.

ستشاهد تحذيرًا يفيد بأن بعض الأمثلة في مجموعة البيانات أطول من طول الإدخال الأقصى الذي يمكن أن يتعامل معه النموذج (600 رمزًا). احذف تلك الأمثلة من مجموعة البيانات. هنا نذهب إلى أبعد من ذلك وحتى نسمح بأحجام دفعات أكبر، نقوم بإزالة أي شيء يزيد عن 200 رمزًا.

بعد ذلك، قم بإنشاء تقسيم أساسي للتدريب/الاختبار:

### جامع البيانات

من أجل دمج أمثلة متعددة في دفعة، يلزمك تحديد جامع بيانات مخصص. سيقوم جامع البيانات هذا بملء التسلسلات الأقصر برموز الملء، مما يضمن أن يكون لجميع الأمثلة نفس الطول. بالنسبة إلى تسميات المخطط الطيفي، يتم استبدال الأجزاء المملوءة بالقيمة الخاصة "-100". تعلم هذه القيمة الخاصة النموذج بتجاهل هذا الجزء من المخطط الطيفي عند حساب فقدان المخطط الطيفي.

في SpeechT5، يتم تقليل إدخال الجزء فك الترميز من النموذج بعامل 2. وبعبارة أخرى، فإنه يتخلص من كل خطوة زمنية أخرى من التسلسل المستهدف. بعد ذلك، يتنبأ فك الترميز بتسلسل يبلغ ضعف طوله. نظرًا لأن طول التسلسل المستهدف الأصلي قد يكون فرديًا، فإن جامع البيانات يتأكد من تقريب الطول الأقصى للدفعة لأسفل ليكون مضاعفًا للعدد 2.

قم بإنشاء جامع بيانات:
## تدريب النموذج

قم بتحميل النموذج المُدرب مسبقًا من نفس نقطة التفتيش التي استخدمتها لتحميل المعالج:

قم بتعطيل الخيار `use_cache=True` فهو غير متوافق مع التدرج الحاسوبي. قم بتعطيله أثناء التدريب.

قم بتعريف حجج التدريب. هنا، لن نقوم بحساب أي مقاييس تقييم أثناء عملية التدريب. بدلاً من ذلك، سننظر فقط إلى الخسارة:

```python
>>> from transformers import Seq2SeqTrainingArguments

>>> training_args = Seq2SeqTrainingArguments(
...     output_dir="speecht5_finetuned_voxpopuli_nl"،  # قم بتغييره إلى اسم مستودع من اختيارك
...     per_device_train_batch_size=4،
...     gradient_accumulation_steps=8،
...     learning_rate=1e-5،
...     warmup_steps=500،
...     max_steps=4000،
...     gradient_checkpointing=True،
...     fp16=True،
...     eval_strategy="steps"،
...     per_device_eval_batch_size=2،
...     save_steps=1000،
...     eval_steps=1000،
...     logging_steps=25،
...     report_to=["tensorboard"]،
...     load_best_model_at_end=True،
...     greater_is_better=False،
...     label_names=["labels"]،
...     push_to_hub=True،
... )
```

قم بتنفيذ كائن `Trainer` ومرر إليه النموذج ومجموعة البيانات ومجمع البيانات:

```py
>>> from transformers import Seq2SeqTrainer

>>> trainer = Seq2SeqTrainer(
...     args=training_args،
...     model=model،
...     train_dataset=dataset["train"]،
...     eval_dataset=dataset["test"]،
...     data_collator=data_collator،
...     tokenizer=processor،
... )
```

والآن، أنت مستعد لبدء التدريب! سيستغرق التدريب عدة ساعات. واعتمادًا على وحدة معالجة الرسوميات (GPU) لديك، فقد تواجه خطأ "نفاد الذاكرة" CUDA عند بدء التدريب. في هذه الحالة، يمكنك تقليل `per_device_train_batch_size` بشكل تدريجي عن طريق عوامل 2 وزيادة `gradient_accumulation_steps` بمقدار 2x للتعويض.

```py
>>> trainer.train()
```

لاستخدام نقطة التفتيش الخاصة بك مع خط أنابيب، تأكد من حفظ المعالج مع نقطة التفتيش:

```py
>>> processor.save_pretrained("YOUR_ACCOUNT_NAME/speecht5_finetuned_voxpopuli_nl")
```

أرسل النموذج النهائي إلى 🤗 Hub:

```py
>>> trainer.push_to_hub()
```

## الاستنتاج

### الاستنتاج باستخدام خط أنابيب

الآن بعد أن قمت بضبط نموذجك، يمكنك استخدامه للاستنتاج!

أولاً، دعونا نرى كيف يمكنك استخدامه مع خط أنابيب مطابق. دعونا نقوم بإنشاء خط أنابيب `"text-to-speech"` مع نقطة التفتيش الخاصة بك:

```py
>>> from transformers import pipeline

>>> pipe = pipeline("text-to-speech"، model="YOUR_ACCOUNT_NAME/speecht5_finetuned_voxpopuli_nl")
```

اختر قطعة نص باللغة الهولندية التي تريد سردها، على سبيل المثال:

```py
>>> text = "hallo allemaal, ik praat nederlands. groetjes aan iedereen!"
```

لاستخدام SpeechT5 مع خط الأنابيب، ستحتاج إلى تضمين صوت المتحدث. دعونا نحصل عليه من مثال في مجموعة الاختبار:

```py
>>> example = dataset["test"][304]
>>> speaker_embeddings = torch.tensor(example["speaker_embeddings"]).unsqueeze(0)
```

الآن يمكنك تمرير النص وتضمين المتحدث إلى خط الأنابيب، وسيتولى بقية المهمة:

```py
>>> forward_params = {"speaker_embeddings": speaker_embeddings}
>>> output = pipe(text, forward_params=forward_params)
>>> output
{'audio': array([-6.82714235e-05, -4.26525949e-04, 1.06134125e-04, ...,
-1.22392643e-03, -7.76011671e-04, 3.29112721e-04], dtype=float32)،
'sampling_rate': 16000}
```

يمكنك بعد ذلك الاستماع إلى النتيجة:

```py
>>> from IPython.display import Audio
>>> Audio(output['audio'], rate=output['sampling_rate'])
```

### تشغيل الاستنتاج يدويًا

يمكنك تحقيق نفس نتائج الاستنتاج بدون استخدام خط الأنابيب، ولكن ستكون هناك حاجة إلى مزيد من الخطوات.

قم بتحميل النموذج من 🤗 Hub:

```py
>>> model = SpeechT5ForTextToSpeech.from_pretrained("YOUR_ACCOUNT/speecht5_finetuned_voxpopuli_nl")
```

اختر مثالاً من مجموعة الاختبار للحصول على تضمين صوت المتحدث.

```py
>>> example = dataset["test"][304]
>>> speaker_embeddings = torch.tensor(example["speaker_embeddings"]).unsqueeze(0)
```

قم بتعريف نص الإدخال ومعالجته:

```py
>>> text = "hallo allemaal, ik praat nederlands. groetjes aan iedereen!"
>>> inputs = processor(text=text, return_tensors="pt")
```

قم بإنشاء مخطط طيفي مع نموذجك:

```py
>>> spectrogram = model.generate_speech(inputs["input_ids"], speaker_embeddings)
```

قم بتصور المخطط الطيفي، إذا أردت:

```py
>>> plt.figure()
>>> plt.imshow(spectrogram.T)
>>> plt.show()
```

أخيرًا، استخدم المحول التناظري الرقمي لتحويل المخطط الطيفي إلى صوت:

```py
>>> with torch.no_grad():
...     speech = vocoder(spectrogram)

>>> from IPython.display import Audio

>>> Audio(speech.numpy(), rate=16000)
```

في تجربتنا، قد يكون من الصعب الحصول على نتائج مرضية من هذا النموذج. تبدو جودة تضمين المتحدث عاملًا مهمًا. نظرًا لأن SpeechT5 تم تدريبه مسبقًا باستخدام x-vectors باللغة الإنجليزية، فإنه يعمل بشكل أفضل عند استخدام تضمين صوت المتحدث باللغة الإنجليزية. إذا كان الكلام المُخلَّق يبدو رديئًا، فجرّب استخدام تضمين صوت متحدث مختلف.

ومن المرجح أن يؤدي زيادة مدة التدريب إلى تحسين جودة النتائج. ومع ذلك، فمن الواضح أن الكلام باللغة الهولندية بدلاً من الإنجليزية، ويتم التقاط خصائص صوت المتحدث (مقارنة بالصوت الأصلي في المثال).

شيء آخر يمكن تجربته هو تكوين النموذج. على سبيل المثال، جرب استخدام `config.reduction_factor = 1` لمعرفة ما إذا كان هذا يحسن النتائج.

أخيرًا، من الضروري مراعاة الاعتبارات الأخلاقية. على الرغم من أن تقنية TTS لديها العديد من التطبيقات المفيدة، إلا أنه يمكن أيضًا استخدامها لأغراض خبيثة، مثل انتحال صوت شخص ما دون معرفته أو موافقته. يرجى استخدام TTS بحكمة وبشكل مسؤول.