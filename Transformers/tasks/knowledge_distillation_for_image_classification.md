# استخلاص المعرفة للرؤية الحاسوبية

تعد تقنية استخلاص المعرفة طريقة لنقل المعرفة من نموذج أكبر وأكثر تعقيدًا (المعلم) إلى نموذج أصغر وأبسط (الطالب). ولاستخلاص المعرفة من نموذج إلى آخر، نستخدم نموذج معلم مُدرب مسبقًا على مهمة معينة (تصنيف الصور في هذه الحالة) ونقوم بتطبيق عملية التهيئة العشوائية على نموذج الطالب ليتم تدريبه على تصنيف الصور. بعد ذلك، نقوم بتدريب نموذج الطالب لتقليل الاختلاف بين مخرجاته ومخرجات المعلم، مما يجعله يحاكي سلوك المعلم. وقد تم تقديم هذه التقنية لأول مرة في ورقة "استخلاص المعرفة من الشبكة العصبية" من قبل هينتون وآخرون. في هذا الدليل، سنقوم باستخلاص المعرفة الخاصة بمهمة معينة. وسنستخدم مجموعة بيانات beans لهذا الغرض.

يوضح هذا الدليل كيفية استخلاص نموذج ViT (نموذج المعلم) الذي تم ضبطه بدقة إلى MobileNet (نموذج الطالب) باستخدام واجهة برمجة تطبيقات Trainer من مكتبة Transformers من Hugging Face.

دعونا نقوم بتثبيت المكتبات اللازمة لعملية الاستخلاص وتقييم العملية.

في هذا المثال، نستخدم نموذج "merve/beans-vit-224" كنموذج معلم. وهو نموذج لتصنيف الصور، يعتمد على النموذج "google/vit-base-patch16-224-in21k" الذي تم ضبطه بدقة على مجموعة بيانات beans. سنقوم باستخلاص هذا النموذج إلى MobileNetV2 الذي تم تهيئته بشكل عشوائي.

سنقوم الآن بتحميل مجموعة البيانات.

يمكننا استخدام معالج الصور من أي من النماذج، حيث أنه في هذه الحالة يعيد نفس المخرجات بنفس الدقة. سنستخدم طريقة map() من dataset لتطبيق المعالجة المسبقة على كل جزء من مجموعة البيانات.

في الأساس، نريد من نموذج الطالب (MobileNet الذي تم تهيئته بشكل عشوائي) أن يحاكي نموذج المعلم (نموذج الرؤية المحول الذي تم ضبطه بدقة). ولتحقيق ذلك، نقوم أولاً بالحصول على مخرجات logits من المعلم والطالب. بعد ذلك، نقوم بقسمة كل منهما على معامل "temperature" الذي يتحكم في أهمية كل هدف ناعم. وهناك معامل يسمى "lambda" يزن أهمية فقدان الاستخلاص. في هذا المثال، سنستخدم "temperature=5" و "lambda=0.5". سنستخدم دالة فقدان Kullback-Leibler لحساب الانحراف بين الطالب والمعلم. تعبر دالة KL عن مقدار المعلومات الإضافية التي نحتاجها لتمثيل P باستخدام Q. إذا كان الاثنان متطابقين، فإن انحرافهما KL يكون صفرًا، حيث لا توجد معلومات أخرى مطلوبة لشرح P من Q. وبالتالي، فإن دالة KL مفيدة في سياق استخلاص المعرفة.

سنقوم الآن بتسجيل الدخول إلى Hugging Face Hub حتى نتمكن من دفع نموذجنا إلى Hugging Face Hub من خلال Trainer.

دعونا نقوم بضبط TrainingArguments، ونموذج المعلم ونموذج الطالب.

عدد التصنيفات هو عدد الأسماء في ميزة "labels" في قسم "train" من مجموعة البيانات.

نموذج المعلم هو نموذج لتصنيف الصور، مع عدد من التصنيفات يساوي عدد الأسماء في ميزة "labels" في قسم "train" من مجموعة البيانات. يتم تجاهل عدم تطابق الأحجام.

يتم تدريب نموذج MobileNetV2 من البداية.

يمكننا استخدام دالة compute_metrics لتقييم نموذجنا على مجموعة الاختبار. ستُستخدم هذه الدالة أثناء عملية التدريب لحساب دقة F1 للنموذج.

دعونا نقوم بتهيئة Trainer باستخدام TrainingArguments التي حددناها. سنقوم أيضًا بتهيئة مجمع البيانات Data Collator.

يمكننا الآن تدريب نموذجنا.

يمكننا تقييم النموذج على مجموعة الاختبار.

يصل نموذجنا إلى دقة 72% على مجموعة الاختبار. ولإجراء فحص للتحقق من كفاءة عملية الاستخلاص، قمنا أيضًا بتدريب MobileNet على مجموعة بيانات beans من البداية باستخدام نفس المعلمات ووجدنا أن الدقة تبلغ 63% على مجموعة الاختبار. ندعو القراء إلى تجربة نماذج معلم مختلفة، وهندسات الطالب، ومعلمات الاستخلاص، والإبلاغ عن النتائج. يمكن العثور على سجلات التدريب ونقاط التفتيش للنموذج المستخلص في هذا المستودع، ويمكن العثور على MobileNetV2 الذي تم تدريبه من البداية في هذا المستودع.