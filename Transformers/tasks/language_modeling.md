# ุงูููุฐุฌุฉ ุงููุบููุฉ ุงูุณุจุจูุฉ

ููุงู ููุนุงู ูู ุงูููุฐุฌุฉ ุงููุบููุฉุ ุงูููุฐุฌุฉ ุงูุณุจุจูุฉ ูุงูููุฐุฌุฉ ุงููููุนุฉ. ููุถุญ ูุฐุง ุงูุฏููู ุงูููุฐุฌุฉ ุงููุบููุฉ ุงูุณุจุจูุฉ.

ุชูุณุชุฎุฏู ููุงุฐุฌ ุงููุบุฉ ุงูุณุจุจูุฉ ุจุดูู ูุชูุฑุฑ ูุชูููุฏ ุงููุตูุต. ููููู ุงุณุชุฎุฏุงู ูุฐู ุงูููุงุฐุฌ ูุชุทุจููุงุช ุฅุจุฏุงุนูุฉ ูุซู ุงุฎุชูุงุฑ ูุบุงูุฑุชู ุงููุตูุฉ ุงูุฎุงุตุฉ ุฃู ูุณุงุนุฏ ุชุฑููุฒ ุฐูู ูุซู Copilot ุฃู CodeParrot.

ุชุชูุจุฃ ุงูููุฐุฌุฉ ุงููุบููุฉ ุงูุณุจุจูุฉ ุจุงูุฑูุฒ ุงูุชุงูู ูู ุชุณูุณู ุงูุฑููุฒุ ููุง ูููู ูููููุฐุฌ ุณูู ุงูุงูุชูุงู ุจุงูุฑููุฒ ุงูููุฌูุฏุฉ ุนูู ุงููุณุงุฑ. ููุฐุง ูุนูู ุฃู ุงููููุฐุฌ ูุง ููููู ุฑุคูุฉ ุงูุฑููุฒ ุงููุณุชูุจููุฉ. ูุนุฏ GPT-2 ูุซุงููุง ุนูู ูููุฐุฌ ุงููุบุฉ ุงูุณุจุจู.

ุณููุถุญ ูุฐุง ุงูุฏููู ููููุฉ:

1. ุถุจุท ูููุฐุฌ DistilGPT2 ุงูุฏููู ุนูู ูุฌููุนุฉ ูุฑุนูุฉ ูู r/askscience ูู ูุฌููุนุฉ ุจูุงูุงุช ELI5.
2. ุงุณุชุฎุฏุงู ูููุฐุฌู ุงูุฏููู ููุงุณุชูุชุงุฌ.

ูุจู ุฃู ุชุจุฏุฃุ ุชุฃูุฏ ูู ุชุซุจูุช ุฌููุน ุงูููุชุจุงุช ุงูุถุฑูุฑูุฉ:

```bash
pip install transformers datasets evaluate
```

ูุญู ูุดุฌุนู ุนูู ุชุณุฌูู ุงูุฏุฎูู ุฅูู ุญุณุงุจ Hugging Face ุงูุฎุงุต ุจู ุญุชู ุชุชููู ูู ุชุญููู ูููุฐุฌู ููุดุงุฑูุชู ูุน ุงููุฌุชูุน. ุนูุฏูุง ููุทูุจ ููู ุฐููุ ุฃุฏุฎู ุฑูุฒู ููุชุณุฌูู:

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

## ุชุญููู ูุฌููุนุฉ ุจูุงูุงุช ELI5

ุงุจุฏุฃ ุจุชุญููู ุฃูู 5000 ูุซุงู ูู ูุฌููุนุฉ ุจูุงูุงุช [ELI5-Category](https://huggingface.co/datasets/eli5_category) ุจุงุณุชุฎุฏุงู ููุชุจุฉ Datasets ๐ค. ุณูุนุทูู ูุฐุง ูุฑุตุฉ ููุชุฌุฑุจุฉ ูุงูุชุฃูุฏ ูู ุฃู ูู ุดูุก ูุนูู ูุจู ูุถุงุก ุงููุฒูุฏ ูู ุงูููุช ูู ุงูุชุฏุฑูุจ ุนูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุงููุงููุฉ.

```py
>>> from datasets import load_dataset

>>> eli5 = load_dataset("eli5_category", split="train[:5000]")
```

ูุณููู ูุฌููุนุฉ ุงูุจูุงูุงุช ุฅูู ูุฌููุนุงุช ูุฑุนูุฉ ููุชุฏุฑูุจ ูุงูุงุฎุชุจุงุฑ ุจุงุณุชุฎุฏุงู ุทุฑููุฉ [`~datasets.Dataset.train_test_split`]:

```py
>>> eli5 = eli5.train_test_split(test_size=0.2)
```

ุซู ุงูู ูุธุฑุฉ ุนูู ูุซุงู:

```py
>>> eli5["train"][0]
{'q_id': '7h191n',
'title': 'ูุง ุงูุฐู ูุนููู ูุดุฑูุน ูุงููู ุงูุถุฑุงุฆุจ ุงูุฐู ุชู ุชูุฑูุฑู ุงููููุ ููู ุณูุคุซุฑ ุนูู ุงูุฃูุฑููููู ูู ูู ุดุฑูุญุฉ ุถุฑูุจูุฉุ',
'selftext': '',
'category': 'ุงูุชุตุงุฏ',
'subreddit': 'explainlikeimfive',
'answers': {'a_id': ['dqnds8l', 'dqnd1jl', 'dqng3i1', 'dqnku5x'],
'ุงููุต': ["ูุดุฑูุน ูุงููู ุงูุถุฑุงุฆุจ ุนุจุงุฑุฉ ุนู 500 ุตูุญุฉ ููุงูุช ููุงู ุงููุซูุฑ ูู ุงูุชุบููุฑุงุช ุงูุชู ูุง ุชุฒุงู ุฌุงุฑูุฉ ุญุชู ุงูููุงูุฉ. ุงูุฃูุฑ ูุง ููุชุตุฑ ุนูู ุชุนุฏูู ุดุฑุงุฆุญ ุถุฑูุจุฉ ุงูุฏุฎูุ ุจู ูู ูุฌููุนุฉ ูุงููุฉ ูู ุงูุชุบููุฑุงุช. ูุจุงูุชุงูู ูุง ููุฌุฏ ุฅุฌุงุจุฉ ุฌูุฏุฉ ุนูู ุณุคุงูู. ุงููุชุงุฆุฌ ุงูุฑุฆูุณูุฉ ูู: - ุงูุฎูุงุถ ูุจูุฑ ูู ูุนุฏู ุถุฑูุจุฉ ุงูุดุฑูุงุช ุณูุฌุนู ุงูุดุฑูุงุช ุงููุจุฑู ุณุนูุฏุฉ ููุบุงูุฉ. - ุณูุคุฏู ุชุบููุฑ ูุนุฏู ุงููุฑูุฑ ุฅูู ุฌุนู ุจุนุถ ุฃููุงุท ุงูุฃุนูุงู (ุดุฑูุงุช ุงููุงููู ูุตูุงุฏูู ุงูุชุญูุท) ุณุนูุฏุฉ ููุบุงูุฉ - ุชุนุฏููุงุช ุถุฑูุจุฉ ุงูุฏุฎู ูุนุชุฏูุฉุ ููู ุงูููุฑุฑ ุฃู ุชูุชูู (ุนูู ุงูุฑุบู ูู ุฃููุง ุงูููุน ุงูุฐู ูุฏ ูุชู ุฅุนุงุฏุฉ ุชุทุจููู ุฏุงุฆููุง ุฏูู ุฌุนูู ุฏุงุฆููุง) - ูุฎุณุฑ ุงูุฃุดุฎุงุต ูู ุงูููุงูุงุช ุฐุงุช ุงูุถุฑุงุฆุจ ุงููุฑุชูุนุฉ (ูุงููููุฑููุง ููููููุฑู)ุ ููุฏ ููุชูู ุงูุฃูุฑ ุจุฒูุงุฏุฉ ุงูุถุฑุงุฆุจ.",
'ูุง ุดูุก ุญุชู ุงูุขู. ูุฌุจ ุงูุชูููู ุจููู ูุจูู ูุดุฑูุน ูุงููู ูุฌูุณ ุงูููุงุจ ุงููุฎุชูู ุชูุงููุง ุซู ุชูุฑูุฑู ูุฑุฉ ุฃุฎุฑู.',
'ุฃูุถุง: ูู ููุทุจู ูุฐุง ุนูู ุถุฑุงุฆุจ 2017ุ ุฃู ุฃููุง ุชุจุฏุฃ ุจุถุฑุงุฆุจ 2018ุ',
'ุชูุถุญ ูุฐู ุงูููุงูุฉ ููุง ูู ูุดุฑูุนู ูุฌูุณ ุงูููุงุจ ูุงูุดููุฎุ ุจูุง ูู ุฐูู ุงูุชุบููุฑุงุช ุงูููุชุฑุญุฉ ุนูู ุถุฑุงุฆุจ ุงูุฏุฎู ุงูุฎุงุตุฉ ุจู ุจูุงุกู ุนูู ูุณุชูู ุฏุฎูู. URL_0'],
'ุงูุญุณุงุจ': [21ุ 19ุ 5ุ 3]ุ
'ูุต_ุงูุนูุงููู': [[],
[]ุ
[]ุ
['https://www.investopedia.com/news/trumps-tax-reform-what-can-be-done/']]},
'ุงูุนูุงููู_ุงูุนูุงููู': ['url'],
'selftext_urls': ['url']}
```

ุนูู ุงูุฑุบู ูู ุฃู ูุฐุง ูุฏ ูุจุฏู ูุซูุฑูุงุ ุฅูุง ุฃูู ููุชู ุญููุง ุจุญูู "ุงููุต". ูุง ูู ุฑุงุฆุน ุญูู ููุงู ุงูููุฐุฌุฉ ุงููุบููุฉ ูู ุฃูู ูุง ุชุญุชุงุฌ ุฅูู ุชุณููุงุช (ุชูุนุฑู ุฃูุถูุง ุจุงุณู ุงููููุฉ ุบูุฑ ุงูุฎุงุถุนุฉ ููุฅุดุฑุงู) ูุฃู ุงููููุฉ ุงูุชุงููุฉ *ูู* ุงูุชุณููุฉ.

## ูุนุงูุฌุฉ ูุณุจูุฉ

ุงูุฎุทูุฉ ุงูุชุงููุฉ ูู ุชุญููู ุจุฑูุงูุฌ ุชุดููุฑ DistilGPT2 ููุนุงูุฌุฉ ุญูู "ุงููุต":

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("distilbert/distilgpt2")
```

ุณุชูุงุญุธ ูู ุงููุซุงู ุฃุนูุงูุ ุฃู ุญูู "ุงููุต" ููุฌูุฏ ุจุงููุนู ุฏุงุฎู "ุงูุฅุฌุงุจุงุช". ููุฐุง ูุนูู ุฃูู ุณุชุญุชุงุฌ ุฅูู ุงุณุชุฎุฑุงุฌ ุญูู "ุงููุต" ูู ููููู ุงููุถูู ุจุงุณุชุฎุฏุงู ุทุฑููุฉ ["flatten"](https://huggingface.co/docs/datasets/process#flatten):

```py
>>> eli5 = eli5.flatten()
>>> eli5["train"][0]
{'q_id': '7h191n',
'title': 'ูุง ุงูุฐู ูุนููู ูุดุฑูุน ูุงููู ุงูุถุฑุงุฆุจ ุงูุฐู ุชู ุชูุฑูุฑู ุงููููุ ููู ุณูุคุซุฑ ุนูู ุงูุฃูุฑููููู ูู ูู ุดุฑูุญุฉ ุถุฑูุจูุฉุ',
'selftext': '',
'category': 'ุงูุชุตุงุฏ',
'subreddit': 'explainlikeimfive',
'answers.a_id': ['dqnds8l', 'dqnd1jl', 'dqng3i1', 'dqnku5x'],
'answers.text': ["ูุดุฑูุน ูุงููู ุงูุถุฑุงุฆุจ ุนุจุงุฑุฉ ุนู 500 ุตูุญุฉ ููุงูุช ููุงู ุงููุซูุฑ ูู ุงูุชุบููุฑุงุช ุงูุชู ูุง ุชุฒุงู ุฌุงุฑูุฉ ุญุชู ุงูููุงูุฉ. ุงูุฃูุฑ ูุง ููุชุตุฑ ุนูู ุชุนุฏูู ุดุฑุงุฆุญ ุถุฑูุจุฉ ุงูุฏุฎูุ ุจู ูู ูุฌููุนุฉ ูุงููุฉ ูู ุงูุชุบููุฑุงุช. ูุจุงูุชุงูู ูุง ููุฌุฏ ุฅุฌุงุจุฉ ุฌูุฏุฉ ุนูู ุณุคุงูู. ุงููุชุงุฆุฌ ุงูุฑุฆูุณูุฉ ูู: - ุงูุฎูุงุถ ูุจูุฑ ูู ูุนุฏู ุถุฑูุจุฉ ุงูุดุฑูุงุช ุณูุฌุนู ุงูุดุฑูุงุช ุงููุจุฑู ุณุนูุฏุฉ ููุบุงูุฉ. - ุณูุคุฏู ุชุบููุฑ ูุนุฏู ุงููุฑูุฑ ุฅูู ุฌุนู ุจุนุถ ุฃููุงุท ุงูุฃุนูุงู (ุดุฑูุงุช ุงููุงููู ูุตูุงุฏูู ุงูุชุญูุท) ุณุนูุฏุฉ ููุบุงูุฉ - ุชุนุฏููุงุช ุถุฑูุจุฉ ุงูุฏุฎู ูุนุชุฏูุฉุ ููู ุงูููุฑุฑ ุฃู ุชูุชูู (ุนูู ุงูุฑุบู ูู ุฃููุง ุงูููุน ุงูุฐู ูุฏ ูุชู ุฅุนุงุฏุฉ ุชุทุจููู ุฏุงุฆููุง ุฏูู ุฌุนูู ุฏุงุฆููุง) - ูุฎุณุฑ ุงูุฃุดุฎุงุต ูู ุงูููุงูุงุช ุฐุงุช ุงูุถุฑุงุฆุจ ุงููุฑุชูุนุฉ (ูุงููููุฑููุง ููููููุฑู)ุ ููุฏ ููุชูู ุงูุฃูุฑ ุจุฒูุงุฏุฉ ุงูุถุฑุงุฆุจ.",
'ูุง ุดูุก ุญุชู ุงูุขู. ูุฌุจ ุงูุชูููู ุจููู ูุจูู ูุดุฑูุน ูุงููู ูุฌูุณ ุงูููุงุจ ุงููุฎุชูู ุชูุงููุง ุซู ุชูุฑูุฑู ูุฑุฉ ุฃุฎุฑู.',
'ุฃูุถุง: ูู ููุทุจู ูุฐุง ุนูู ุถุฑุงุฆุจ 2017ุ ุฃู ุฃููุง ุชุจุฏุฃ ุจุถุฑุงุฆุจ 2018ุ',
'ุชูุถุญ ูุฐู ุงูููุงูุฉ ููุง ูู ูุดุฑูุนู ูุฌูุณ ุงูููุงุจ ูุงูุดููุฎุ ุจูุง ูู ุฐูู ุงูุชุบููุฑุงุช ุงูููุชุฑุญุฉ ุนูู ุถุฑุงุฆุจ ุงูุฏุฎู ุงูุฎุงุตุฉ ุจู ุจูุงุกู ุนูู ูุณุชูู ุฏุฎูู. URL_0'],
'answers.score': [21, 19, 5, 3],
'answers.text_urls': [[],
[]ุ
[]ุ
['https://www.investopedia.com/news/trumps-tax-reform-what-can-be-done/']],
'ุงูุนูุงููู_ุงูุนูุงููู': ['url'],
'selftext_urls': ['url']}
```

ูู ุญูู ูุฑุนู ูู ุงูุขู ุนููุฏ ูููุตู ููุง ูู ููุถุญ ุจุงูุจุงุฏุฆุฉ "ุงูุฅุฌุงุจุงุช"ุ ูุญูู "ุงููุต" ูู ูุงุฆูุฉ ุงูุขู. ุจุฏูุงู ูู ุชุดููุฑ ูู ุฌููุฉ ุจุดูู ูููุตูุ ูู ุจุชุญููู ุงููุงุฆูุฉ ุฅูู ุณูุณูุฉ ุญุชู ุชุชููู ูู ุชุดููุฑูุง ุจุดูู ูุดุชุฑู.

ูุฐู ูู ุฏุงูุฉ ุงููุนุงูุฌุฉ ุงููุณุจูุฉ ุงูุฃููู ูุฏูุฌ ูุงุฆูุฉ ุงูุณูุงุณู ููู ูุซุงู ูุชุดููุฑ ุงููุชูุฌุฉ:

```py
>>> def preprocess_function(examples):
...     return tokenizer([" ".join(x) for x in examples["answers.text"]])
```

ูุชุทุจูู ุฏุงูุฉ ุงููุนุงูุฌุฉ ุงููุณุจูุฉ ูุฐู ุนูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุจุฃููููุงุ ุงุณุชุฎุฏู ุทุฑููุฉ [`~datasets.Dataset.map`] ูู ููุชุจุฉ Datasets ๐ค. ููููู ุชุณุฑูุน ูุธููุฉ "ุงููุฎุทุท" ุนู ุทุฑูู ุชุนููู "batched=True" ููุนุงูุฌุฉ ุนูุงุตุฑ ูุชุนุฏุฏุฉ ูู ูุฌููุนุฉ ุงูุจูุงูุงุช ูู ููุช ูุงุญุฏุ ูุฒูุงุฏุฉ ุนุฏุฏ ุงูุนูููุงุช ุจุงุณุชุฎุฏุงู "num_proc". ุงุญุฐู ุฃู ุฃุนูุฏุฉ ูุง ุชุญุชุงุฌูุง:

```py
>>> tokenized_eli5 = eli5.map(
...     preprocess_function,
...     batched=True,
...     num_proc=4,
...     remove_columns=eli5["train"].column_names,
... )
```

ุชุญุชูู ูุฌููุนุฉ ุงูุจูุงูุงุช ูุฐู ุนูู ุชุณูุณูุงุช ุงูุฑููุฒุ ูููู ุจุนุถูุง ุฃุทูู ูู ุทูู ุงูุฅุฏุฎุงู ุงูุฃูุตู ูููููุฐุฌ.

ุงูุขู ููููู ุงุณุชุฎุฏุงู ุฏุงูุฉ ุงููุนุงูุฌุฉ ุงููุณุจูุฉ ุงูุซุงููุฉ ู:

- ุฏูุฌ ุฌููุน ุงูุชุณูุณูุงุช
- ุชูุณูู ุงูุชุณูุณูุงุช ุงููุฏูุฌุฉ ุฅูู ูุทุน ุฃูุตุฑ ูุญุฏุฏุฉ ุจูุงุณุทุฉ "block_size"ุ ูุงูุชู ูุฌุจ ุฃู ุชููู ุฃูุตุฑ ูู ุทูู ุงูุฅุฏุฎุงู ุงูุฃูุตู ููุตูุฑุฉ ุจุฏุฑุฌุฉ ูุงููุฉ ูุฐุงูุฑุฉ GPU RAM.

```py
>>> block_size = 128


>>> def group_texts(examples):
...     # Concatenate all texts.
...     concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}
...     total_length = len(concatenated_examples[list(examples.keys())[0]])
...     # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can
...     # customize this part to your needs.
...     if total_length >= block_size:
...         total_length = (total_length // block_size) * block_size
...     # Split by chunks of block_size.
...     result = {
...         k: [t[i : i + block_size] for i in range(0, total_length, block_size)]
...         for k, t in concatenated_examples.items()
...     }
...     result["labels"] = result["input_ids"].copy()
...     return result
```

ุทุจู ูุธููุฉ "group_texts" ุนูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุจุฃููููุง:

```py
>>> lm_dataset = tokenized_eli5.map(group_texts, batched=True, num_proc=4)
```

ุงูุขู ูู ุจุฅูุดุงุก ุฏูุนุฉ ูู ุงูุฃูุซูุฉ ุจุงุณุชุฎุฏุงู [`DataCollatorForLanguageModeling`]. ูู ุงูุฃูุซุฑ ููุงุกุฉ *ุญุดู* ุงูุฌูู ุฏููุงูููููุง ุฅูู ุฃุทูู ุทูู ูู ุฏูุนุฉ ุฃุซูุงุก ุงูุชุฌููุนุ ุจุฏูุงู ูู ุญุดู ูุฌููุนุฉ ุงูุจูุงูุงุช ุจุฃููููุง ุฅูู ุงูุทูู ุงูุฃูุตู.

<frameworkcontent>
<pt>
ุงุณุชุฎุฏู ุฑูุฒ ููุงูุฉ ุงูุชุณูุณู ูุฑููุฒ ุญุดู ููู ุจุชุนููู "mlm=False". ุณูุชู ุงุณุชุฎุฏุงู ูุฐุง ููุฏุฎูุงุช ูุนูุงูุงุช ููุฒุงุญุฉ ุฅูู ุงููููู ุจูุงุณุทุฉ ุนูุตุฑ ูุงุญุฏ:

```py
>>> from transformers import DataCollatorForLanguageModeling

>>> tokenizer.pad_token = tokenizer.eos_token
>>> data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)
```

</pt>
<tf>
ุงุณุชุฎุฏู ุฑูุฒ ููุงูุฉ ุงูุชุณูุณู ูุฑููุฒ ุญุดู ููู ุจุชุนููู "mlm=False". ุณูุชู ุงุณุชุฎุฏุงู ูุฐุง ููุฏุฎูุงุช ูุนูุงูุงุช ููุฒุงุญุฉ ุฅูู ุงููููู ุจูุงุณุทุฉ ุนูุตุฑ ูุงุญุฏ:

```py
>>> from transformers import DataCollatorForLanguageModeling

>>> data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=Falseุ return_tensors="tf")
```

</tf>
</frameworkcontent>
ุจุงูุชุฃููุฏุ ุณุฃุชุจุน ุชุนูููุงุชู ุจุฏูุฉ. ูููุง ููู ุชุฑุฌูุฉ ุงููุต ุงูููุฌูุฏ ูู ุงูููุฑุงุช ูุงูุนูุงููู:

## ุงูุชุฏุฑูุจ

ุฅุฐุง ูู ุชูู ูุนุชุงุฏูุง ุนูู ุถุจุท ูููุฐุฌ ุจุงุณุชุฎุฏุงู [`Trainer`]ุ ูุฑุงุฌุน [ุงูุจุฑูุงูุฌ ุงูุชุนูููู ุงูุฃุณุงุณู] (../training#train-with-pytorch-trainer)

ุฃูุช ุงูุขู ุนูู ุงุณุชุนุฏุงุฏ ูุจุฏุก ุชุฏุฑูุจ ูููุฐุฌู! ูู ุจุชุญููู DistilGPT2 ุจุงุณุชุฎุฏุงู [`AutoModelForCausalLM`]:

```python
>>> from transformers import AutoModelForCausalLM, TrainingArguments, Trainer

>>> model = AutoModelForCausalLM.from_pretrained("distilbert/distilgpt2")
```

ูู ูุฐู ุงููุฑุญูุฉุ ูู ูุชุจู ุณูู ุซูุงุซ ุฎุทูุงุช:

1. ุญุฏุฏ ูุฑุท ูุนููุงุช ุงูุชุฏุฑูุจ ุงูุฎุงุตุฉ ุจู ูู [`TrainingArguments`]. ุงููุนููุฉ ุงููุทููุจุฉ ุงููุญูุฏุฉ ูู `output_dir` ุงูุชู ุชุญุฏุฏ ุฃูู ูุชู ุญูุธ ูููุฐุฌู. ุณุชููู ุจุฏูุน ูุฐุง ุงููููุฐุฌ ุฅูู ุงููุญูุฑ ุนู ุทุฑูู ุชุนููู `push_to_hub=True` (ูุฌุจ ุฃู ุชููู ูุฏ ุณุฌูุช ุงูุฏุฎูู ุฅูู Hugging Face ูุชุญููู ูููุฐุฌู).

2. ูุฑุฑ ูุฑุท ูุนููุงุช ุงูุชุฏุฑูุจ ุฅูู [`Trainer`] ุฌูุจูุง ุฅูู ุฌูุจ ูุน ุงููููุฐุฌ ููุฌููุนุงุช ุงูุจูุงูุงุช ููุฌูุน ุงูุจูุงูุงุช.

3. ุงุณุชุฏุนุงุก [`~Trainer.train`] ูุถุจุท ูููุฐุฌู.

```python
>>> training_args = TrainingArguments(
...     output_dir="my_awesome_eli5_clm-model",
...     eval_strategy="epoch"ุ
...     learning_rate=2e-5,
...     weight_decay=0.01,
...     push_to_hub=True,
... )

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=lm_dataset["train"]ุ
...     eval_dataset=lm_dataset["test"]ุ
...     data_collator=data_collator,
... )

>>> trainer.train()
```

ุจูุฌุฑุฏ ุงูุชูุงู ุงูุชุฏุฑูุจุ ุงุณุชุฎุฏู ุทุฑููุฉ [`~transformers.Trainer.evaluate`] ูุชูููู ูููุฐุฌู ูุงูุญุตูู ุนูู ุบููุถู:

```python
>>> import math

>>> eval_results = trainer.evaluate()
>>> print(f"Perplexity: {math.exp(eval_results['eval_loss']):.2f}")
Perplexity: 49.61
```

ุจุนุฏ ุฐููุ ุดุงุฑู ูููุฐุฌู ูู ุงููุญูุฑ ุจุงุณุชุฎุฏุงู ุทุฑููุฉ [`~transformers.Trainer.push_to_hub`] ุญุชู ูุชููู ุงูุฌููุน ูู ุงุณุชุฎุฏุงู ูููุฐุฌู:

```python
>>> trainer.push_to_hub()
```

ุฅุฐุง ูู ุชูู ูุนุชุงุฏูุง ุนูู ุถุจุท ูููุฐุฌ ุจุงุณุชุฎุฏุงู Kerasุ ูุฑุงุฌุน [ุงูุจุฑูุงูุฌ ุงูุชุนูููู ุงูุฃุณุงุณู] (../training#train-a-tensorflow-model-with-keras)

ูุถุจุท ูููุฐุฌ ูู TensorFlowุ ุงุจุฏุฃ ุจุฅุนุฏุงุฏ ุฏุงูุฉ ูุญุณู ููุนุฏู ุชุนูู ูุฌุฏููุ ูุจุนุถ ูุฑุท ูุนููุงุช ุงูุชุฏุฑูุจ:

```python
>>> from transformers import create_optimizer, AdamWeightDecay

>>> optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)
```

ุจุนุฏ ุฐููุ ููููู ุชุญููู DistilGPT2 ุจุงุณุชุฎุฏุงู [`TFAutoModelForCausalLM`]:

```python
>>> from transformers import TFAutoModelForCausalLM

>>> model = TFAutoModelForCausalLM.from_pretrained("distilbert/distilgpt2")
```

ูู ุจุชุญููู ูุฌููุนุงุช ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจู ุฅูู ุชูุณูู `tf.data.Dataset` ุจุงุณุชุฎุฏุงู [`~transformers.TFPreTrainedModel.prepare_tf_dataset`]:

```python
>>> tf_train_set = model.prepare_tf_dataset(
...     lm_dataset["train"]ุ
...     shuffle=True,
...     batch_size=16,
...     collate_fn=data_collator,
... )

>>> tf_test_set = model.prepare_tf_dataset(
...     lm_dataset["test"]ุ
...     shuffle=False,
...     batch_size=16,
...     collate_fn=data_collator,
... )
```

ูู ุจุชูููู ุงููููุฐุฌ ููุชุฏุฑูุจ ูุน [`compile`] (https://keras.io/api/models/model_training_apis/#compile-method). ูุงุญุธ ุฃู ุฌููุน ููุงุฐุฌ Transformers ูุฏููุง ุฏุงูุฉ ุฎุณุงุฑุฉ ุฐุงุช ุตูุฉ ุจุงููููุฉ ุจุดูู ุงูุชุฑุงุถูุ ูุฐูู ูุง ุชุญุชุงุฌ ุฅูู ุชุญุฏูุฏ ูุงุญุฏุฉ ูุง ูู ุชุฑุบุจ ูู ุฐูู:

```python
>>> import tensorflow as tf

>>> model.compile(optimizer=optimizer) # ูุง ุชูุฌุฏ ุญุฌุฉ ุงูุฎุณุงุฑุฉ!
```

ูููู ุงูููุงู ุจุฐูู ุนู ุทุฑูู ุชุญุฏูุฏ ุงูููุงู ุงูุฐู ุณุชุฏูุน ููู ูููุฐุฌู ููุนุงูุฌ ุงููุตูุต ูู [`~transformers.PushToHubCallback`]:

```python
>>> from transformers.keras_callbacks import PushToHubCallback

>>> callback = PushToHubCallback(
...     output_dir="my_awesome_eli5_clm-model"ุ
...     tokenizer=tokenizer,
... )
```

ุฃุฎูุฑูุงุ ุฃูุช ุนูู ุงุณุชุนุฏุงุฏ ูุจุฏุก ุชุฏุฑูุจ ูููุฐุฌู! ุงุณุชุฏุนุงุก [`fit`] (https://keras.io/api/models/model_training_apis/#fit-method) ูุน ูุฌููุนุงุช ุงูุจูุงูุงุช ุงูุชุฏุฑูุจูุฉ ูุงูุชุญูู ูู ุตุญุชูุงุ ูุนุฏุฏ ุงูุนุตูุฑุ ูุงุณุชุฏุนุงุก ุงูุนูุฏุฉ ุงูุฎุงุตุฉ ุจู ูุถุจุท ุงููููุฐุฌ:

```python
>>> model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3, callbacks=[callback])
```

ุจูุฌุฑุฏ ุงูุชูุงู ุงูุชุฏุฑูุจุ ูุชู ุชุญููู ูููุฐุฌู ุชููุงุฆููุง ุฅูู ุงููุญูุฑ ุญุชู ูุชููู ุงูุฌููุน ูู ุงุณุชุฎุฏุงูู!

ููุญุตูู ุนูู ูุซุงู ุฃูุซุฑ ุชูุตููุงู ุญูู ููููุฉ ุถุจุท ูููุฐุฌ ููููุฐุฌุฉ ุงููุบููุฉ ุงูุณุจุจูุฉุ ุฑุงุฌุน ุงูุฏูุชุฑ ุงูููุงุจู
[ุฏูุชุฑ ููุงุญุธุงุช PyTorch] (https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb)
ุฃู [ุฏูุชุฑ TensorFlow] (https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb).

## ุงูุงุณุชูุชุงุฌ

ุฑุงุฆุนุ ุงูุขู ุจุนุฏ ุฃู ุถุจุทุช ูููุฐุฌูุงุ ููููู ุงุณุชุฎุฏุงูู ููุงุณุชูุชุงุฌ!

ููุฑ ูู ููุฌู ุชุฑุบุจ ูู ุชูููุฏ ูุต ููู:

```python
>>> prompt = "ุชุณูุญ ุงูุทูุฑุฉ ุงูุฌุณุฏูุฉ ููุธุงู ุงูููุงุนุฉ"
```

ุฃุจุณุท ุทุฑููุฉ ูุชุฌุฑุจุฉ ูููุฐุฌู ุงููุถุจูุท ููุงุณุชูุชุงุฌ ูู ุงุณุชุฎุฏุงูู ูู [`pipeline`]. ูู ุจุชูููุฐ ูุซูู `pipeline` ูุชูููุฏ ุงููุต ุจุงุณุชุฎุฏุงู ูููุฐุฌูุ ููุฑุฑ ูุตู ุฅููู:

```python
>>> from transformers import pipeline

>>> generator = pipeline("text-generation", model="username/my_awesome_eli5_clm-model")
>>> generator(prompt)
[{'generated_text': 'ุชุณูุญ ุงูุทูุฑุฉ ุงูุฌุณุฏูุฉ ููุธุงู ุงูููุงุนุฉ ุจุงูุชุนุงูู ูุน ุงูุฃุฏููุฉ ุจุงููุฏุฑุฉ ุนูู ุงูุชููู ูุน ูุถุน ุจูุฆู ูุฎุชูู.\n\n\nุฅู ุงูุถุฑุฑ ุงูุฐู ููุญู ุจุงูุนุฏูู ูุงุชุฌ ุนู ูุฏุฑุฉ ูุธุงู ุงูููุงุนุฉ ุนูู ุฃุฏุงุก ููุงูู ุงูุฎุงุตุฉ ุจุงูุชุตุญูุญ ุงูุฐุงุชู. "}]
```

ูู ุจุฑูุฒ ุงููุต ูุฅุฑุฌุงุน `input_ids` ูุชูุชุฑุงุช PyTorch:

```python
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("username/my_awesome_eli5_clm-model")
>>> inputs = tokenizer(prompt, return_tensors="pt").input_ids
```

ุงุณุชุฎุฏู ุทุฑููุฉ [`~generation.GenerationMixin.generate`] ูุชูููุฏ ุงููุต.
ููุฒูุฏ ูู ุงูุชูุงุตูู ุญูู ุงุณุชุฑุงุชูุฌูุงุช ุชูููุฏ ุงููุต ุงููุฎุชููุฉ ูุงููุนููุงุช ููุชุญูู ูู ุงูุชูููุฏุ ุฑุงุฌุน ุตูุญุฉ [ุงุณุชุฑุงุชูุฌูุงุช ุชูููุฏ ุงููุต] (../generation_strategies).

```python
>>> from transformers import AutoModelForCausalLM

>>> model = AutoModelForCausalLM.from_pretrained("username/my_awesome_eli5_clm-model")
>>> outputs = model.generate(inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)
```

ูู ุจูู ุฑููุฒ ุฑููุฒ ุงูุฑููุฒ ุงููููุฏุฉ ูุฑุฉ ุฃุฎุฑู ุฅูู ูุต:

```python
>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)
["ุชุณูุญ ุงูุทูุฑุฉ ุงูุฌุณุฏูุฉ ููุธุงู ุงูููุงุนุฉ ุจุงูุชุนุงูู ูุน ุงูุฃุฏููุฉ ุจุงููุฏุฑุฉ ุนูู ุงูุชููู ูุน ูุถุน ุจูุฆู ูุฎุชูู. ูุจุนุจุงุฑุฉ ุฃุฎุฑูุ ูููู ููุธุงู "ุงูุทูุฑุฉ" ูุณุงุนุฏุฉ ูุธุงู ุงูููุงุนุฉ ุนูู ุงูุชููู ูุน ูุถุน ุจูุฆู ูุฎุชูู ุฃู ูู ุจุนุถ ุงูุญุงูุงุช ุญุชู ุญูุงุฉ ูุงุญุฏุฉ. ุนูู ุงููููุถ ูู ุฐููุ ูุฌุฏ ุงูุจุงุญุซูู ูู ุฌุงูุนุฉ ูุงุณุงุชุดูุณุชุณ ูู ุจูุณุทู ุฃู "ุงูุทูุฑุฉ" ุฃููู ุจูุซูุฑ ูู ุงููุฆุฑุงู ูููุง ูู ุงูุจุดุฑ ูููู ูููู ุงูุนุซูุฑ ุนูููุง ูู ุงูุจุดุฑุ ูุฃููุง ููุณุช ูุฌูููุฉ ุชูุงููุง ููุธุงู ุงูููุงุนุฉ. ุฏุฑุงุณุฉ ุญูู ููููุฉ ูุธุงู ุงูููุงุนุฉ"]
```

ูู ุจุฑูุฒ ุงููุต ูุฅุฑุฌุงุน `input_ids` ูุชูุชุฑุงุช TensorFlow:

```python
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("username/my_awesome_eli5_clm-model")
>>> inputs = tokenizer(prompt, return_tensors="tf").input_ids
```

ุงุณุชุฎุฏู ุทุฑููุฉ [`~transformers.generation_tf_utils.TFGenerationMixin.generate`] ูุฅูุดุงุก ุงูููุฎุต. ููุฒูุฏ ูู ุงูุชูุงุตูู ุญูู ุงุณุชุฑุงุชูุฌูุงุช ุชูููุฏ ุงููุต ุงููุฎุชููุฉ ูุงููุนููุงุช ููุชุญูู ูู ุงูุชูููุฏุ ุฑุงุฌุน ุตูุญุฉ [ุงุณุชุฑุงุชูุฌูุงุช ุชูููุฏ ุงููุต] (../generation_strategies).

```python
>>> from transformers import TFAutoModelForCausalLM

>>> model = TFAutoModelForCausalLM.from_pretrained("username/my_awesome_eli5_clm-model")
>>> outputs = model.generate(input_ids=inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)
```

ูู ุจูู ุฑููุฒ ุฑููุฒ ุงูุฑููุฒ ุงููููุฏุฉ ูุฑุฉ ุฃุฎุฑู ุฅูู ูุต:

```python
>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)
['ุชุณูุญ ุงูุทูุฑุฉ ุงูุฌุณุฏูุฉ ููุธุงู ุงูููุงุนุฉ ุจุงููุดู ุนู ูุฌูุฏ ููุฑูุณุงุช ุฃุฎุฑู ูุน ุฒูุงุฏุฉ ุงูุชุดุงุฑูุง. ูุฐููุ ุญุฏุฏ ุงูุจุงุญุซูู ูุณุจุฉ ุนุงููุฉ ูู ุงูููุฑูุณุงุช ุงูุจุดุฑูุฉ. ุชุฒุฏุงุฏ ูุณุจุฉ ุงูููุฑูุณุงุช ุงููุฑุชุจุทุฉ ุจุงูููุฑูุณุงุช ูู ุฏุฑุงุณุชูุง ูุน ุชูุฏู ุงูุนูุฑ. ูุฐููุ ููุชุฑุญ ุฎูุงุฑุฒููุฉ ุจุณูุทุฉ ูููุดู ุนู ูุฌูุฏ ูุฐู ุงูููุฑูุณุงุช ุงูุฌุฏูุฏุฉ ูู ุนููุงุชูุง ูุนูุงูุฉ ุนูู ุชุญุณู ุงูููุงุนุฉ. ุชูุฏู ุฏุฑุงุณุฉ ุฃููู ูุงุฆูุฉ ุนูู ูุฐุง ุงูุฎูุงุฑุฒููุฉุ ูุงูุชู ุณุชูุดุฑ ูู ูุฌูุฉ Science ููู ุงูุฌูุนุฉุ ุฅูู ุฅุธูุงุฑ ุฃู ูุฐุง ุงูุงูุชุดุงู ูููู ุฃู ูุชุฑุฌู ุฅูู ุชุทููุฑ ููุงุญ ุฃูุถู ูููู ุฃูุซุฑ ูุนุงููุฉ']
```