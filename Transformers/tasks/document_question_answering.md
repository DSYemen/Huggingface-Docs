## ุงูุฅุฌุงุจุฉ ุนูู ุงูุฃุณุฆูุฉ ุงููุงุฑุฏุฉ ูู ุงููุณุชูุฏ

[[open-in-colab]]

ุงูุฅุฌุงุจุฉ ุนูู ุงูุฃุณุฆูุฉ ุงููุงุฑุฏุฉ ูู ุงููุณุชูุฏุ ูุงูุชู ูุดุงุฑ ุฅูููุง ุฃูุถูุง ุจุงุณู ุงูุฅุฌุงุจุฉ ุงููุฑุฆูุฉ ุนูู ุงูุฃุณุฆูุฉ ุงููุงุฑุฏุฉ ูู ุงููุณุชูุฏุ ูู ูููุฉ ุชุชุถูู ุชูุฏูู ุฅุฌุงุจุงุช ุนูู ุงูุฃุณุฆูุฉ ุงููุทุฑูุญุฉ ุญูู ุตูุฑ ุงููุณุชูุฏุงุช. ุงููุฏุฎูุงุช ุฅูู ุงูููุงุฐุฌ ุงูุชู ุชุฏุนู ูุฐู ุงููููุฉ ูู ุนุงุฏุฉ ูุฒูุฌ ูู ุงูุตูุฑุฉ ูุงูุณุคุงูุ ูุงููุงุชุฌ ูู ุฅุฌุงุจุฉ ูุนุจุฑ ุนููุง ุจุงููุบุฉ ุงูุทุจูุนูุฉ. ุชุณุชุฎุฏู ูุฐู ุงูููุงุฐุฌ ุทุฑุงุฆู ูุชุนุฏุฏุฉุ ุจูุง ูู ุฐูู ุงููุตุ ูููุงุถุน ุงููููุงุช (ุญุฏูุฏ ุงูุฅุญุฏุงุซูุงุช)ุ ูุงูุตูุฑุฉ ููุณูุง.

ููุถุญ ูุฐุง ุงูุฏููู ููููุฉ:

- ุถุจุท ูููุฐุฌ [LayoutLMv2](../model_doc/layoutlmv2) ุงูุฏููู ุนูู ูุฌููุนุฉ ุจูุงูุงุช [DocVQA](https://huggingface.co/datasets/nielsr/docvqa_1200_examples_donut).
- ุงุณุชุฎุฏุงู ุงููููุฐุฌ ุงููุถุจูุท ุฏููููุง ููุงุณุชูุชุงุฌ.

<Tip>

ูุฑุคูุฉ ุฌููุน ุงูุชุตูููุงุช ูููุงุท ุงูุชุญูู ุงููุชูุงููุฉ ูุน ูุฐู ุงููููุฉุ ููุตู ุจุงูุชุญูู ูู [ุตูุญุฉ ุงููููุฉ](https://huggingface.co/tasks/image-to-text).

</Tip>

ูุญู LayoutLMv2 ูุดููุฉ ุงูุฅุฌุงุจุฉ ุนูู ุงูุฃุณุฆูุฉ ูู ุงููุณุชูุฏ ุนู ุทุฑูู ุฅุถุงูุฉ ุฑุฃุณ ููุฅุฌุงุจุฉ ุนูู ุงูุฃุณุฆูุฉ ุฃุนูู ุญุงูุงุช ุงูุฑููุฒ ุงูููุงุฆูุฉุ ููุชูุจุค ุจููุงุถุน ุงูุฑููุฒ ุงูุฃูููุฉ ูุงูููุงุฆูุฉ ููุฅุฌุงุจุฉ. ูุจุนุจุงุฑุฉ ุฃุฎุฑูุ ุชุชู ูุนุงููุฉ ุงููุดููุฉ ุนูู ุฃููุง ุฅุฌุงุจุฉ ุงุณุชุฎูุงุตูุฉ ุนูู ุงูุฃุณุฆูุฉ: ุงุณุชุฎุฑุงุฌ ูุทุนุฉ ุงููุนูููุงุช ุงูุชู ุชุฌูุจ ุนูู ุงูุณุคุงูุ ุจุงููุธุฑ ุฅูู ุงูุณูุงู. ูุฃุชู ุงูุณูุงู ูู ุฅุฎุฑุงุฌ ูุญุฑู ุงูุชุนุฑู ุงูุจุตุฑู ุนูู ุงูุญุฑูู (OCR)ุ ูููุง ูู ุจุฑูุงูุฌ Tesseract ูู Google.

ูุจู ุงูุจุฏุกุ ุชุฃูุฏ ูู ุชุซุจูุช ุฌููุน ุงูููุชุจุงุช ุงูุถุฑูุฑูุฉ. ูุนุชูุฏ LayoutLMv2 ุนูู detectron2 ูtorchvision ูtesseract.

```bash
pip install -q transformers datasets
```

```bash
pip install 'git+https://github.com/facebookresearch/detectron2.git'
pip install torchvision
```

```bash
sudo apt install tesseract-ocr
pip install -q pytesseract
```

ุจูุฌุฑุฏ ุชุซุจูุช ุฌููุน ุงูุชุจุนูุงุชุ ุฃุนุฏ ุชุดุบูู ููุช ุงูุชุดุบูู ุงูุฎุงุต ุจู.

ูุญู ูุดุฌุนู ุนูู ูุดุงุฑูุฉ ูููุฐุฌู ูุน ุงููุฌุชูุน. ูู ุจุชุณุฌูู ุงูุฏุฎูู ุฅูู ุญุณุงุจ Hugging Face ุงูุฎุงุต ุจู ูุชุญูููู ุฅูู ๐ค Hub.

ุนูุฏ ูุทุงูุจุชูุ ุฃุฏุฎู ุฑูุฒู ููุชุณุฌูู:

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

ุฏุนููุง ูุญุฏุฏ ุจุนุถ ุงููุชุบูุฑุงุช ุงูุนุงูููุฉ.

```py
>>> model_checkpoint = "microsoft/layoutlmv2-base-uncased"
>>> batch_size = 4
```

## ุชุญููู ุงูุจูุงูุงุช

ูู ูุฐุง ุงูุฏูููุ ูุณุชุฎุฏู ุนููุฉ ุตุบูุฑุฉ ูู DocVQA ุงููุนุงูุฌุฉ ูุณุจููุง ูุงูุชู ููููู ุงูุนุซูุฑ ุนูููุง ุนูู ๐ค Hub. ุฅุฐุง ููุช ุชุฑุบุจ ูู ุงุณุชุฎุฏุงู ูุฌููุนุฉ DocVQA ุงููุงููุฉุ ูููููู ุงูุชุณุฌูู ูุชูุฒูููุง ูู [ุงูุตูุญุฉ ุงูุฑุฆูุณูุฉ ูู DocVQA](https://rrc.cvc.uab.es/?ch=17). ุฅุฐุง ููุช ุจุฐููุ ููุชุงุจุนุฉ ูุฐุง ุงูุฏูููุ ุฑุงุฌุน [ููููุฉ ุชุญููู ุงููููุงุช ุฅูู ูุฌููุนุฉ ุจูุงูุงุช ๐ค](https://huggingface.co/docs/datasets/loading#local-and-remote-files).

```py
>>> from datasets import load_dataset

>>> dataset = load_dataset("nielsr/docvqa_1200_examples")
>>> dataset
DatasetDict({
train: Dataset({
features: ['id', 'image', 'query', 'answers', 'words', 'bounding_boxes', 'answer'],
num_rows: 1000
})
test: Dataset({
features: ['id', 'image', 'query', 'answers', 'words', 'bounding_boxes', 'answer'],
num_rows: 200
})
})
```

ููุง ุชุฑูุ ุชู ุชูุณูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุจุงููุนู ุฅูู ูุฌููุนุงุช ุชุฏุฑูุจ ูุงุฎุชุจุงุฑ. ุงูู ูุธุฑุฉ ุนูู ูุซุงู ุนุดูุงุฆู ููุชุนุฑู ุนูู ุงูููุฒุงุช.

```py
>>> dataset["train"].features
```

ูุฐุง ูุง ุชูุซูู ุงูุญููู ุงููุฑุฏูุฉ:

- `id`: ูุนุฑู ุงููุซุงู
- `image`: ูุงุฆู PIL.Image.Image ูุญุชูู ุนูู ุตูุฑุฉ ุงููุณุชูุฏ
- `query`: ุณูุณูุฉ ุงูุงุณุชุนูุงู - ุณุคุงู ุงููุบุฉ ุงูุทุจูุนูุฉ ุงููุทุฑูุญุฉุ ุจุนุฏุฉ ูุบุงุช
- `answers`: ูุงุฆูุฉ ุงูุฅุฌุงุจุงุช ุงูุตุญูุญุฉ ุงูุชู ูุฏููุง ูุนูููู ุงูุจูุงูุงุช
- `words` ู`bounding_boxes`: ูุชุงุฆุฌ ุงูุชุนุฑู ุงูุจุตุฑู ุนูู ุงูุญุฑููุ ูุงูุชู ูู ูุณุชุฎุฏููุง ููุง
- `answer`: ุฅุฌุงุจุฉ ุชูุช ูุทุงุจูุชูุง ุจูุงุณุทุฉ ูููุฐุฌ ูุฎุชูู ูู ูุณุชุฎุฏูู ููุง

ุฏุนููุง ูุชุฑู ููุท ุงูุฃุณุฆูุฉ ุจุงููุบุฉ ุงูุฅูุฌููุฒูุฉุ ููููู ุจุฅุณูุงุท ููุฒุฉ "ุงูุฅุฌุงุจุฉ" ุงูุชู ูุจุฏู ุฃููุง ุชุญุชูู ุนูู ุชูุจุคุงุช ุจูููุฐุฌ ุขุฎุฑ. ุณูุฃุฎุฐ ุฃูุถูุง ุงูุฅุฌุงุจุฉ ุงูุฃููู ูู ูุฌููุนุฉ ุงูุฅุฌุงุจุงุช ุงูููุฏูุฉ ูู ุงููุนูููู. ุฃู ููููู ุฃุฎุฐ ุนููุฉ ุนุดูุงุฆูุฉ ูููุง.

```py
>>> updated_dataset = dataset.map(lambda example: {"question": example["query"]["en"]}, remove_columns=["query"])
>>> updated_dataset = updated_dataset.map(
...     lambda example: {"answer": example["answers"][0]}, remove_columns=["answer", "answers"]
... )
```

ูุงุญุธ ุฃู ููุทุฉ ุงูุชุญูู LayoutLMv2 ุงูุชู ูุณุชุฎุฏููุง ูู ูุฐุง ุงูุฏููู ุชู ุชุฏุฑูุจูุง ุจุงุณุชุฎุฏุงู `max_position_embeddings = 512` (ููููู ุงูุนุซูุฑ ุนูู ูุฐู ุงููุนูููุงุช ูู ููู `config.json` ุงูุฎุงุต ุจููุทุฉ ุงูุชุญูู [ููุง](https://huggingface.co/microsoft/layoutlmv2-base-uncased/blob/main/config.json#L18)).

ูููููุง ุชูููุต ุงูุฃูุซูุฉ ูููู ูุชุฌูุจ ุงููููู ุงูุฐู ูุฏ ุชููู ููู ุงูุฅุฌุงุจุฉ ูู ููุงูุฉ ูุณุชูุฏ ุทููู ูุชูุชูู ุจุงูุงูุชุทุงุนุ ููุง ุณูุฒูู ุงูุฃูุซูุฉ ุงูููููุฉ ุงูุชู ูู ุงููุญุชูู ุฃู ููุชูู ูููุง ุชุถููููุง ุฅูู ุฃูุซุฑ ูู 512.

ุฅุฐุง ูุงูุช ูุนุธู ุงููุณุชูุฏุงุช ูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจู ุทูููุฉุ ูููููู ุชูููุฐ ุฅุณุชุฑุงุชูุฌูุฉ ุงููุงูุฐุฉ ุงูููุฒููุฉ - ุฑุงุฌุน [ูุฐุง ุงูุฏูุชุฑ](https://github.com/huggingface/notebooks/blob/main/examples/question_answering.ipynb) ููุญุตูู ุนูู ุงูุชูุงุตูู.

```py
>>> updated_dataset = updated_dataset.filter(lambda x: len(x["words"]) + len(x["question"].split()) < 512)
```

ูู ูุฐู ุงููุฑุญูุฉุ ุฏุนูุง ูุฒูู ุฃูุถูุง ููุฒุงุช ุงูุชุนุฑู ุงูุจุตุฑู ุนูู ุงูุญุฑูู ูู ูุฐู ุงููุฌููุนุฉ ูู ุงูุจูุงูุงุช. ูุฐู ูู ูุชูุฌุฉ ุงูุชุนุฑู ุงูุจุตุฑู ุนูู ุงูุญุฑูู ูุถุจุท ูููุฐุฌ ูุฎุชูู. ูุง ูุฒุงููู ุจุญุงุฌุฉ ุฅูู ุจุนุถ ุงููุนุงูุฌุฉ ุฅุฐุง ุฃุฑุฏูุง ุงุณุชุฎุฏุงููุงุ ุญูุซ ูุง ุชุชุทุงุจู ูุน ูุชุทูุจุงุช ุงูุฅุฏุฎุงู ูููููุฐุฌ ุงูุฐู ูุณุชุฎุฏูู ูู ูุฐุง ุงูุฏููู. ุจุฏูุงู ูู ุฐููุ ูููููุง ุงุณุชุฎุฏุงู [`LayoutLMv2Processor`] ุนูู ุงูุจูุงูุงุช ุงูุฃุตููุฉ ููู ูู ุงูุชุนุฑู ุงูุจุตุฑู ุนูู ุงูุญุฑูู ูุงูุชููููุฒ. ุจูุฐู ุงูุทุฑููุฉุ ุณูุญุตู ุนูู ุงูุฅุฏุฎุงูุงุช ุงูุชู ุชุชุทุงุจู ูุน ุงูุฅุฏุฎุงู ุงููุชููุน ูููููุฐุฌ. ุฅุฐุง ููุช ุชุฑูุฏ ูุนุงูุฌุฉ ุงูุตูุฑ ูุฏูููุงุ ูุฑุงุฌุน ูุซุงุฆู ูููุฐุฌ [`LayoutLMv2`](../model_doc/layoutlmv2) ููุนุฑูุฉ ุชูุณูู ุงูุฅุฏุฎุงู ุงูุฐู ูุชููุนู ุงููููุฐุฌ.

```py
>>> updated_dataset = updated_dataset.remove_columns("words")
>>> updated_dataset = updated_dataset.remove_columns("bounding_boxes")
```

ุฃุฎูุฑูุงุ ูู ุชูุชูู ุงุณุชูุดุงู ุงูุจูุงูุงุช ุฅุฐุง ูู ูููู ูุธุฑุฉ ุนูู ูุซุงู ุตูุฑุฉ.

```py
>>> updated_dataset["train"][11]["image"]
```

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/docvqa_example.jpg" alt="ูุซุงู DocVQA"/>
</div>

## ูุนุงูุฌุฉ ุงูุจูุงูุงุช ูุณุจููุง

ูููุฉ ุงูุฅุฌุงุจุฉ ุนูู ุงูุฃุณุฆูุฉ ุงููุงุฑุฏุฉ ูู ุงููุณุชูุฏ ูู ูููุฉ ูุชุนุฏุฏุฉ ุงููุณุงุฆุทุ ููุฌุจ ุงูุชุฃูุฏ ูู ูุนุงูุฌุฉ ุงูุฅุฏุฎุงูุงุช ูู ูู ูุณูุท ููููุง ูุชููุนุงุช ุงููููุฐุฌ. ุฏุนููุง ูุจุฏุฃ ุจุชุญููู [`LayoutLMv2Processor`]ุ ูุงูุฐู ูุฌูุน ุฏุงุฎูููุง ุจูู ูุนุงูุฌ ุงูุตูุฑ ุงูุฐู ููููู ุงูุชุนุงูู ูุน ุจูุงูุงุช ุงูุตูุฑ ููุนุงูุฌ ุงูุชุฑููุฒ ุงูุฐู ููููู ุชุดููุฑ ุจูุงูุงุช ุงููุต.

```py
>>> from transformers import AutoProcessor

>>> processor = AutoProcessor.from_pretrained(model_checkpoint)
```

### ูุนุงูุฌุฉ ุตูุฑ ุงููุณุชูุฏุงุช

ุฃููุงูุ ุฏุนููุง ูุนุฏ ุตูุฑ ุงููุณุชูุฏุงุช ูููููุฐุฌ ุจูุณุงุนุฏุฉ `image_processor` ูู ุงููุนุงูุฌ.

ุจุดูู ุงูุชุฑุงุถูุ ูููู ูุนุงูุฌ ุงูุตูุฑ ุจุฅุนุงุฏุฉ ุชุญุฌูู ุงูุตูุฑ ุฅูู 224x224ุ ูุงูุชุฃูุฏ ูู ุฃู ูุฏููุง ุงูุชุฑุชูุจ ุงูุตุญูุญ ููููุงุช ุงูุฃููุงูุ ูุชุทุจูู ุงูุชุนุฑู ุงูุจุตุฑู ุนูู ุงูุญุฑูู ุจุงุณุชุฎุฏุงู ุจุฑูุงูุฌ Tesseract ููุญุตูู ุนูู ุงููููุงุช ูุญุฏูุฏ ุงูุฅุญุฏุงุซูุงุช ุงููุนูุงุฑูุฉ. ูู ูุฐุง ุงูุจุฑูุงูุฌ ุงูุชุนููููุ ูู ูุฐู ุงูุงูุชุฑุงุถูุงุช ูู ุจุงูุถุจุท ูุง ูุญุชุงุฌู.

ุงูุชุจ ุฏุงูุฉ ุชุทุจู ูุนุงูุฌุฉ ุงูุตูุฑ ุงูุงูุชุฑุงุถูุฉ ุนูู ุฏูุนุฉ ูู ุงูุตูุฑ ูุชุนูุฏ ูุชุงุฆุฌ ุงูุชุนุฑู ุงูุจุตุฑู ุนูู ุงูุญุฑูู.

```py
>>> image_processor = processor.image_processor


>>> def get_ocr_words_and_boxes(examples):
...     images = [image.convert("RGB") for image in examples["image"]]
...     encoded_inputs = image_processor(images)

...     examples["image"] = encoded_inputs.pixel_values
...     examples["words"] = encoded_inputs.words
...     examples["boxes"] = encoded_inputs.boxes

...     return examples
```

ูุชุทุจูู ูุฐู ุงููุนุงูุฌุฉ ุงููุณุจูุฉ ุนูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุจุฃููููุง ุจุทุฑููุฉ ุณุฑูุนุฉุ ุงุณุชุฎุฏู [`~datasets.Dataset.map`] .

```py
>>> dataset_with_ocr = updated_dataset.map(get_ocr_words_and_boxes, batched=True, batch_size=2)
```
### ูุนุงูุฌุฉ ูุต ุงูุจูุงูุงุช 
ุจุนุฏ ุชุทุจูู ุงูุชุนุฑู ุงูุจุตุฑู ุนูู ุงูุญุฑูู ุนูู ุงูุตูุฑุ ูุญุชุงุฌ ุฅูู ุชุดููุฑ ุงูุฌุฒุก ุงููุตู ูู ูุฌููุนุฉ ุงูุจูุงูุงุช ูุฅุนุฏุงุฏูุง ููููุฐุฌุฉ.
ูููุทูู ุฐูู ุนูู ุชุญููู ุงููููุงุช ูุงูุตูุงุฏูู ุงูุชู ุญุตููุง ุนูููุง ูู ุงูุฎุทูุฉ ุงูุณุงุจูุฉ ุฅูู ูุณุชูู ุงูุฑูุฒ `input_ids` ู`attention_mask` ู`token_type_ids` ู`bbox`. ูููุนุงูุฌุฉ ุงููุตุ ุณูุญุชุงุฌ ุฅูู `tokenizer` ูู ุงููุนุงูุฌ.
```py
>>> tokenizer = processor.tokenizer
```
ุจุงูุฅุถุงูุฉ ุฅูู ุงููุนุงูุฌุฉ ุงููุฐููุฑุฉ ุฃุนูุงูุ ูุญุชุงุฌ ุฃูุถูุง ุฅูู ุฅุถุงูุฉ ุงูุชุณููุงุช ููููุฐุฌุฉ. ูุจุงููุณุจุฉ ูููุงุฐุฌ `xxxForQuestionAnswering` ูู ๐ค Transformersุ ุชุชููู ุงูุชุณููุงุช ูู `start_positions` ู`end_positions`ุ ูุงูุชู ุชุดูุฑ ุฅูู ุงูุฑูุฒ ุงูุฐู ููุซู ุจุฏุงูุฉ ุงูุฅุฌุงุจุฉ ูููุงูุชูุง.
ุฏุนููุง ูุจุฏุฃ ุจุฐูู. ูู ุจุชุนุฑูู ุฏุงูุฉ ูุณุงุนุฏุฉ ูููููุง ุงูุนุซูุฑ ุนูู ูุงุฆูุฉ ูุฑุนูุฉ (ุงูุฅุฌุงุจุฉ ููุณูุฉ ุฅูู ูููุงุช) ูู ูุงุฆูุฉ ุฃูุจุฑ (ูุงุฆูุฉ ุงููููุงุช).
ุณุชุฃุฎุฐ ูุฐู ุงูุฏุงูุฉ ูุงุฆูุชูู ููุฏุฎูุงุชุ `words_list` ู`answer_list`. ุซู ุณุชุชูุฑุฑ ุนุจุฑ `words_list` ูุงูุชุญูู ููุง ุฅุฐุง ูุงูุช ุงููููุฉ ุงูุญุงููุฉ ูู `words_list` (words_list[i]) ุชุณุงูู ุงููููุฉ ุงูุฃููู ูู answer_list (answer_list[0]) ูุฅุฐุง ูุงูุช ุงููุงุฆูุฉ ุงููุฑุนูุฉ ูู `words_list` ุจุฏุกูุง ูู ุงููููุฉ ุงูุญุงููุฉ ูุทูููุง ูุณุงูู ุทูู `answer_list` ูุณุงูู `answer_list`.
ุฅุฐุง ูุงู ูุฐุง ุงูุดุฑุท ุตุญูุญูุงุ ููุฐุง ูุนูู ุฃูู ุชู ุงูุนุซูุฑ ุนูู ุชุทุงุจูุ ูุณุชููู ุงูุฏุงูุฉ ุจุชุณุฌูู ุงูุชุทุงุจู ูููุฑุณ ุงูุจุฏุงูุฉ ุงูุฎุงุต ุจู (idx) ูููุฑุณ ุงูููุงูุฉ (idx + len(answer_list) - 1). ุฅุฐุง ุชู ุงูุนุซูุฑ ุนูู ุฃูุซุฑ ูู ุชุทุงุจู ูุงุญุฏุ ูุณุชุนูุฏ ุงูุฏุงูุฉ ุงูุชุทุงุจู ุงูุฃูู ููุท.
ุฅุฐุง ูู ูุชู ุงูุนุซูุฑ ุนูู ุฃู ุชุทุงุจูุ ุชููู ุงูุฏุงูุฉ ุจุฅุฑุฌุงุน (Noneุ 0ุ ู0).
```py
>>> def subfinder(words_list, answer_list):
...     matches = []
...     start_indices = []
...     end_indices = []
...     for idx, i in enumerate(range(len(words_list))):
...         if words_list[i] == answer_list[0] and words_list[i : i + len(answer_list)] == answer_list:
...             matches.append(answer_list)
...             start_indices.append(idx)
...             end_indices.append(idx + len(answer_list) - 1)
...     if matches:
...         return matches[0], start_indices[0], end_indices[0]
...     else:
...         return None, 0, 0
```
ููุชูุถูุญ ููููุฉ ุนุซูุฑ ูุฐู ุงูุฏุงูุฉ ุนูู ููุถุน ุงูุฅุฌุงุจุฉุ ุฏุนููุง ูุณุชุฎุฏููุง ูู ูุซุงู:
```py
>>> example = dataset_with_ocr["train"][1]
>>> words = [word.lower() for word in example["words"]]
>>> match, word_idx_start, word_idx_end = subfinder(words, example["answer"].lower().split())
>>> print("Question: ", example["question"])
>>> print("Words:", words)
>>> print("Answer: ", example["answer"])
>>> print("start_index", word_idx_start)
>>> print("end_index", word_idx_end)
Question:  ูู ูู ูู ุณู ุณู ูู ูุฐู ุงูุฑุณุงูุฉุ
ุงููููุงุช: ['wie', 'baw', 'brown', '&', 'williamson', 'tobacco', 'corporation', 'research', '&', 'development', 'internal', 'correspondence', 'to:', 'r.', 'h.', 'honeycutt', 'ce:', 't.f.', 'riehl', 'from:', '.', 'c.j.', 'cook', 'date:', 'may', '8,', '1995', 'subject:', 'review', 'of', 'existing', 'brainstorming', 'ideas/483', 'the', 'major', 'function', 'of', 'the', 'product', 'innovation', 'graup', 'is', 'to', 'develop', 'marketable', 'nove!', 'products', 'that', 'would', 'be', 'profitable', 'to', 'manufacture', 'and', 'sell.', 'novel', 'is', 'defined', 'as:', 'of', 'a', 'new', 'kind,', 'or', 'different', 'from', 'anything', 'seen', 'or', 'known', 'before.', 'innovation', 'is', 'defined', 'as:', 'something', 'new', 'or', 'different', 'introduced;', 'act', 'of', 'innovating;', 'introduction', 'of', 'new', 'things', 'or', 'methods.', 'the', 'products', 'may', 'incorporate', 'the', 'latest', 'technologies,', 'materials', 'and', 'know-how', 'available', 'to', 'give', 'then', 'a', 'unique', 'taste', 'or', 'look.', 'the', 'first', 'task', 'of', 'the', 'product', 'innovation', 'group', 'was', 'to', 'assemble,', 'review', 'and', 'categorize', 'a', 'list', 'of', 'existing', 'brainstorm
ing', 'ideas.', 'ideas', 'were', 'grouped', 'into', 'two', 'major', 'categories', 'labeled', 'appearance', 'and', 'taste/aroma.', 'these', 'categories', 'are', 'used', 'for', 'novel', 'products', 'that', 'may', 'differ', 'from', 'a', 'visual', 'and/or', 'taste/aroma', 'point', 'of', 'view', 'compared', 'to', 'canventional', 'cigarettes.', 'other', 'categories', 'include', 'a', 'combination', 'of', 'the', 'above,', 'filters,', 'packaging', 'and', 'brand', 'extensions.', 'appearance', 'this', 'category', 'is', 'used', 'for', 'novel', 'cigarette', 'constructions', 'that', 'yield', 'visually', 'different', 'products', 'with', 'minimal', 'changes', 'in', 'smoke', 'chemistry', 'two', 'cigarettes', 'in', 'cne.', 'emulti-plug', 'te', 'build', 'yaur', 'awn', 'cigarette.', 'eswitchable', 'menthol', 'or', 'non', 'menthol', 'cigarette.', '*cigarettes', 'with', 'interspaced', 'perforations', 'to', 'enable', 'smoker', 'to', 'separate', 'unburned', 'section', 'for', 'future', 'smoking.', 'ยซshort', 'cigarette,', 'tobacco', 'section', '30', 'mm.', 'ยซextremely', 'fast', 'buming', 'cigarette.', 'ยซnovel', 'cigarette', 'constructions', 'that', 'permit', 'a', 'significant', 'reduction', 'iretobacco', 'weight', 'while', 'maintaining', 'smoking', 'mechanics', 'and', 'visual', 'characteristics.', 'higher', 'basis', 'weight', 'paper:', 'potential', 'reduction', 'in', 'tobacco', 'weight.', 'ยซmore', 'rigid', 'tobacco', 'column;', 'stiffing', 'agent', 'for', 'tobacco;', 'e.g.', 'starch', '*colored', 'tow', 'and', 'cigarette', 'papers;', 'seasonal', 'promotions,', 'e.g.', 'pastel', 'colored', 'cigarettes', 'for', 'easter', 'or', 'in', 'an', 'ebony', 'and', 'ivory', 'brand', 'containing', 'a', 'mixture', 'of', 'all', 'black', '(black', 'paper', 'and', 'tow)', 'and', 'ail', 'white', 'cigarettes.']
ุงูุฅุฌุงุจุฉ:  ุช.ู. ุฑููู
ููุฑุณ ุงูุจุฏุงูุฉ 17
ููุฑุณ ุงูููุงูุฉ 18
```
ููุน ุฐููุ ุจุนุฏ ุงูุชุฑููุฒุ ุณุชุจุฏู ุงูุฃูุซูุฉ ุนูู ุงููุญู ุงูุชุงูู:
```py
>>> encoding = tokenizer(example["question"], example["words"], example["boxes"])
>>> tokenizer.decode(encoding["input_ids"])
[CLS] ูู ูู ูู ุณู ุณู ูู ูุฐู ุงูุฑุณุงูุฉุ [SEP] wie baw brown & williamson tobacco corporation research & development ...
```
ุณูุญุชุงุฌ ุฅูู ุงูุนุซูุฑ ุนูู ููุถุน ุงูุฅุฌุงุจุฉ ูู ุงูุฅุฏุฎุงู ุงููุดูุฑ.
* `token_type_ids` ุชุฎุจุฑูุง ุจุงูุฑููุฒ ุงูุชู ุชุนุฏ ุฌุฒุกูุง ูู ุงูุณุคุงูุ ูุฃููุง ุฌุฒุก ูู ูููุงุช ุงููุณุชูุฏ.
* `tokenizer.cls_token_id` ุณูุณุงุนุฏ ูู ุงูุนุซูุฑ ุนูู ุงูุฑูุฒ ุงูุฎุงุต ูู ุจุฏุงูุฉ ุงูุฅุฏุฎุงู.
* `word_ids` ุณุชุณุงุนุฏ ูู ูุทุงุจูุฉ ุงูุฅุฌุงุจุฉ ุงูููุฌูุฏุฉ ูู ุงููููุงุช ุงูุฃุตููุฉ `words` ูุน ููุณ ุงูุฅุฌุงุจุฉ ูู ุงูุฅุฏุฎุงู ุงููุดูุฑ ูุชุญุฏูุฏ ููุถุน ุงูุจุฏุงูุฉ/ุงูููุงูุฉ ููุฅุฌุงุจุฉ ูู ุงูุฅุฏุฎุงู ุงููุดูุฑ.
ูุน ุฃุฎุฐ ุฐูู ูู ุงูุงุนุชุจุงุฑุ ุฏุนููุง ููุดุฆ ุฏุงูุฉ ูุชุดููุฑ ุฏูุนุฉ ูู ุงูุฃูุซูุฉ ูู ูุฌููุนุฉ ุงูุจูุงูุงุช:
```py
>>> def encode_dataset(examples, max_length=512):
...     questions = examples["question"]
...     words = examples["words"]
...     boxes = examples["boxes"]
...     answers = examples["answer"]

...     # ูู ุจุชุดููุฑ ุฏูุนุฉ ุงูุฃูุซูุฉ ูุจุฏุก ุชููุฆุฉ ููุงูุน ุงูุจุฏุงูุฉ ูุงูููุงูุฉ
...     encoding = tokenizer(questions, words, boxes, max_length=max_length, padding="max_length", truncation=True)
...     start_positions = []
...     end_positions = []

...     # ุญูู ุนุจุฑ ุงูุฃูุซูุฉ ูู ุงูุฏูุนุฉ
...     for i in range(len(questions)):
...         cls_index = encoding["input_ids"][i].index(tokenizer.cls_token_id)

...         # ุงูุนุซูุฑ ุนูู ููุถุน ุงูุฅุฌุงุจุฉ ูู ูููุงุช ุงููุซุงู
...         words_example = [word.lower() for word in words[i]]
...         answer = answers[i]
...         match, word_idx_start, word_idx_end = subfinder(words_example, answer.lower().split())

...         if match:
...             # ุฅุฐุง ุชู ุงูุนุซูุฑ ุนูู ุชุทุงุจูุ ุงุณุชุฎุฏู `token_type_ids` ููุนุซูุฑ ุนูู ุงูููุงู ุงูุฐู ุชุจุฏุฃ ููู ุงููููุงุช ูู ุงูุชุฑููุฒ
...             token_type_ids = encoding["token_type_ids"][i]
...             token_start_index = 0
...             while token_type_ids[token_start_index] != 1:
...                 token_start_index += 1

...             token_end_index = len(encoding["input_ids"][i]) - 1
...             while token_type_ids[token_end_index] != 1:
...                 token_end_index -= 1

...             word_ids = encoding.word_ids(i)[token_start_index : token_end_index + 1]
...             start_position = cls_index
...             end_position = cls_index

...             # ูู ุจุงูุญููุฉ ููู word_ids ูุฒูุงุฏุฉ `token_start_index` ุญุชู ุชุชุทุงุจู ูุน ููุถุน ุงูุฅุฌุงุจุฉ ูู ุงููููุงุช
...             # ุจูุฌุฑุฏ ุญุฏูุซ ุงูุชุทุงุจูุ ุงุญูุธ `token_start_index` ูู `start_position` ููุฅุฌุงุจุฉ ูู ุงูุชุฑููุฒ
...             for id in word_ids:
...                 if id == word_idx_start:
...                     start_position = token_start_index
...                 else:
...                     token_start_index += 1

...             # ูุจุงููุซูุ ูู ุจุงูุญููุฉ ููู `word_ids` ุจุฏุกูุง ูู ุงูููุงูุฉ ููุนุซูุฑ ุนูู `end_position` ููุฅุฌุงุจุฉ
...             for id in word_ids[::-1]:
...                 if id == word_idx_end:
...                     end_position = token_end_index
...                 else:
...                     token_end_index -= 1

...             start_positions.append(start_position)
...             end_positions.append(end_position)

...         else:
...             start_positions.append(cls_index)
...             end_positions.append(cls_index)

...     encoding["image"] = examples["image"]
...     encoding["start_positions"] = start_positions
...     encoding["end_positions"] = end_positions

...     return encoding
```
ุงูุขู ุจุนุฏ ุฃู ุฃุตุจุญุช ูุฏููุง ุฏุงูุฉ ุงููุนุงูุฌุฉ ูุฐูุ ูููููุง ุชุดููุฑ ูุฌููุนุฉ ุงูุจูุงูุงุช ุจุฃููููุง:
```py
>>> encoded_train_dataset = dataset_with_ocr["train"].map(
...     encode_dataset, batched=True, batch_size=2, remove_columns=dataset_with_ocr["train"].column_names
... )
>>> encoded_test_dataset = dataset_with_ocr["test"].map(
...     encode_dataset, batched=True, batch_size=2, remove_columns=dataset_with_ocr["test"].column_names
... )
```
ุฏุนููุง ูุชุญูู ูู ููุฒุงุช ูุฌููุนุฉ ุงูุจูุงูุงุช ุงููุดูุฑุฉ:
```py
>>> encoded_train_dataset.features
{'image': Sequence(feature=Sequence(feature=Sequence(feature=Value(dtype='uint8', id=None), length=-1, id=None), length=-1, id=None), length=-1, id=None),
'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),
'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),
'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),
'bbox': Sequence(feature=Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), length=-1, id=None),
'start_positions': Value(dtype='int64', id=None),
'end_positions': Value(dtype='int64', id=None)}
```
## ุงูุชูููู
ูุชุทูุจ ุชูููู ุงูุฅุฌุงุจุฉ ุนูู ุฃุณุฆูุฉ ุงููุณุชูุฏุงุช ูุฏุฑูุง ูุจูุฑูุง ูู ูุง ุจุนุฏ ุงููุนุงูุฌุฉ. ููุชุฌูุจ ุงุณุชุบุฑุงู ุงููุซูุฑ ูู ููุชูุ ูุชุฎุทู ูุฐุง ุงูุฏููู ุฎุทูุฉ ุงูุชูููู. ูุง ูุฒุงู [ูุฏุฑุจ] ูุญุณุจ ุฎุณุงุฑุฉ ุงูุชูููู ุฃุซูุงุก ุงูุชุฏุฑูุจ ุญุชู ูุง ุชููู ูู ุงูุธูุงู ุชูุงููุง ุจุดุฃู ุฃุฏุงุก ูููุฐุฌู. ุนุงุฏุฉู ูุง ูุชู ุชูููู ุงูุฅุฌุงุจุฉ ุงูุงุณุชุฎุฑุงุฌูุฉ ุนูู ุงูุฃุณุฆูุฉ ุจุงุณุชุฎุฏุงู F1/exact match.
ุฅุฐุง ููุช ุชุฑุบุจ ูู ุชูููุฐูุง ุจููุณูุ ูุฑุงุฌุน ูุตู [ุงูุฅุฌุงุจุฉ ุนูู ุงูุฃุณุฆูุฉ](https://huggingface.co/course/chapter7/7?fw=pt#postprocessing) ูู ุฏูุฑุฉ Hugging Face ููุงุณุชููุงู.

## ุงูุชุฏุฑูุจ
ุชูุงูููุง! ููุฏ ูุฌุญุช ูู ุงุฌุชูุงุฒ ุฃุตุนุจ ุฌุฒุก ูู ูุฐุง ุงูุฏูููุ ูุงูุขู ุฃูุช ูุณุชุนุฏ ูุชุฏุฑูุจ ูููุฐุฌู ุงูุฎุงุต.
ูุชุถูู ุงูุชุฏุฑูุจ ุงูุฎุทูุงุช ุงูุชุงููุฉ:
* ูู ุจุชุญููู ุงููููุฐุฌ ุจุงุณุชุฎุฏุงู [`AutoModelForDocumentQuestionAnswering`] ุจุงุณุชุฎุฏุงู ููุณ ููุทุฉ ุงูุชูุชูุด ููุง ูู ูุฑุญูุฉ ูุง ูุจู ุงููุนุงูุฌุฉ.
* ุญุฏุฏ ูุฑุท ูุนููุงุชู ุงูุชุฏุฑูุจูุฉ ูู [`TrainingArguments`].
* ุญุฏุฏ ุฏุงูุฉ ูุฏูุฌ ุงูุฃูุซูุฉ ูุนูุงุ ูููุง ุณุชููู [`DefaultDataCollator`] ุฌูุฏุฉ ุจูุง ููู ุงูููุงูุฉ
* ูู ุจุชูุฑูุฑ ุงูุญุฌุฌ ุงูุชุฏุฑูุจูุฉ ุฅูู [`Trainer`] ุฌูุจูุง ุฅูู ุฌูุจ ูุน ุงููููุฐุฌ ููุฌููุนุฉ ุงูุจูุงูุงุช ููุฌูุน ุงูุจูุงูุงุช.
* ุงุณุชุฏุนุงุก [`~Trainer.train`] ูุถุจุท ูููุฐุฌู ุจุดูู ุฏููู.
```py
>>> from transformers import AutoModelForDocumentQuestionAnswering

>>> model = AutoModelForDocumentQuestionAnswering.from_pretrained(model_checkpoint)
```
ูู [`TrainingArguments`]ุ ุงุณุชุฎุฏู `output_dir` ูุชุญุฏูุฏ ููุงู ุญูุธ ูููุฐุฌูุ ููู ุจุชูููู ูุฑุท ุงููุนููุงุช ููุง ุชุฑุงู ููุงุณุจูุง.
ุฅุฐุง ููุช ุชุฑุบุจ ูู ูุดุงุฑูุฉ ูููุฐุฌู ูุน ุงููุฌุชูุนุ ูุงุถุจุท `push_to_hub` ุนูู `True` (ูุฌุจ ุฃู ุชููู ูุฏ ุณุฌูุช ุงูุฏุฎูู ุฅูู Hugging Face ูุชุญููู ูููุฐุฌู).
ูู ูุฐู ุงูุญุงูุฉุ ุณูููู `output_dir` ุฃูุถูุง ุงุณู ุงููุณุชูุฏุน ุญูุซ ุณูุชู ุฏูุน ููุทุฉ ุชูุชูุด ุงููููุฐุฌ ุงูุฎุงุต ุจู.
```py
>>> from transformers import TrainingArguments

>>> # ุงุณุชุจุฏู ูุฐุง ุจูุนุฑู ุงููุณุชูุฏุน ุงูุฎุงุต ุจู
>>> repo_id = "MariaK/layoutlmv2-base-uncased_finetuned_docvqa"

>>> training_args = TrainingArguments(
...     output_dir=repo_id,
...     per_device_train_batch_size=4,
...     num_train_epochs=20,
...     save_steps=200,
...     logging_steps=50,
...     eval_strategy="steps",
...     learning_rate=5e-5,
...     save_total_limit=2,
...     remove_unused_columns=False,
...     push_to_hub=True,
... )
```
ูู ุจุชุนุฑูู ูุฌูุน ุจูุงูุงุช ุจุณูุท ูุฏูุฌ ุงูุฃูุซูุฉ ูุนูุง.
```py
>>> from transformers import DefaultDataCollator

>>> data_collator = DefaultDataCollator()
```
ุฃุฎูุฑูุงุ ูู ุจุฌูุน ูู ุดูุก ูุนูุงุ ูุงุณุชุฏุนุงุก [`~Trainer.train`]:
```py
>>> from transformers import Trainer

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     data_collator=data_collator,
...     train_dataset=encoded_train_dataset,
...     eval_dataset=encoded_test_dataset,
...     tokenizer=processor,
... )

>>> trainer.train()
```
ูุฅุถุงูุฉ ุงููููุฐุฌ ุงูููุงุฆู ุฅูู ๐ค Hubุ ูู ุจุฅูุดุงุก ุจุทุงูุฉ ูููุฐุฌ ูุงุณุชุฏุนุงุก `push_to_hub`:
```py
>>> trainer.create_model_card()
>>> trainer.push_to_hub()
```

## ุงูุงุณุชูุชุงุฌ
ุงูุขู ุจุนุฏ ุฃู ููุช ุจุชุฏุฑูุจ ูููุฐุฌ LayoutLMv2 ูุชุญูููู ุฅูู ๐ค Hubุ ููููู ุงุณุชุฎุฏุงูู ููุงุณุชูุชุงุฌ. ุฃุจุณุท ุทุฑููุฉ ูุชุฌุฑุจุฉ ูููุฐุฌู ุงููุฏุฑุจ ููุงุณุชูุชุงุฌ ูู ุงุณุชุฎุฏุงูู ูู [`Pipeline`].
ููุฃุฎุฐ ูุซุงูุงู:
```py
>>> example = dataset["test"][2]
>>> question = example["query"]["en"]
>>> image = example["image"]
>>> print(question)
>>> print(example["answers"])
'Who is โpresidingโ TRRF GENERAL SESSION (PART 1)?'
['TRRF Vice President', 'lee a. waller']
```
ุจุนุฏ ุฐููุ ูู ุจุชูููุฐ ุฎุท ุฃูุงุจูุจ ููุฅุฌุงุจุฉ ุนูู ุฃุณุฆูุฉ ุงููุณุชูุฏุงุช ุจุงุณุชุฎุฏุงู ูููุฐุฌูุ ููุฑุฑ ูุฒูุฌ ุงูุตูุฑุฉ + ุงูุณุคุงู ุฅููู.
```py
>>> from transformers import pipeline

>>> qa_pipeline = pipeline("document-question-answering", model="MariaK/layoutlmv2-base-uncased_finetuned_docvqa")
>>> qa_pipeline(image, question)
[{'score': 0.9949808120727539,
'answer': 'Lee A. Waller',
'start': 55,
'end': 57}]
```
ููููู ุฃูุถูุง ูุฏูููุง ุชูุฑุงุฑ ูุชุงุฆุฌ ุฎุท ุงูุฃูุงุจูุจ ุฅุฐุง ููุช ุชุฑุบุจ ูู ุฐูู:
1. ุฎุฐ ุตูุฑุฉ ูุณุคุงูุ ููู ุจุฅุนุฏุงุฏููุง ูููููุฐุฌ ุจุงุณุชุฎุฏุงู ุงููุนุงูุฌ ูู ูููุฐุฌู.
2. ูู ุจุชูุฑูุฑ ูุชูุฌุฉ ูุง ูุจู ุงููุนุงูุฌุฉ ุฃู ูู ุฎูุงู ุงููููุฐุฌ.
3. ูุนูุฏ ุงููููุฐุฌ `start_logits` ู`end_logits`ุ ูุงูุชู ุชุดูุฑ ุฅูู ุงูุฑูุฒ ุงูุฐู ูููู ูู ุจุฏุงูุฉ ุงูุฅุฌุงุจุฉ ูุงูุฑูุฒ ุงูุฐู ูููู ูู ููุงูุฉ ุงูุฅุฌุงุจุฉ. ููุงููุง ูู ุดูู (batch_sizeุ sequence_length).
4. ุฎุฐ argmax ุนูู ุงูุจุนุฏ ุงูุฃุฎูุฑ ูู ูู ูู `start_logits` ู`end_logits` ููุญุตูู ุนูู `start_idx` ุงููุชููุน ู`end_idx`.
5. ูู ุชุดููุฑ ุงูุฅุฌุงุจุฉ ุจุงุณุชุฎุฏุงู ุงููุนุงูุฌ.
```py
>>> import torch
>>> from transformers import AutoProcessor
>>> from transformers import AutoModelForDocumentQuestionAnswering

>>> processor = AutoProcessor.from_pretrained("MariaK/layoutlmv2-base-uncased_finetuned_docvqa")
>>> model = AutoModelForDocumentQuestionAnswering.from_pretrained("MariaK/layoutlmv2-base-uncased_finetuned_docvqa")

>>> with torch.no_grad():
...     encoding = processor(image.convert("RGB"), question, return_tensors="pt")
...     outputs = model(**encoding)
...     start_logits = outputs.start_logits
...     end_logits = outputs.end_logits
...     predicted_start_idx = start_logits.argmax(-1).item()
...     predicted_end_idx = end_logits.argmax(-1).item()

>>> processor.tokenizer.decode(encoding.input_ids.squeeze()[predicted_start_idx : predicted_end_idx + 1])
'lee a. waller'
```