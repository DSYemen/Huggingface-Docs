## ØªØµÙ†ÙŠÙ Ø§Ù„Ø±Ù…ÙˆØ² 

ÙŠÙØ¹ÙŠÙ‘Ù† ØªØµÙ†ÙŠÙ Ø§Ù„Ø±Ù…ÙˆØ² Ø¹Ù„Ø§Ù…Ø© ØªØµÙ†ÙŠÙ Ù„ÙƒÙ„ Ø±Ù…Ø² ÙÙŠ Ø¬Ù…Ù„Ø©. Ø¥Ø­Ø¯Ù‰ Ù…Ù‡Ø§Ù… ØªØµÙ†ÙŠÙ Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© Ù‡ÙŠ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³Ù…Ø§Ø© (NER). ØªØ­Ø§ÙˆÙ„ NER Ø¥ÙŠØ¬Ø§Ø¯ ØªØ³Ù…ÙŠØ© Ù„ÙƒÙ„ ÙƒÙŠØ§Ù† ÙÙŠ Ø¬Ù…Ù„Ø©ØŒ Ù…Ø«Ù„ Ø´Ø®Øµ Ø£Ùˆ Ù…ÙˆÙ‚Ø¹ Ø£Ùˆ Ù…Ù†Ø¸Ù…Ø©.

Ø³ÙŠÙˆØ¶Ø­ Ù„Ùƒ Ù‡Ø°Ø§ Ø§Ù„Ø¯Ù„ÙŠÙ„ ÙƒÙŠÙÙŠØ©:

1. Ø¶Ø¨Ø· Ø¯Ù‚Ø© [DistilBERT](https://huggingface.co/distilbert/distilbert-base-uncased) Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª [WNUT 17](https://huggingface.co/datasets/wnut_17) Ù„Ù„ÙƒØ´Ù Ø¹Ù† ÙƒÙŠØ§Ù†Ø§Øª Ø¬Ø¯ÙŠØ¯Ø©.
2. Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¶Ø¨ÙˆØ· Ø¯Ù‚Ø© Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ Ù„Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬.

Ù‚Ø¨Ù„ Ø£Ù† ØªØ¨Ø¯Ø£ØŒ ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ©:

```bash
pip install transformers datasets evaluate seqeval
```

Ù†Ø­Ù† Ù†Ø´Ø¬Ø¹Ùƒ Ø¹Ù„Ù‰ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø¥Ù„Ù‰ Ø­Ø³Ø§Ø¨ Hugging Face Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ Ø­ØªÙ‰ ØªØªÙ…ÙƒÙ† Ù…Ù† ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬Ùƒ ÙˆÙ…Ø´Ø§Ø±ÙƒØªÙ‡ Ù…Ø¹ Ø§Ù„Ù…Ø¬ØªÙ…Ø¹. Ø¹Ù†Ø¯Ù…Ø§ ÙŠÙØ·Ù„Ø¨ Ù…Ù†Ùƒ Ø°Ù„ÙƒØŒ Ø£Ø¯Ø®Ù„ Ø±Ù…Ø²Ùƒ Ù„Ù„ØªØ³Ø¬ÙŠÙ„:

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

## ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª WNUT 17 

Ø§Ø¨Ø¯Ø£ Ø¨ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª WNUT 17 Ù…Ù† Ù…ÙƒØªØ¨Ø© Datasets ğŸ¤—:

```py
>>> from datasets import load_dataset

>>> wnut = load_dataset("wnut_17")
```

Ø«Ù… Ø§Ù„Ù‚ Ù†Ø¸Ø±Ø© Ø¹Ù„Ù‰ Ù…Ø«Ø§Ù„:

```py
>>> wnut["train"][0]
{'id': '0',
'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0],
'tokens': ['@paulwalk', 'It', "'s", 'the', 'view', 'from', 'where', 'I', "'m", 'living', 'for', 'two', 'weeks', '.', 'Empire', 'State', 'Building', '=', 'ESB', '.', 'Pretty', 'bad', 'storm', 'here', 'last', 'evening', '.']
}
```

ÙŠÙ…Ø«Ù„ ÙƒÙ„ Ø±Ù‚Ù… ÙÙŠ `ner_tags` ÙƒÙŠØ§Ù†Ù‹Ø§. Ù‚Ù… Ø¨ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø¥Ù„Ù‰ Ø£Ø³Ù…Ø§Ø¡ ØªØ³Ù…ÙŠØ§ØªÙ‡Ø§ Ù„Ù…Ø¹Ø±ÙØ© Ù…Ø§Ù‡ÙŠØ© Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª:

```py
>>> label_list = wnut["train"].features[f"ner_tags"].feature.names
>>> label_list
[
"O",
"B-corporation",
"I-corporation",
"B-creative-work",
"I-creative-work",
"B-group",
"I-group",
"B-location",
"I-location",
"B-person",
"I-person",
"B-product",
"I-product",
]
```

ÙŠØ´ÙŠØ± Ø§Ù„Ø­Ø±Ù Ø§Ù„Ø°ÙŠ ÙŠØ³Ø¨Ù‚ ÙƒÙ„ `ner_tag` Ø¥Ù„Ù‰ Ù…ÙˆØ¶Ø¹ Ø§Ù„Ø±Ù…Ø² Ø§Ù„Ø®Ø§Øµ Ø¨Ø§Ù„ÙƒÙŠØ§Ù†:

- `B-` ÙŠØ´ÙŠØ± Ø¥Ù„Ù‰ Ø¨Ø¯Ø§ÙŠØ© Ø§Ù„ÙƒÙŠØ§Ù†.
- `I-` ÙŠØ´ÙŠØ± Ø¥Ù„Ù‰ Ø£Ù† Ø§Ù„Ø±Ù…Ø² Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø®Ù„ Ù†ÙØ³ Ø§Ù„ÙƒÙŠØ§Ù† (Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ø±Ù…Ø² `State` Ù‡Ùˆ Ø¬Ø²Ø¡ Ù…Ù† ÙƒÙŠØ§Ù† Ù…Ø«Ù„ `Empire State Building`).
- `0` ÙŠØ´ÙŠØ± Ø¥Ù„Ù‰ Ø£Ù† Ø§Ù„Ø±Ù…Ø² Ù„Ø§ ÙŠÙ‚Ø§Ø¨Ù„ Ø£ÙŠ ÙƒÙŠØ§Ù†.

## Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø³Ø¨Ù‚Ø© 

Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„ØªØ§Ù„ÙŠØ© Ù‡ÙŠ ØªØ­Ù…ÙŠÙ„ Ù…Ø¹Ø§Ù„Ø¬ Ø±Ù…ÙˆØ² DistilBERT Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø­Ù‚Ù„ `tokens`:

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("distilbert/distilbert-base-uncased")
```

ÙƒÙ…Ø§ Ø±Ø£ÙŠØª ÙÙŠ Ø­Ù‚Ù„ `tokens` Ø§Ù„Ù…Ø«Ø§Ù„ Ø£Ø¹Ù„Ø§Ù‡ØŒ ÙŠØ¨Ø¯Ùˆ Ø£Ù† Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ù‚Ø¯ ØªÙ… ØªÙ‚Ø³ÙŠÙ…Ù‡Ø§ Ø¥Ù„Ù‰ Ø±Ù…ÙˆØ² Ø¨Ø§Ù„ÙØ¹Ù„. Ù„ÙƒÙ† Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ù„Ù… ÙŠØªÙ… ØªÙ‚Ø³ÙŠÙ…Ù‡Ø§ Ø¥Ù„Ù‰ Ø±Ù…ÙˆØ² Ø¨Ø§Ù„ÙØ¹Ù„ØŒ ÙˆØ³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ¹ÙŠÙŠÙ† `is_split_into_words=True` Ù„ØªÙ‚Ø³ÙŠÙ… Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø¥Ù„Ù‰ Ø±Ù…ÙˆØ² ÙØ±Ø¹ÙŠØ©. Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„:

```py
>>> example = wnut["train"][0]
>>> tokenized_input = tokenizer(example["tokens"], is_split_into_words=True)
>>> tokens = tokenizer.convert_ids_to_tokens(tokenized_input["input_ids"])
>>> tokens
['[CLS]', '@', 'paul', '##walk', 'it', "'", 's', 'the', 'view', 'from', 'where', 'i', "'", 'm', 'living', 'for', 'two', 'weeks', '.', 'empire', 'state', 'building', '=', 'es', '##b', '.', 'pretty', 'bad', 'storm', 'here', 'last', 'evening', '.', '[SEP]']
```

ÙˆÙ…Ø¹ Ø°Ù„ÙƒØŒ ÙØ¥Ù† Ù‡Ø°Ø§ ÙŠØ¶ÙŠÙ Ø¨Ø¹Ø¶ Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø®Ø§ØµØ© `[CLS]` Ùˆ`[SEP]`ØŒ ÙˆØªØ¤Ø¯ÙŠ ØªÙ‚Ø³ÙŠÙ… Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø¥Ù„Ù‰ Ø±Ù…ÙˆØ² ÙØ±Ø¹ÙŠØ© Ø¥Ù„Ù‰ Ø¹Ø¯Ù… ØªØ·Ø§Ø¨Ù‚ Ø¨ÙŠÙ† Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª ÙˆØ§Ù„ØªØ³Ù…ÙŠØ§Øª. Ù‚Ø¯ ØªÙ†Ù‚Ø³Ù… Ø§Ù„Ø¢Ù† ÙƒÙ„Ù…Ø© ÙˆØ§Ø­Ø¯Ø© ØªÙ‚Ø§Ø¨Ù„ ØªØ³Ù…ÙŠØ© ÙˆØ§Ø­Ø¯Ø© Ø¥Ù„Ù‰ Ø±Ù…Ø²ÙŠÙ† ÙØ±Ø¹ÙŠÙŠÙ†. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ø¹Ø§Ø¯Ø© Ù…Ø­Ø§Ø°Ø§Ø© Ø§Ù„Ø±Ù…ÙˆØ² ÙˆØ§Ù„Ø¹Ù„Ø§Ù…Ø§Øª Ù…Ù† Ø®Ù„Ø§Ù„:

1. Ù‚Ù… Ø¨ØªØ¹ÙŠÙŠÙ† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø±Ù…ÙˆØ² Ø¥Ù„Ù‰ ÙƒÙ„Ù…Ø§ØªÙ‡Ø§ Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø·Ø±ÙŠÙ‚Ø© [`word_ids`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.BatchEncoding.word_ids).
2. Ù‚Ù… Ø¨ØªØ¹ÙŠÙŠÙ† Ø§Ù„ØªØ³Ù…ÙŠØ© `-100` Ø¥Ù„Ù‰ Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø®Ø§ØµØ© `[CLS]` Ùˆ`[SEP]` Ø­ØªÙ‰ ÙŠØªÙ… ØªØ¬Ø§Ù‡Ù„Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø¯Ø§Ù„Ø© Ø§Ù„Ø®Ø³Ø§Ø±Ø© PyTorch (Ø§Ù†Ø¸Ø± [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)).
3. Ù‚Ù… Ø¨ØªØ³Ù…ÙŠØ© Ø§Ù„Ø±Ù…Ø² Ø§Ù„Ø£ÙˆÙ„ Ù„Ù„ÙƒÙ„Ù…Ø© ÙÙ‚Ø·. Ù‚Ù… Ø¨ØªØ¹ÙŠÙŠÙ† `-100` Ø¥Ù„Ù‰ Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„ÙØ±Ø¹ÙŠØ© Ø§Ù„Ø£Ø®Ø±Ù‰ Ù…Ù† Ù†ÙØ³ Ø§Ù„ÙƒÙ„Ù…Ø©.

Ù‡Ù†Ø§ ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ù†Ø´Ø§Ø¡ ÙˆØ¸ÙŠÙØ© Ù„Ø¥Ø¹Ø§Ø¯Ø© Ù…Ø­Ø§Ø°Ø§Ø© Ø§Ù„Ø±Ù…ÙˆØ² ÙˆØ§Ù„Ø¹Ù„Ø§Ù…Ø§ØªØŒ ÙˆÙ‚Øµ Ø§Ù„ØªØ³Ù„Ø³Ù„Ø§Øª Ø¨Ø­ÙŠØ« Ù„Ø§ ØªÙƒÙˆÙ† Ø£Ø·ÙˆÙ„ Ù…Ù† Ø·ÙˆÙ„ Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù€ DistilBERT:

```py
>>> def tokenize_and_align_labels(examples):
...     tokenized_inputs = tokenizer(examples["tokens"], truncation=True, is_split_into_words=True)

...     labels = []
...     for i, label in enumerate(examples[f"ner_tags"]):
...         word_ids = tokenized_inputs.word_ids(batch_index=i)  # Ù‚Ù… Ø¨ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ø±Ù…ÙˆØ² Ø¥Ù„Ù‰ ÙƒÙ„Ù…Ø§ØªÙ‡Ø§ Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„Ø©.
...         previous_word_idx = None
...         label_ids = []
...         for word_idx in word_ids:  # Ù‚Ù… Ø¨ØªØ¹ÙŠÙŠÙ† Ø§Ù„ØªØ³Ù…ÙŠØ§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¥Ù„Ù‰ -100.
...             if word_idx is None:
...                 label_ids.append(-100)
...             elif word_idx != previous_word_idx:  # Ù‚Ù… Ø¨ØªØ³Ù…ÙŠØ© Ø§Ù„Ø±Ù…Ø² Ø§Ù„Ø£ÙˆÙ„ Ù„Ù„ÙƒÙ„Ù…Ø© ÙÙ‚Ø·.
...                 label_ids.append(label[word_idx])
...             else:
...                 label_ids.append(-100)
...             previous_word_idx = word_idx
...         labels.append(label_ids)

...     tokenized_inputs["labels"] = labels
...     return tokenized_inputs
```

Ù„ØªØ·Ø¨ÙŠÙ‚ ÙˆØ¸ÙŠÙØ© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³Ø¨Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø£ÙƒÙ…Ù„Ù‡Ø§ØŒ Ø§Ø³ØªØ®Ø¯Ù… ÙˆØ¸ÙŠÙØ© [`~datasets.Dataset.map`] ÙÙŠ Ù…ÙƒØªØ¨Ø© Datasets ğŸ¤—. ÙŠÙ…ÙƒÙ†Ùƒ ØªØ³Ø±ÙŠØ¹ ÙˆØ¸ÙŠÙØ© `map` Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªØ¹ÙŠÙŠÙ† `batched=True` Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¹Ù†Ø§ØµØ± Ù…ØªØ¹Ø¯Ø¯Ø© Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ ÙˆÙ‚Øª ÙˆØ§Ø­Ø¯:

```py
>>> tokenized_wnut = wnut.map(tokenize_and_align_labels, batched=True)
```

Ø§Ù„Ø¢Ù† Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ø¯ÙØ¹Ø© Ù…Ù† Ø§Ù„Ø£Ù…Ø«Ù„Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`DataCollatorWithPadding`]. Ù…Ù† Ø§Ù„Ø£ÙƒØ«Ø± ÙƒÙØ§Ø¡Ø© *Ø§Ù„ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ* Ù„Ù„Ø¬Ù…Ù„ Ø¥Ù„Ù‰ Ø£Ø·ÙˆÙ„ Ø·ÙˆÙ„ ÙÙŠ Ø¯ÙØ¹Ø© Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø¬Ù…Ø¹ØŒ Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† ØªÙ‚Ø³ÙŠÙ… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø£ÙƒÙ…Ù„Ù‡Ø§ Ø¥Ù„Ù‰ Ø§Ù„Ø·ÙˆÙ„ Ø§Ù„Ø£Ù‚ØµÙ‰.

<frameworkcontent>

<pt>

```py
>>> from transformers import DataCollatorForTokenClassification

>>> data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)
```

</pt>

<tf>

```py
>>> from transformers import DataCollatorForTokenClassification

>>> data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, return_tensors="tf")
```

</tf>

</frameworkcontent>

## ØªÙ‚ÙŠÙŠÙ… 

ØºØ§Ù„Ø¨Ù‹Ø§ Ù…Ø§ ÙŠÙƒÙˆÙ† Ù…Ù† Ø§Ù„Ù…ÙÙŠØ¯ ØªØ¶Ù…ÙŠÙ† Ù…Ù‚ÙŠØ§Ø³ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù„ØªÙ‚ÙŠÙŠÙ… Ø£Ø¯Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬Ùƒ. ÙŠÙ…ÙƒÙ†Ùƒ ØªØ­Ù…ÙŠÙ„ Ø·Ø±ÙŠÙ‚Ø© ØªÙ‚ÙŠÙŠÙ… Ø¨Ø³Ø±Ø¹Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙƒØªØ¨Ø© ğŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index). Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù‡Ø°Ù‡ Ø§Ù„Ù…Ù‡Ù…Ø©ØŒ Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø¥Ø·Ø§Ø± Ø¹Ù…Ù„ [seqeval](https://huggingface.co/spaces/evaluate-metric/seqeval) (Ø±Ø§Ø¬Ø¹ Ø¬ÙˆÙ„Ø© ğŸ¤— Evaluate [Ø§Ù„Ø³Ø±ÙŠØ¹Ø©](https://huggingface.co/docs/evaluate/a_quick_tour) Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…Ø²ÙŠØ¯ Ø­ÙˆÙ„ ÙƒÙŠÙÙŠØ© ØªØ­Ù…ÙŠÙ„ ÙˆØ­Ø³Ø§Ø¨ Ù…Ù‚ÙŠØ§Ø³). ÙÙŠ Ø§Ù„ÙˆØ§Ù‚Ø¹ØŒ ÙŠÙ†ØªØ¬ Seqeval Ø¹Ø¯Ø© Ø¯Ø±Ø¬Ø§Øª: Ø§Ù„Ø¯Ù‚Ø© ÙˆØ§Ù„Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ ÙˆF1 ÙˆØ§Ù„Ø¯Ù‚Ø©.

```py
>>> import evaluate

>>> seqeval = evaluate.load("seqeval")
```

Ø§Ø­ØµÙ„ Ø¹Ù„Ù‰ ØªØ³Ù…ÙŠØ§Øª NER Ø£ÙˆÙ„Ø§Ù‹ØŒ Ø«Ù… Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ ÙˆØ¸ÙŠÙØ© ØªÙ…Ø±Ø± ØªÙ†Ø¨Ø¤Ø§ØªÙƒ Ø§Ù„ØµØ­ÙŠØ­Ø© ÙˆØªØ³Ù…ÙŠØ§ØªÙƒ Ø§Ù„ØµØ­ÙŠØ­Ø© Ø¥Ù„Ù‰ [`~evaluate.EvaluationModule.compute`] Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¯Ø±Ø¬Ø§Øª:

```py
>>> import numpy as np

>>> labels = [label_list[i] for i in example[f"ner_tags"]]


>>> def compute_metrics(p):
...     predictions, labels = p
...     predictions = np.argmax(predictions, axis=2)

...     true_predictions = [
...         [label_list[p] for (p, l) in zip(prediction, label) if l != -100]
...         for prediction, label in zip(predictions, labels)
...     ]
...     true_labels = [
...         [label_list[l] for (p, l) in zip(prediction, label) if l != -100]
...         for prediction, label in zip(predictions, labels)
...     ]

...     results = seqeval.compute(predictions=true_predictions, references=true_labels)
...     return {
...         "precision": results["overall_precision"],
...         "recall": results["overall_recall"],
...         "f1": results["overall_f1"],
...         "accuracy": results["overall_accuracy"],
...     }
```

ÙˆØ¸ÙŠÙØ© `compute_metrics` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ Ø¬Ø§Ù‡Ø²Ø© Ø§Ù„Ø¢Ù†ØŒ ÙˆØ³ØªØ¹ÙˆØ¯ Ø¥Ù„ÙŠÙ‡Ø§ Ø¹Ù†Ø¯ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ.
## Ø§Ù„ØªØ¯Ø±ÙŠØ¨

Ù‚Ø¨Ù„ Ø§Ù„Ø¨Ø¯Ø¡ ÙÙŠ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø®Ø§Øµ Ø¨ÙƒØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ù…Ù† Ø§Ù„Ù…Ø¹Ø±ÙØ§Øª Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø© Ø¥Ù„Ù‰ ØªØ³Ù…ÙŠØ§ØªÙ‡Ø§ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… `id2label` Ùˆ `label2id`:

```python
>>> id2label = {
...     0: "O",
...     1: "B-corporation",
...     2: "I-corporation",
...     3: "B-creative-work",
...     4: "I-creative-work",
...     5: "B-group",
...     6: "I-group",
...     7: "B-location",
...     8: "I-location",
...     9: "B-person",
...     10: "I-person",
...     11: "B-product",
...     12: "I-product",
... }
>>> label2id = {
...     "O": 0,
...     "B-corporation": 1,
...     "I-corporation": 2,
...     "B-creative-work": 3,
...     "I-creative-work": 4,
...     "B-group": 5,
...     "I-group": 6,
...     "B-location": 7,
...     "I-location": 8,
...     "B-person": 9,
...     "I-person": 10,
...     "B-product": 11,
...     "I-product": 12,
... }
```

<frameworkcontent>
<pt>
<Tip>
Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…Ø¹ØªØ§Ø¯Ù‹Ø§ Ø¹Ù„Ù‰ Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`Trainer`]ØŒ ÙØ±Ø§Ø¬Ø¹ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ [Ù‡Ù†Ø§] (../training # train-with-pytorch-trainer)
</Tip>

Ø£Ù†Øª Ø§Ù„Ø¢Ù† Ø¹Ù„Ù‰ Ø§Ø³ØªØ¹Ø¯Ø§Ø¯ Ù„Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ! Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ DistilBERT Ù…Ø¹ [`AutoModelForTokenClassification`] Ø¬Ù†Ø¨Ù‹Ø§ Ø¥Ù„Ù‰ Ø¬Ù†Ø¨ Ù…Ø¹ Ø¹Ø¯Ø¯ Ø§Ù„ØªØ³Ù…ÙŠØ§Øª Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©ØŒ ÙˆØ®Ø±Ø§Ø¦Ø· Ø§Ù„ØªØ³Ù…ÙŠØ§Øª:

```python
>>> from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer

>>> model = AutoModelForTokenClassification.from_pretrained(
...     "distilbert/distilbert-base-uncased", num_labels=13, id2label=id2label, label2id=label2id
... )
```

ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø±Ø­Ù„Ø©ØŒ Ù„Ù… ÙŠØªØ¨Ù‚ Ø³ÙˆÙ‰ Ø«Ù„Ø§Ø« Ø®Ø·ÙˆØ§Øª:

1. Ø­Ø¯Ø¯ ÙØ±Ø· Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ ÙÙŠ [`TrainingArguments`]. Ø§Ù„Ù…Ø¹Ù„Ù…Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ø§Ù„ÙˆØ­ÙŠØ¯Ø© Ù‡ÙŠ `output_dir` Ø§Ù„ØªÙŠ ØªØ­Ø¯Ø¯ Ø£ÙŠÙ† ÙŠØªÙ… Ø­ÙØ¸ Ù†Ù…ÙˆØ°Ø¬Ùƒ. Ø³ØªÙ‚ÙˆÙ… Ø¨Ø§Ù„Ø¯ÙØ¹ Ø¨Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù„Ù‰ Hub Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªØ¹ÙŠÙŠÙ† `push_to_hub=True` (ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ù…Ø³Ø¬Ù„Ø§Ù‹ Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø¥Ù„Ù‰ Hugging Face Ù„ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬Ùƒ). ÙÙŠ Ù†Ù‡Ø§ÙŠØ© ÙƒÙ„ Ø­Ù‚Ø¨Ø©ØŒ Ø³ÙŠÙ‚ÙˆÙ… [`Trainer`] Ø¨ØªÙ‚ÙŠÙŠÙ… Ø¯Ø±Ø¬Ø§Øª seqeval ÙˆØ­ÙØ¸ Ù†Ù‚Ø·Ø© ØªÙØªÙŠØ´ Ø§Ù„ØªØ¯Ø±ÙŠØ¨.

2. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø§Ù„Ø­Ø¬Ø¬ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ÙŠØ© Ø¥Ù„Ù‰ [`Trainer`] Ø¬Ù†Ø¨Ù‹Ø§ Ø¥Ù„Ù‰ Ø¬Ù†Ø¨ Ù…Ø¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆÙ…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ù„Ù…Ø­Ù„Ù„ Ø§Ù„Ù„ØºÙˆÙŠ ÙˆÙ…Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆÙˆØ¸ÙŠÙØ© `compute_metrics`.

3. Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ [`~Trainer.train`] Ù„Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬Ùƒ.

```python
>>> training_args = TrainingArguments(
...     output_dir="my_awesome_wnut_model"ØŒ
...     learning_rate=2e-5ØŒ
...     per_device_train_batch_size=16ØŒ
...     per_device_eval_batch_size=16ØŒ
...     num_train_epochs=2ØŒ
...     weight_decay=0.01ØŒ
...     eval_strategy="epoch"ØŒ
...     save_strategy="epoch"ØŒ
...     load_best_model_at_end=TrueØŒ
...     push_to_hub=TrueØŒ
... )

>>> trainer = Trainer(
...     model=modelØŒ
...     args=training_argsØŒ
...     train_dataset=tokenized_wnut["train"]ØŒ
...     eval_dataset=tokenized_wnut["test"]ØŒ
...     tokenizer=tokenizerØŒ
...     data_collator=data_collatorØŒ
...     compute_metrics=compute_metricsØŒ
... )

>>> trainer.train()
```

Ø¨Ù…Ø¬Ø±Ø¯ Ø§ÙƒØªÙ…Ø§Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ØŒ Ø´Ø§Ø±Ùƒ Ù†Ù…ÙˆØ°Ø¬Ùƒ Ù…Ø¹ Hub Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø·Ø±ÙŠÙ‚Ø© [`~transformers.Trainer.push_to_hub`] Ø­ØªÙ‰ ÙŠØªÙ…ÙƒÙ† Ø§Ù„Ø¬Ù…ÙŠØ¹ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬Ùƒ:

```python
>>> trainer.push_to_hub()
```

</pt>
<tf>
<Tip>
Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…Ø¹ØªØ§Ø¯Ù‹Ø§ Ø¹Ù„Ù‰ Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… KerasØŒ ÙØ±Ø§Ø¬Ø¹ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ [Ù‡Ù†Ø§] (../training # train-a-tensorflow-model-with-keras)
</Tip>

Ù„Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ TensorFlowØŒ Ø§Ø¨Ø¯Ø£ Ø¨Ø¥Ø¹Ø¯Ø§Ø¯ Ø¯Ø§Ù„Ø© Ù…Ø­Ø³Ù†ØŒ ÙˆØ¬Ø¯ÙˆÙ„ Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù…ØŒ ÙˆØ¨Ø¹Ø¶ ÙØ±Ø· Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨:

```python
>>> from transformers import create_optimizer

>>> batch_size = 16
>>> num_train_epochs = 3
>>> num_train_steps = (len (tokenized_wnut ["train"]) // batch_size) * num_train_epochs
>>> optimizerØŒ lr_schedule = create_optimizer (
...     init_lr=2e-5ØŒ
...     num_train_steps=num_train_stepsØŒ
...     weight_decay_rate=0.01ØŒ
...     num_warmup_steps=0ØŒ
... )
```

Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ­Ù…ÙŠÙ„ DistilBERT Ù…Ø¹ [`TFAutoModelForTokenClassification`] Ø¬Ù†Ø¨Ù‹Ø§ Ø¥Ù„Ù‰ Ø¬Ù†Ø¨ Ù…Ø¹ Ø¹Ø¯Ø¯ Ø§Ù„ØªØ³Ù…ÙŠØ§Øª Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©ØŒ ÙˆØ®Ø±Ø§Ø¦Ø· Ø§Ù„ØªØ³Ù…ÙŠØ§Øª:

```python
>>> from transformers import TFAutoModelForTokenClassification

>>> model = TFAutoModelForTokenClassification.from_pretrained(
...     "distilbert/distilbert-base-uncased"ØŒ num_labels=13ØŒ id2label=id2labelØŒ label2id=label2id
... )
```

Ù‚Ù… Ø¨ØªØ­ÙˆÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ `tf.data.Dataset` Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`~transformers.TFPreTrainedModel.prepare_tf_dataset`]:

```python
>>> tf_train_set = model.prepare_tf_dataset(
...     tokenized_wnut ["train"]ØŒ
...     shuffle=TrueØŒ
...     batch_size=16ØŒ
...     collate_fn=data_collatorØŒ
... )

>>> tf_validation_set = model.prepare_tf_dataset(
...     tokenized_wnut ["validation"]ØŒ
...     shuffle=FalseØŒ
...     batch_size=16ØŒ
...     collate_fn=data_collatorØŒ
... )
```

Ù‚Ù… Ø¨ØªÙƒÙˆÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`compile`] (https://keras.io/api/models/model_training_apis/#compile-method). Ù„Ø§Ø­Ø¸ Ø£Ù† Ø¬Ù…ÙŠØ¹ Ù†Ù…Ø§Ø°Ø¬ Transformers Ø¨Ù‡Ø§ Ø¯Ø§Ù„Ø© Ø®Ø³Ø§Ø±Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø°Ø§Øª ØµÙ„Ø© Ø¨Ø§Ù„Ù…Ù‡Ù…Ø©ØŒ Ù„Ø°Ù„Ùƒ Ù„Ø§ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ­Ø¯ÙŠØ¯ ÙˆØ§Ø­Ø¯Ø© Ù…Ø§ Ù„Ù… ØªØ±ØºØ¨ ÙÙŠ Ø°Ù„Ùƒ:

```python
>>> import tensorflow as tf

>>> model.compile(optimizer=optimizer) # Ù„Ø§ ØªÙˆØ¬Ø¯ Ø­Ø¬Ø© Ø§Ù„Ø®Ø³Ø§Ø±Ø©!
```

Ø§Ù„Ø£Ù…Ø±Ø§Ù† Ø§Ù„Ø£Ø®ÙŠØ±Ø§Ù† Ø§Ù„Ù„Ø°Ø§Ù† ÙŠØ¬Ø¨ Ø¥Ø¹Ø¯Ø§Ø¯Ù‡Ù…Ø§ Ù‚Ø¨Ù„ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù‡Ù…Ø§ Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø§Øª seqeval Ù…Ù† Ø§Ù„ØªÙˆÙ‚Ø¹Ø§ØªØŒ ÙˆØªÙˆÙÙŠØ± Ø·Ø±ÙŠÙ‚Ø© Ù„Ø¯ÙØ¹ Ù†Ù…ÙˆØ°Ø¬Ùƒ Ø¥Ù„Ù‰ Hub. ÙŠØªÙ… ØªÙ†ÙÙŠØ° ÙƒÙ„Ø§Ù‡Ù…Ø§ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [Keras callbacks] (../main_classes/keras_callbacks).

Ù…Ø±Ø± Ø¯Ø§Ù„ØªÙƒ `compute_metrics` Ø¥Ù„Ù‰ [`~transformers.KerasMetricCallback`]:

```python
>>> from transformers.keras_callbacks import KerasMetricCallback

>>> metric_callback = KerasMetricCallback(metric_fn=compute_metricsØŒ eval_dataset=tf_validation_set)
```

Ø­Ø¯Ø¯ Ø£ÙŠÙ† ØªØ¯ÙØ¹ Ù†Ù…ÙˆØ°Ø¬Ùƒ ÙˆÙ…Ø­Ù„Ù„ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ ÙÙŠ [`~transformers.PushToHubCallback`]:

```python
>>> from transformers.keras_callbacks import PushToHubCallback

>>> push_to_hub_callback = PushToHubCallback(
...     output_dir="my_awesome_wnut_model"ØŒ
...     tokenizer=tokenizerØŒ
... )
```

Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ Ù‚Ù… Ø¨ØªØ¬Ù…ÙŠØ¹ Ù…ÙƒØ§Ù„Ù…Ø§ØªÙƒ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰:

```python
>>> callbacks = [metric_callbackØŒ push_to_hub_callback]
```

Ø£Ø®ÙŠØ±Ù‹Ø§ØŒ Ø£Ù†Øª Ø¹Ù„Ù‰ Ø§Ø³ØªØ¹Ø¯Ø§Ø¯ Ù„Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬Ùƒ! Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ [`fit`] (https://keras.io/api/models/model_training_apis/#fit-method) Ù…Ø¹ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ÙŠØ© ÙˆØ§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­ØªÙ‡Ø§ØŒ ÙˆØ¹Ø¯Ø¯ Ø§Ù„Ø¹ØµÙˆØ±ØŒ ÙˆÙ…ÙƒØ§Ù„Ù…Ø§ØªÙƒ Ù„Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬Ùƒ:

```python
>>> model.fit(x=tf_train_setØŒ validation_data=tf_validation_setØŒ epochs=3ØŒ callbacks=callbacks)
```

Ø¨Ù…Ø¬Ø±Ø¯ Ø§ÙƒØªÙ…Ø§Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ØŒ ÙŠØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬Ùƒ ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ Ø¥Ù„Ù‰ Hub Ø­ØªÙ‰ ÙŠØªÙ…ÙƒÙ† Ø§Ù„Ø¬Ù…ÙŠØ¹ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡!

</tf>
</frameworkcontent>

<Tip>
Ù„Ù…Ø«Ø§Ù„ Ø£ÙƒØ«Ø± Ø¹Ù…Ù‚Ù‹Ø§ Ø­ÙˆÙ„ ÙƒÙŠÙÙŠØ© Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø±Ù…Ø²ÙŠØŒ Ø±Ø§Ø¬Ø¹ Ø§Ù„Ø¯ÙØªØ± Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„
[Ø¯ÙØªØ± Ù…Ù„Ø§Ø­Ø¸Ø§Øª PyTorch] (https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification.ipynb)
Ø£Ùˆ [Ø¯ÙØªØ± Ù…Ù„Ø§Ø­Ø¸Ø§Øª TensorFlow] (https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification-tf.ipynb).
</Tip>

## Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬

Ø±Ø§Ø¦Ø¹ØŒ Ø§Ù„Ø¢Ù† Ø¨Ø¹Ø¯ Ø£Ù† Ø¶Ø¨Ø·Øª Ù†Ù…ÙˆØ°Ø¬Ù‹Ø§ØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬!

Ø§Ø­ØµÙ„ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„ØªÙŠ ØªØ±ÙŠØ¯ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø¹Ù„ÙŠÙ‡Ø§:

```python
>>> text = "Golden State Warriors Ù‡ÙŠ ÙØ±ÙŠÙ‚ ÙƒØ±Ø© Ø³Ù„Ø© Ø£Ù…Ø±ÙŠÙƒÙŠ Ù…Ø­ØªØ±Ù ÙŠÙ‚Ø¹ Ù…Ù‚Ø±Ù‡ ÙÙŠ Ø³Ø§Ù† ÙØ±Ø§Ù†Ø³ÙŠØ³ÙƒÙˆ."
```

Ø£Ø¨Ø³Ø· Ø·Ø±ÙŠÙ‚Ø© Ù„ØªØ¬Ø±Ø¨Ø© Ù†Ù…ÙˆØ°Ø¬Ùƒ Ø§Ù„Ù…Ø¶Ø¨ÙˆØ· Ù„Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ù‡ÙŠ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ ÙÙŠ [`pipeline`]. Ù‚Ù… Ø¨ØªÙ†ÙÙŠØ° Ù…Ø«ÙŠÙ„ `pipeline` Ù„Ù€ NER Ù…Ø¹ Ù†Ù…ÙˆØ°Ø¬ÙƒØŒ ÙˆÙ…Ø±Ø± Ù†ØµÙƒ Ø¥Ù„ÙŠÙ‡:

```python
>>> from transformers import pipeline

>>> classifier = pipeline("ner"ØŒ model="stevhliu/my_awesome_wnut_model")
>>> classifier(text)
[{'entity': 'B-location'ØŒ
'score': 0.42658573ØŒ
'index': 2ØŒ
'word': 'golden'ØŒ
'start': 4ØŒ
'end': 10}ØŒ
{'entity': 'I-location'ØŒ
'score': 0.35856336ØŒ
'index': 3ØŒ
'word': 'state'ØŒ
'start': 11ØŒ
'end': 16}ØŒ
{'entity': 'B-group'ØŒ
'score': 0.3064001ØŒ
'index': 4ØŒ
'word': 'warriors'ØŒ
'start': 17ØŒ
'end': 25}ØŒ
{'entity': 'B-location'ØŒ
'score': 0.65523505ØŒ
'index': 13ØŒ
'word': 'san'ØŒ
'start': 80ØŒ
'end': 83}ØŒ
{'entity': 'B-location'ØŒ
'score': 0.4668663ØŒ
'index': 14ØŒ
'word': 'francisco'ØŒ
'start': 84ØŒ
'end': 93}]
```

ÙŠÙ…ÙƒÙ†Ùƒ Ø£ÙŠØ¶Ù‹Ø§ Ø¥Ø¹Ø§Ø¯Ø© Ø¥Ù†ØªØ§Ø¬ Ù†ØªØ§Ø¦Ø¬ `pipeline` ÙŠØ¯ÙˆÙŠÙ‹Ø§ Ø¥Ø°Ø§ Ø£Ø±Ø¯Øª:

<frameworkcontent>
<pt>
Ù‚Ù… Ø¨ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Øµ ÙˆØ¥Ø±Ø¬Ø§Ø¹ tensers PyTorch:

```python
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("stevhliu/my_awesome_wnut_model")
>>> inputs = tokenizer(textØŒ return_tensors="pt")
```

Ù…Ø±Ø± Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ¥Ø±Ø¬Ø§Ø¹ `logits`:

```python
>>> from transformers import AutoModelForTokenClassification

>>> model = AutoModelForTokenClassification.from_pretrained("stevhliu/my_awesome_wnut_model")
>>> with torch.no_grad():
...     logits = model (** inputs). logits
```

Ø§Ø­ØµÙ„ Ø¹Ù„Ù‰ Ø§Ù„ÙØ¦Ø© Ø°Ø§Øª Ø£Ø¹Ù„Ù‰ Ø§Ø­ØªÙ…Ø§Ù„ØŒ ÙˆØ§Ø³ØªØ®Ø¯Ù… ØªØ¹ÙŠÙŠÙ† `id2label` Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØ­ÙˆÙŠÙ„Ù‡ Ø¥Ù„Ù‰ ØªØ³Ù…ÙŠØ© Ù†ØµÙŠØ©:

```python
>>> predictions = torch.argmax (logitsØŒ dim=2)
>>> predicted_token_class = [model.config.id2label [t.item()] for t in predictions [0]]
>>> predicted_token_class
['O'ØŒ
'O'ØŒ
'B-location'ØŒ
'I-location'ØŒ
'B-group'ØŒ
'O'ØŒ
'O'ØŒ
'O'ØŒ
'O'ØŒ
'O'ØŒ
'O'ØŒ
'O'ØŒ
'O'ØŒ
'B-location'ØŒ
'B-location'ØŒ
'O'ØŒ
'O']
```

</pt>
<tf>
Ù‚Ù… Ø¨ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Øµ ÙˆØ¥Ø±Ø¬Ø§Ø¹ tensers TensorFlow:

```python
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("stevhliu/my_awesome_wnut_model")
>>> inputs = tokenizer(textØŒ return_tensors="tf")
```

Ù…Ø±Ø± Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ¥Ø±Ø¬Ø§Ø¹ `logits`:

```python
>>> from transformers import TFAutoModelForTokenClassification

>>> model = TFAutoModelForTokenClassification.from_pretrained("stevhliu/my_awesome_wnut_model")
>>> logits = model (** inputs). logits
```

Ø§Ø­ØµÙ„ Ø¹Ù„Ù‰ Ø§Ù„ÙØ¦Ø© Ø°Ø§Øª Ø£Ø¹Ù„Ù‰ Ø§Ø­ØªÙ…Ø§Ù„ØŒ ÙˆØ§Ø³ØªØ®Ø¯Ù… ØªØ¹ÙŠÙŠÙ† `id2label` Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØ­ÙˆÙŠÙ„Ù‡ Ø¥Ù„Ù‰ ØªØ³Ù…ÙŠØ© Ù†ØµÙŠØ©:

```python
>>> predicted_token_class_ids = tf.math.argmax (logitsØŒ axis=-1)
>>> predicted_token_class = [model.config.id2label [t] for t in predicted_token_class_ids [0]. numpy(). tolist()]
>>> predicted_token_class
['O'ØŒ
'O'ØŒ
'B-location'ØŒ
'I-location'ØŒ
'B-group'ØŒ
'O'ØŒ
'O'ØŒ
'O'ØŒ
'O'ØŒ
'O'ØŒ
'O'ØŒ
'O'ØŒ
'O'ØŒ
'B-location'ØŒ
'B-location'ØŒ
'O'ØŒ
'O']
```

</tf>
</frameworkcontent>