# قوالب نماذج الدردشة

## مقدمة

أصبح استخدام نماذج اللغة الكبيرة في الدردشة **chat** حالة استخدام شائعة بشكل متزايد. في سياق الدردشة، بدلاً من الاستمرار في سلسلة نصية واحدة (كما هو الحال مع نموذج اللغة القياسي)، يستمر النموذج بدلاً من ذلك في محادثة تتكون من رسالة واحدة أو أكثر، لكل منها **دور**، مثل "المستخدم" أو "المساعد"، بالإضافة إلى نص الرسالة.

وعلى غرار التمييز، تتوقع النماذج المختلفة تنسيقات إدخال مختلفة جدًا للدردشة. هذا هو السبب في أننا أضفنا **قوالب الدردشة** كخاصية. قوالب الدردشة هي جزء من المميِّز. تحدد كيفية تحويل المحادثات، التي تمثلها قوائم الرسائل، إلى سلسلة واحدة قابلة للتمييز بالشكل الذي يتوقعه النموذج.

دعونا نجعل هذا ملموسًا بمثال سريع باستخدام نموذج `BlenderBot`. لدى BlenderBot قالب افتراضي بسيط للغاية، والذي يضيف في الغالب مسافات بيضاء بين جولات الحوار:

```python
>>> from transformers import AutoTokenizer
>>> tokenizer = AutoTokenizer.from_pretrained("facebook/blenderbot-400M-distill")

>>> chat = [
...    {"role": "user", "content": "Hello, how are you?"},
...    {"role": "assistant", "content": "I'm doing great. How can I help you today?"},
...    {"role": "user", "content": "I'd like to show off how chat templating works!"},
... ]

>>> tokenizer.apply_chat_template(chat, tokenize=False)
" Hello, how are you?  I'm doing great. How can I help you today?   I'd like to show off how chat templating works!</s>"
```

لاحظ كيف تم ضغط الدردشة بأكملها في سلسلة واحدة. إذا استخدمنا `tokenize=True`، وهو الإعداد الافتراضي، فسيتم أيضًا تمييز تلك السلسلة لنا. ولكن لمشاهدة قالب أكثر تعقيدًا في العمل، دعنا نستخدم نموذج `mistralai/Mistral-7B-Instruct-v0.1`.

```python
>>> from transformers import AutoTokenizer
>>> tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.1")

>>> chat = [
...   {"role": "user", "content": "Hello, how are you?"},
...   {"role": "assistant", "content": "I'm doing great. How can I help you today?"},
...   {"role": "user", "content": "I'd like to show off how chat templating works!"},
... ]

>>> tokenizer.apply_chat_template(chat, tokenize=False)
"<s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?</s> [INST] I'd like to show off how chat templating works! [/INST]"
```

لاحظ أن هذه المرة، أضاف المميِّز رموز التحكم [INST] و[/INST] للإشارة إلى بداية ونهاية رسائل المستخدم (ولكن ليس رسائل المساعد!). تم تدريب Mistral-instruct باستخدام هذه الرموز، ولكن لم يتم تدريب BlenderBot.

## كيف يمكنني استخدام قوالب الدردشة؟

كما هو موضح في المثال أعلاه، يسهل استخدام قوالب الدردشة. ما عليك سوى إنشاء قائمة بالرسائل، مع مفاتيح `role` و`content`، ثم تمريرها إلى طريقة [`~PreTrainedTokenizer.apply_chat_template`]. بمجرد قيامك بذلك، ستحصل على إخراج جاهز للاستخدام! عند استخدام قوالب الدردشة كإدخال لتوليد النماذج، فمن الجيد أيضًا استخدام `add_generation_prompt=True` لإضافة [مطالبات التوليد](#what-are-generation-prompts).

فيما يلي مثال على إعداد الإدخال لـ `model.generate()`، باستخدام نموذج مساعد `Zephyr`:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

checkpoint = "HuggingFaceH4/zephyr-7b-beta"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForCausalLM.from_pretrained(checkpoint) # قد ترغب في استخدام bfloat16 و/أو الانتقال إلى GPU هنا

messages = [
{
"role": "system",
"content": "You are a friendly chatbot who always responds in the style of a pirate",
},
{"role": "user", "content": "How many helicopters can a human eat in one sitting?"},
]
tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors="pt")
print(tokenizer.decode(tokenized_chat[0]))
```

سيؤدي هذا إلى سلسلة بتنسيق الإدخال الذي يتوقعه Zephyr.

```text
<|system|>
You are a friendly chatbot who always responds in the style of a pirate</s>
<|user|>
How many helicopters can a human eat in one sitting?</s>
<|assistant|>
```

الآن بعد أن تم تنسيق الإدخال بشكل صحيح لـ Zephyr، يمكننا استخدام النموذج لتوليد استجابة لسؤال المستخدم:

```python
outputs = model.generate(tokenized_chat, max_new_tokens=128)
print(tokenizer.decode(outputs[0]))
```

سيؤدي هذا إلى ما يلي:

```text
<|system|>
You are a friendly chatbot who always responds in the style of a pirate</s>
<|user|>
How many helicopters can a human eat in one sitting?</s>
<|assistant|>
Matey, I'm afraid I must inform ye that humans cannot eat helicopters. Helicopters are not food, they are flying machines. Food is meant to be eaten, like a hearty plate o' grog, a savory bowl o' stew, or a delicious loaf o' bread. But helicopters, they be for transportin' and movin' around, not for eatin'. So, I'd say none, me hearties. None at all.
```

أوه، كان الأمر سهلاً بعد كل شيء!

## هل هناك خط أنابيب تلقائي للدردشة؟

نعم هناك! تدعم خطوط أنابيب التوليد النصي إدخالات الدردشة، مما يجعل من السهل استخدام نماذج الدردشة. في الماضي، كنا نستخدم فئة "ConversationalPipeline" مخصصة، ولكن تم الآن إيقافها وتم دمج وظائفها في [`TextGenerationPipeline`]. دعونا نجرب مثال Zephyr مرة أخرى، ولكن هذه المرة باستخدام خط أنابيب:

```python
from transformers import pipeline

pipe = pipeline("text-generation", "HuggingFaceH4/zephyr-7b-beta")
messages = [
{
"role": "system",
"content": "You are a friendly chatbot who always responds in the style of a pirate",
},
{"role": "user", "content": "How many helicopters can a human eat in one sitting?"},
]
print(pipe(messages, max_new_tokens=128)[0]['generated_text'][-1]) # طباعة استجابة المساعد
```

```text
{'role': 'assistant', 'content': "Matey, I'm afraid I must inform ye that humans cannot eat helicopters. Helicopters are not food, they are flying machines. Food is meant to be eaten, like a hearty plate o' grog, a savory bowl o' stew, or a delicious loaf o' bread. But helicopters, they be for transportin' and movin' around, not for eatin'. So, I'd say none, me hearties. None at all."}
```

سيتولى خط الأنابيب جميع تفاصيل التمييز واستدعاء `apply_chat_template` نيابة عنك - بمجرد حصول النموذج على قالب دردشة، كل ما عليك فعله هو تهيئة خط الأنابيب وتمرير قائمة الرسائل إليه!

## ما هي "مطالبات التوليد"؟

قد تكون لاحظت أن طريقة `apply_chat_template` لديها حجة `add_generation_prompt`. يخبر هذا الحجة القالب بإضافة رموز تشير إلى بداية استجابة البوت. على سبيل المثال، ضع في اعتبارك الدردشة التالية:

```python
messages = [
{"role": "user", "content": "Hi there!"},
{"role": "assistant", "content": "Nice to meet you!"},
{"role": "user", "content": "Can I ask a question?"}
]
```

هكذا يبدو الأمر بدون مطالبة التوليد، باستخدام قالب ChatML الذي رأيناه في مثال Zephyr:

```python
tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)
"""<|im_start|>user
Hi there!<|im_end|>
<|im_start|>assistant
Nice to meet you!<|im_end|>
<|im_start|>user
Can I ask a question?<|im_end|>
"""
```

وهكذا يبدو الأمر **مع** مطالبة التوليد:

```python
tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
"""<|im_start|>user
Hi there!<|im_end|>
<|im_start|>assistant
Nice to meet you!<|im_end|>
<|im_start|>user
Can I ask a question?<|im_end|>
<|im_start|>assistant
"""
```

لاحظ أننا هذه المرة أضفنا الرموز التي تشير إلى بداية استجابة البوت. يضمن هذا أنه عندما يقوم النموذج بتوليد النص، فسوف يكتب استجابة البوت بدلاً من القيام بشيء غير متوقع، مثل الاستمرار في رسالة المستخدم. تذكر، أن نماذج الدردشة لا تزال مجرد نماذج للغة - فهي مدربة على الاستمرار في النص، والدردشة هي مجرد نوع خاص من النص بالنسبة لهم! يجب توجيهها باستخدام رموز التحكم المناسبة، حتى تعرف ما هو مطلوب منها.

لا تتطلب جميع النماذج مطالبات التوليد. بعض النماذج، مثل BlenderBot وLLaMA، ليس لديها أي رموز خاصة قبل استجابات البوت. في هذه الحالات، لن يكون لحجة `add_generation_prompt` أي تأثير. سيعتمد التأثير الدقيق لـ `add_generation_prompt` على القالب المستخدم.## هل يمكنني استخدام قوالب الدردشة في التدريب؟

نعم! نوصي بتطبيق قالب الدردشة كخطوة معالجة مسبقة لمجموعة البيانات الخاصة بك. بعد ذلك، يمكنك ببساطة المتابعة مثل أي مهمة تدريب نموذج لغة أخرى. عند التدريب، يجب عليك عادةً تعيين "add_generation_prompt=False"، لأن الرموز المضافة لطلب استجابة المساعد لن تكون مفيدة أثناء التدريب. دعونا نرى مثالًا:

والآن، دعنا نستمر في التدريب كما هو الحال مع مهمة نمذجة اللغة القياسية، باستخدام عمود "formatted_chat".

## متقدم: إدخالات إضافية إلى قوالب الدردشة

الحجة الوحيدة التي يتطلبها "apply_chat_template" هي "messages". ومع ذلك، يمكنك تمرير أي حجة كلمة أساسية إلى "apply_chat_template" وسيكون من الممكن الوصول إليها داخل القالب. يمنحك هذا الكثير من الحرية لاستخدام قوالب الدردشة للعديد من الأشياء. لا توجد قيود على أسماء أو تنسيق هذه الحجج - يمكنك تمرير السلاسل أو القوائم أو القواميس أو أي شيء آخر تريده.

## متقدم: استخدام الأداة / استدعاء الدالة

يمكن لنماذج "استخدام الأداة" اختيار استدعاء الدوال كأدوات خارجية قبل إنشاء إجابة. عند تمرير الأدوات إلى نموذج "استخدام الأداة"، يمكنك ببساطة تمرير قائمة بالدوال إلى وسيطة "الأدوات".

للعمل بشكل صحيح، يجب عليك كتابة دالاتك بتنسيق مشابه لما سبق، بحيث يمكن تحليلها بشكل صحيح كأدوات. على وجه التحديد، يجب عليك اتباع هذه القواعد:

1. يجب أن يكون للدالة اسم وصفي.
2. يجب أن يكون لكل حجة تلميح نوع.
3. يجب أن تحتوي الدالة على سلسلة doc في نمط Google القياسي (بعبارة أخرى، وصف الدالة الأولي متبوعًا بكتلة "Args:" التي تصف الحجج، ما لم تكن الدالة لا تحتوي على أي حجج).
4. لا تقم بتضمين الأنواع في كتلة "Args". وبعبارة أخرى، اكتب "a: الرقم الأول الذي يجب ضربه"، وليس "a (int): الرقم الأول الذي يجب ضربه". يجب أن تذهب تلميحات الأنواع في رأس الدالة بدلاً من ذلك.
5. يمكن أن يكون للدالة نوع إرجاع وكتلة "Returns:" في سلسلة doc. ومع ذلك، فهذه اختيارية لأن معظم نماذج استخدام الأدوات تتجاهلها.

### تمرير نتائج الأداة إلى النموذج

1. قم بتفسير إخراج النموذج للحصول على اسم (أسماء) الأداة والحجج.
2. أضف (أضف) استدعاء (استدعاءات) أداة النموذج إلى المحادثة.
3. قم باستدعاء الدالة (الدالات) المقابلة بتلك الحجج.
4. أضف النتيجة (النتائج) إلى المحادثة.

### مثال كامل على استخدام الأداة

لننتقل عبر مثال على استخدام الأداة، خطوة بخطوة. لهذا المثال، سنستخدم نموذج "Hermes-2-Pro" بحجم 8 بتا، حيث إنه أحد أفضل نماذج استخدام الأدوات أداءً في فئة الحجم الخاصة به في وقت الكتابة. إذا كان لديك الذاكرة، فيمكنك التفكير في استخدام نموذج أكبر بدلاً من ذلك، مثل "Command-R" أو "Mixtral-8x22B"، وكلاهما يدعم أيضًا استخدام الأدوات ويقدم أداءً أقوى.

أولاً، دعنا نحمل نموذجنا ومحولنا:

الآن، دعنا نحدد قائمة الأدوات:

الآن، دعنا نقوم بإعداد محادثة لروبوت الدردشة الخاص بنا:

بعد ذلك، دعنا نطبق قالب الدردشة ونولد استجابة:

والنتيجة:

وقد استدعى النموذج الدالة بحجج صالحة، بالتنسيق الذي طلبته سلسلة doc. لقد استنتج أنه من المرجح أننا نشير إلى باريس في فرنسا، وتذكر أنه، باعتبارها موطن الوحدات الدولية، يجب بالتأكيد عرض درجة الحرارة في فرنسا بالدرجة المئوية.

دعنا نضيف استدعاء أداة النموذج إلى المحادثة. لاحظ أننا نولد "tool_call_id" عشوائيًا هنا. لا تستخدم جميع النماذج هذه المعرفات، ولكنها تسمح للنماذج بإصدار عدة استدعاءات للأدوات في نفس الوقت وتتبع الاستجابة المقابلة لكل استدعاء. يمكنك إنشاؤها بالطريقة التي تريدها، ولكن يجب أن تكون فريدة داخل كل دردشة.

الآن بعد أن أضفنا استدعاء الأداة إلى المحادثة، يمكننا استدعاء الدالة وإضافة النتيجة إلى المحادثة. نظرًا لأننا نستخدم دالة وهمية لهذا المثال تعيد دائمًا 22.0، فيمكننا ببساطة إضافة تلك النتيجة مباشرةً. مرة أخرى، لاحظ "tool_call_id" - يجب أن يتطابق هذا مع المعرّف المستخدم في استدعاء الأداة أعلاه.

أخيرًا، دع مساعد الدردشة يقرأ إخراج الدالة ويواصل الدردشة مع المستخدم:

والنتيجة:

على الرغم من أن هذا كان مجرد عرض توضيحي بسيط باستخدام أدوات وهمية ومكالمة واحدة، إلا أن نفس التقنية تعمل باستخدام أدوات متعددة حقيقية ومحاورات أطول. يمكن أن تكون هذه طريقة فعالة لتوسيع قدرات وكلاء المحادثة باستخدام معلومات في الوقت الفعلي، أو أدوات حسابية مثل الآلات الحاسبة، أو الوصول إلى قواعد بيانات كبيرة.## فهم مخططات الأدوات

يتم تحويل كل دالة تمررها إلى وسيط "tools" في "apply_chat_template" إلى مخطط JSON. ثم يتم تمرير هذه المخططات إلى نموذج قالب الدردشة. وبعبارة أخرى، فإن نماذج استخدام الأدوات لا ترى وظائفك مباشرة، ولا ترى أبدًا التعليمات البرمجية الفعلية الموجودة بها. ما يهمها هو تعريفات الوظيفة والحجج التي تحتاج إلى تمريرها إليها - فهي تهتم بما تفعله الأدوات وكيفية استخدامها، وليس كيفية عملها! يتوقف الأمر عليك في قراءة نتائجها، والتحقق مما إذا كانت قد طلبت استخدام أداة، وتمرير حججها إلى دالة الأداة، وإرجاع الاستجابة في الدردشة.

يجب أن يكون توليد مخططات JSON لتمريرها إلى القالب تلقائيًا وغير مرئي طالما أن وظائفك تتبع المواصفات المذكورة أعلاه، ولكن إذا واجهتك مشكلات، أو إذا كنت تريد فقط مزيدًا من التحكم في التحويل، فيمكنك التعامل مع التحويل يدويًا. فيما يلي مثال على التحويل اليدوي للمخطط.

## المتقدم: الاسترجاع المعزز للجيل

يمكن لنماذج "LLMs" المعززة بالاسترجاع أو "RAG" البحث في مجموعة من الوثائق للحصول على معلومات قبل الرد على استعلام. يسمح هذا للنماذج بتوسيع قاعدة معارفها بشكل كبير إلى ما هو أبعد من حجم السياق المحدود. توصيتنا لنماذج RAG هي أن قالبها يجب أن يقبل وسيط "documents". يجب أن تكون هذه قائمة بالوثائق، حيث تكون كل "وثيقة" عبارة عن قاموس واحد مع مفاتيح "title" و "contents"، وكلاهما من نوع string. نظرًا لأن هذا التنسيق أبسط بكثير من مخططات JSON المستخدمة للأدوات، فلا داعي لاستخدام وظائف مساعدة.

فيما يلي مثال على نموذج RAG يعمل على قالب:

## المتقدم: كيف تعمل قوالب الدردشة؟

يتم تخزين قالب الدردشة للنموذج على سمة "tokenizer.chat_template". إذا لم يتم تعيين أي قالب دردشة، يتم استخدام القالب الافتراضي لتلك الفئة النموذجية بدلاً من ذلك. دعنا نلقي نظرة على قالب "BlenderBot":

هذا مخيف بعض الشيء. دعونا نضيف بعض الأسطر الجديدة والتباعد لتحسين قابلية القراءة. لاحظ أن السطر الجديد الأول بعد كل كتلة وأي مسافة بادئة قبل الكتلة يتم تجاهلهما بشكل افتراضي، وذلك باستخدام أعلام Jinja "trim_blocks" و "lstrip_blocks". ومع ذلك، كن حذرًا - على الرغم من أنه يتم إزالة المسافات البادئة الرئيسية في كل سطر، إلا أن المسافات بين الكتل الموجودة على نفس السطر لا يتم إزالتها. نوصي بشدة بالتحقق من عدم طباعة قالبك لمساحات إضافية حيث لا ينبغي أن تكون!

إذا لم تشاهد واحدًا من هذه القوالب من قبل، فهو قالب Jinja. Jinja هو لغة قوالب تسمح لك بكتابة كود بسيط لتوليد النص. وفي العديد من النواحي، يشبه الكود والبناء النحوي لـ Python. في Python النقي، سيبدو هذا القالب كما يلي:

يقوم القالب بشكل فعال بثلاثة أشياء:

1. لكل رسالة، إذا كانت الرسالة رسالة مستخدم، فقم بإضافة مسافة فارغة قبلها، وإلا فلا تطبع شيئًا.
2. أضف محتوى الرسالة
3. إذا لم تكن الرسالة هي الرسالة الأخيرة، فأضف مسافتين بعدها. بعد الرسالة الأخيرة، قم بطباعة رمز EOS.

هذا قالب بسيط جدًا - فهو لا يضيف أي رموز تحكم، ولا يدعم رسائل "النظام"، والتي تعد طريقة شائعة لإعطاء النموذج توجيهات حول كيفية التصرف في المحادثة اللاحقة. ولكن Jinja يمنحك الكثير من المرونة للقيام بذلك! دعونا نرى قالب Jinja الذي يمكنه تنسيق الإدخالات بشكل مشابه للطريقة التي يقوم بها LLaMA (لاحظ أن قالب LLaMA الفعلي يتضمن التعامل مع رسائل النظام الافتراضية ومعالجة رسائل النظام بشكل مختلف بعض الشيء بشكل عام - لا تستخدم هذا في كودك الفعلي!)

## المتقدم: إضافة قوالب الدردشة وتحريرها

### كيف أنشئ قالب دردشة؟

ببساطة، قم بكتابة قالب Jinja وتعيين "tokenizer.chat_template". قد تجد أنه من الأسهل البدء بقالب موجود من نموذج آخر وتحريره ببساطة ليناسب احتياجاتك! على سبيل المثال، يمكننا أخذ قالب LLaMA أعلاه وإضافة "[ASST]" و "[/ASST]" إلى رسائل المساعد:

الآن، قم ببساطة بتعيين سمة "tokenizer.chat_template". في المرة القادمة التي تستخدم فيها [~ PreTrainedTokenizer.apply_chat_template]، فسيستخدم قالبك الجديد! سيتم حفظ هذه السمة في ملف "tokenizer_config.json"، لذا يمكنك استخدام [~ utils.PushToHubMixin.push_to_hub] لتحميل قالبك الجديد إلى Hub والتأكد من استخدام الجميع للقالب الصحيح لنموذجك!

تتم استدعاء طريقة [~ PreTrainedTokenizer.apply_chat_template]، التي تستخدم قالب الدردشة الخاص بك، بواسطة فئة [TextGenerationPipeline]، لذا بمجرد تعيين قالب الدردشة الصحيح، سيصبح نموذجك متوافقًا تلقائيًا مع [TextGenerationPipeline].

<Tip>
إذا كنت تقوم بتدريب نموذج للدردشة، بالإضافة إلى تعيين قالب دردشة، فيجب عليك على الأرجح إضافة أي رموز تحكم للدردشة الجديدة كرموز خاصة في المحلل اللغوي. لا يتم أبدًا تقسيم الرموز الخاصة، مما يضمن التعامل مع رموز التحكم الخاصة بك دائمًا كرموز فردية بدلاً من تقسيمها إلى أجزاء. يجب عليك أيضًا تعيين سمة "eos_token" للمحلل اللغوي إلى الرمز الذي يشير إلى نهاية الأجيال المساعدة في قالبك. سيسمح هذا لأدوات توليد النص بتحديد وقت التوقف عن توليد النص بشكل صحيح.
</Tip>### ما هي القوالب "الافتراضية"؟
قبل تقديم قوالب الدردشة، كان التعامل مع الدردشة مُشفّرًا بصورة ثابتة على مستوى فئة النموذج. ولأغراض التوافق مع الإصدارات السابقة، احتفظنا بهذا التعامل المُحدد للفئة باعتباره قوالب افتراضية، يتم تحديدها أيضًا على مستوى الفئة. إذا لم يكن للنموذج قالب دردشة مُحدد، ولكن هناك قالب افتراضي لفئة النموذج الخاصة به، فستستخدم فئة "TextGenerationPipeline" والطرق مثل "apply_chat_template" قالب الفئة بدلاً من ذلك. يمكنك معرفة القالب الافتراضي لبرنامج التمييز الخاص بك عن طريق التحقق من خاصية "tokenizer.default_chat_template".

ونحن نقوم بذلك فقط لأسباب تتعلق بالتوافق مع الإصدارات السابقة، وذلك لتجنب تعطيل أي سير عمل موجودة. حتى عندما يكون قالب الفئة مناسبًا لنموذجك، نوصي بشدة بتجاوز القالب الافتراضي عن طريق تحديد خاصية "chat_template" بشكل صريح، وذلك لتوضيح للمستخدمين أن نموذجك تم تهيئته بشكل صحيح للدردشة.

والآن بعد اعتماد قوالب الدردشة الفعلية على نطاق واسع، تم إيقاف استخدام القوالب الافتراضية وسيتم إزالتها في إصدار مستقبلي. نوصي بشدة بتحديد خاصية "chat_template" لأي برامج تمييز لا تزال تعتمد عليها!

### ما هو القالب الذي يجب أن استخدمه؟
عند تحديد قالب لنموذج تم تدريبه بالفعل على الدردشة، يجب التأكد من أن القالب يتطابق تمامًا مع تنسيق الرسائل الذي شاهده النموذج أثناء التدريب، وإلا فمن المحتمل أن تواجه تدهورًا في الأداء. هذا صحيح حتى إذا كنت تقوم بتدريب النموذج بشكل إضافي - فمن المحتمل أن تحصل على أفضل أداء إذا قمت بإبقاء رموز الدردشة ثابتة. وهذا مشابه جدًا للتمييز - فعادةً ما تحصل على أفضل أداء للاستنتاج أو الضبط الدقيق عند مطابقة التمييز المستخدم أثناء التدريب بدقة.

من ناحية أخرى، إذا كنت تقوم بتدريب نموذج من الصفر، أو ضبط نموذج لغة أساسي للدردشة، فستتمتع بحرية كبيرة في اختيار قالب مناسب! تمتلك LLMs الذكاء الكافي للتعامل مع العديد من تنسيقات الإدخال المختلفة. أحد الخيارات الشائعة هو تنسيق "ChatML"، وهو خيار جيد ومرن للعديد من حالات الاستخدام. يبدو هذا التنسيق على النحو التالي:

```
{% for message in messages %}
{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}
{% endfor %}
```

إذا أعجبك هذا، فإليك نسخة جاهزة للاستخدام في سطر واحد، يمكنك نسخها ولصقها في الكود الخاص بك. يتضمن السطر الواحد أيضًا دعمًا مُفيدًا لـ [مطالبات التوليد](#what-are-generation-prompts)، ولكن لاحظ أنه لا يضيف رموز BOS أو EOS! إذا كان نموذجك يتوقع تلك الرموز، فلن يتم إضافتها تلقائيًا بواسطة "apply_chat_template" - وبعبارة أخرى، سيتم تمييز النص باستخدام "add_special_tokens=False". وذلك لتجنب التضارب المحتمل بين القالب ومنطق "add_special_tokens". إذا كان نموذجك يتوقع رموزًا خاصة، فتأكد من إضافتها إلى القالب!

```python
tokenizer.chat_template = "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}"
```

يغلف هذا القالب كل رسالة بين الرمزين "<|im_start|>" و "<|im_end|>"، ويكتب ببساطة الدور كسلسلة نصية، مما يسمح بالمرونة في الأدوار التي يتم التدريب عليها. يبدو الناتج على النحو التالي:

```text
<|im_start|>system
You are a helpful chatbot that will do its best not to say anything so stupid that people tweet about it.<|im_end|>
<|im_start|>user
How are you?<|im_end|>
<|im_start|>assistant
I'm doing great!<|im_end|>
```

تعد أدوار "user" و "system" و "assistant" هي الأدوار القياسية للدردشة، ونوصي باستخدامها عندما يكون ذلك منطقيًا، خاصة إذا كنت تريد أن يعمل نموذجك بشكل جيد مع ["TextGenerationPipeline"]. ومع ذلك، فأنت لست مقيدًا بهذه الأدوار - حيث تتيح القوالب المرونة الشديدة، ويمكن أن تكون أي سلسلة نصية بمثابة دور.

### أريد إضافة بعض قوالب الدردشة! كيف يمكنني البدء؟
إذا كان لديك أي نماذج دردشة، فيجب عليك تحديد خاصية "tokenizer.chat_template" الخاصة بها واختبارها باستخدام ["~PreTrainedTokenizer.apply_chat_template"]، ثم دفع برنامج التمييز المحدث إلى Hub. ينطبق هذا حتى إذا لم تكن مالك النموذج - إذا كنت تستخدم نموذجًا بقالب دردشة فارغًا، أو نموذجًا لا يزال يستخدم قالب الفئة الافتراضي، فيرجى فتح طلب سحب "pull request" إلى مستودع النموذج حتى يمكن تحديد هذه الخاصية بشكل صحيح!

بمجرد تحديد الخاصية، تكون قد انتهيت! سيتم الآن تشغيل "tokenizer.apply_chat_template" بشكل صحيح لهذا النموذج، مما يعني أنه مدعوم أيضًا بشكل تلقائي في أماكن مثل "TextGenerationPipeline"!

عن طريق التأكد من أن النماذج لديها هذه الخاصية، يمكننا التأكد من أن المجتمع بأكمله يستفيد من القوة الكاملة للنماذج مفتوحة المصدر. لقد كانت عدم تطابق التنسيق تطارد المجال وتضر الأداء بصمت لفترة طويلة جدًا - لقد حان الوقت لوضع حد لها!

## متقدم: نصائح لكتابة القوالب
إذا كنت غير معتاد على Jinja، فإننا نجد بشكل عام أن أسهل طريقة لكتابة قالب دردشة هي أولاً كتابة نص برمجي Python قصير يقوم بتنسيق الرسائل بالطريقة التي تريدها، ثم تحويل هذا النص البرمجي إلى قالب.

تذكر أن مُعالج القالب سيستقبل سجل المحادثة كمتغير يسمى "messages". كل رسالة عبارة عن قاموس له مفتاحان، "role" و "content". ستتمكن من الوصول إلى "messages" في قالبك تمامًا كما هو الحال في Python، مما يعني أنه يمكنك التكرار خلاله باستخدام "{% for message in messages %}"، أو الوصول إلى الرسائل الفردية، على سبيل المثال، "{{ messages[0] }}".

يمكنك أيضًا استخدام النصائح التالية لتحويل الكود الخاص بك إلى Jinja:

### حلقات التكرار
تبدو حلقات التكرار في Jinja على النحو التالي:

```
{% for message in messages %}
{{ message['content'] }}
{% endfor %}
```

لاحظ أن أي شيء داخل كتلة التعبير "{{ expression block }}" سيتم طباعته في الإخراج. يمكنك استخدام عوامل التشغيل مثل "+" لدمج السلاسل النصية داخل كتل التعبير.

### جمل الشرط
تبدو جمل الشرط في Jinja على النحو التالي:

```
{% if message['role'] == 'user' %}
{{ message['content'] }}
{% endif %}
```

لاحظ كيف أن Python يستخدم المسافات البيضاء لعلامات بدايات ونهايات كتل "for" و "if"، بينما تتطلب Jinja منك إنهاءها بشكل صريح باستخدام "{% endfor %}" و "{% endif %}".

### المتغيرات الخاصة
داخل القالب الخاص بك، ستتمكن من الوصول إلى قائمة "messages"، ولكن يمكنك أيضًا الوصول إلى العديد من المتغيرات الخاصة الأخرى. تشمل هذه الرموز الخاصة مثل "bos_token" و "eos_token"، بالإضافة إلى متغير "add_generation_prompt" الذي ناقشناه أعلاه. يمكنك أيضًا استخدام متغير "loop" للوصول إلى معلومات حول التكرار الحالي للحلقة، على سبيل المثال، باستخدام "{% if loop.last %}" للتحقق مما إذا كانت الرسالة الحالية هي الرسالة الأخيرة في المحادثة. فيما يلي مثال يجمع هذه الأفكار معًا لإضافة موجه توليد في نهاية المحادثة إذا كان "add_generation_prompt" يساوي "True":

```
{% if loop.last and add_generation_prompt %}
{{ bos_token + 'Assistant:\n' }}
{% endif %}
```

### ملاحظات حول المسافات البيضاء
لقد حاولنا قدر الإمكان جعل Jinja تتجاهل المسافات البيضاء خارج كتل "{{ expressions }}". ومع ذلك، كن على علم بأن Jinja هي محرك قوالب عام، وقد تعامل المسافات البيضاء بين الكتل الموجودة على نفس السطر كمسافات ذات أهمية وطباعتها في الإخراج. نوصي بشدة بالتحقق من أن القالب الخاص بك لا يطبع مسافات إضافية حيث لا ينبغي أن تكون قبل تحميله!