# استراتيجيات توليد النص

يعد توليد النص أساسيًا في العديد من مهام معالجة اللغات الطبيعية، مثل توليد النص المفتوح، والتلخيص، والترجمة، وأكثر من ذلك. كما أنه يلعب دورًا في مجموعة متنوعة من تطبيقات الطرائق المختلطة التي يكون النص هو ناتجها مثل تحويل الكلام إلى نص، والصور إلى نص. بعض النماذج التي يمكنها توليد النص تشمل: GPT2، XLNet، OpenAI GPT، CTRL، TransformerXL، XLM، Bart، T5، GIT، Whisper.

اطلع على بعض الأمثلة التي تستخدم طريقة [~generation.GenerationMixin.generate] لإنتاج النصوص لمهام مختلفة:

- [تلخيص النص](./tasks/summarization#inference)
- [وضع عنوان للصورة](./model_doc/git#transformers.GitForCausalLM.forward.example)
- [نسخ الصوت](./model_doc/whisper#transformers.WhisperForConditionalGeneration.forward.example)

لاحظ أن المدخلات إلى طريقة التوليد تعتمد على طريقة النموذج. يتم إرجاعها بواسطة فئة معالجة النموذج، مثل AutoTokenizer أو AutoProcessor. إذا أنشأت معالجة النموذج أكثر من نوع واحد من الإدخال، فقم بتمرير جميع الإدخالات إلى generate(). يمكنك معرفة المزيد حول معالجة النماذج الفردية في وثائق النموذج المقابلة.

تُعرف عملية اختيار الرموز المميزة للإخراج لتوليد النص باسم فك التشفير، ويمكنك تخصيص إستراتيجية فك التشفير التي ستستخدمها طريقة `generate()`. لا يؤدي تعديل إستراتيجية فك التشفير إلى تغيير قيم أي معلمات قابلة للتدريب. ومع ذلك، يمكن أن يكون له تأثير ملحوظ على جودة الإخراج المولد. يمكن أن يساعد في تقليل التكرار في النص وجعله أكثر تماسكًا.

يصف هذا الدليل ما يلي:

- تكوين التوليد الافتراضي
- استراتيجيات فك التشفير الشائعة ومعلماتها الرئيسية
- حفظ ومشاركة تكوينات التوليد المخصصة مع نموذج التدريب الدقيق الخاص بك على 🤗 Hub

#  تكوين التوليد الافتراضي

تتم تعريف إستراتيجية فك تشفير النموذج في تكوين التوليد الخاص به. عند استخدام النماذج المدربة مسبقًا للاستنتاج داخل [`pipeline`]، تقوم النماذج باستدعاء طريقة `PreTrainedModel.generate()` التي تطبق تكوين التوليد الافتراضي تحت الغطاء. يتم أيضًا استخدام التكوين الافتراضي عندما لا يتم حفظ أي تكوين مخصص مع النموذج.

عندما تقوم بتحميل نموذج بشكل صريح، يمكنك فحص تكوين التوليد الذي يأتي معه من خلال `model.generation_config`:

```python
>>> from transformers import AutoModelForCausalLM

>>> model = AutoModelForCausalLM.from_pretrained("distilbert/distilgpt2")
>>> model.generation_config
GenerationConfig {
"bos_token_id": 50256,
"eos_token_id": 50256
}
```

يكشف طباعة `model.generation_config` فقط عن القيم التي تختلف عن تكوين التوليد الافتراضي، ولا يدرج أيًا من القيم الافتراضية.

يقتصر تكوين التوليد الافتراضي على حجم الإخراج المدمج مع موجه الإدخال بحد أقصى 20 رمزًا لتجنب مواجهة قيود الموارد. إستراتيجية فك التشفير الافتراضية هي البحث الجشع، والتي تعد أبسط إستراتيجية فك تشفير تختار رمزًا مميزًا له أعلى احتمال باعتباره الرمز المميز التالي. بالنسبة للعديد من المهام وأحجام الإخراج الصغيرة، يعمل هذا بشكل جيد. ومع ذلك، عندما يتم استخدامه لتوليد مخرجات أطول، يمكن أن يبدأ البحث الجشع في إنتاج نتائج متكررة للغاية.

## تخصيص توليد النص

يمكنك تجاوز أي `generation_config` عن طريق تمرير المعلمات وقيمها مباشرةً إلى طريقة [`generate`]:

```python
>>> my_model.generate(**inputs, num_beams=4, do_sample=True)  # doctest: +SKIP
```

حتى إذا كانت إستراتيجية فك التشفير الافتراضية تعمل بشكل أساسي لمهمتك، فلا يزال بإمكانك ضبط بعض الأشياء. بعض المعلمات التي يتم ضبطها بشكل شائع تشمل ما يلي:

- `max_new_tokens`: العدد الأقصى للرموز المميزة التي سيتم توليدها. وبعبارة أخرى، حجم تسلسل الإخراج، باستثناء الرموز المميزة الموجودة في الموجه. كبديل لاستخدام طول الإخراج كمعيار إيقاف، يمكنك اختيار إيقاف التوليد في أي وقت يتجاوز فيه التوليد الكامل مقدارًا معينًا من الوقت. لمعرفة المزيد، راجع [`StoppingCriteria`].
- `num_beams`: من خلال تحديد عدد الحزم أكبر من 1، فأنت تقوم بالتبديل بشكل فعال من البحث الجشع إلى البحث الشعاعي. تقيّم هذه الإستراتيجية العديد من الفرضيات في كل خطوة زمنية وتختار في النهاية الفرضية التي يكون لها أعلى احتمال إجمالي للتسلسل بأكمله. تتمثل ميزة هذه الإستراتيجية في تحديد تسلسلات عالية الاحتمال تبدأ برموز مميزة أولية أقل احتمالًا والتي ستتجاهلها طريقة البحث الجشع. قم بتصور كيفية عملها [هنا](https://huggingface.co/spaces/m-ric/beam_search_visualizer).
- `do_sample`: إذا تم تعيينه على `True`، فإن هذه المعلمة تمكن إستراتيجيات فك التشفير مثل العينات متعددة الحدود، والبحث الشعاعي متعدد الحدود، والعينات الأعلى K، والعينات الأعلى p. تقوم جميع هذه الإستراتيجيات بتحديد الرمز المميز التالي من توزيع الاحتمالية عبر المفردات بالكامل مع تعديلات محددة للإستراتيجية.
- `num_return_sequences`: عدد تسلسلات المرشحين التي سيتم إرجاعها لكل إدخال. هذا الخيار متاح فقط لإستراتيجيات فك التشفير التي تدعم عدة تسلسلات مرشحة، على سبيل المثال، اختلافات البحث الشعاعي والنمذجة. تعيد إستراتيجيات فك التشفير مثل البحث الجشع والبحث التبايني تسلسل إخراج واحد.

## حفظ إستراتيجية فك تشفير مخصصة مع نموذج

إذا كنت ترغب في مشاركة نموذج التدريب الدقيق مع تكوين توليد محدد، فيمكنك:

1. إنشاء مثيل فئة [`GenerationConfig`]
2. تحديد معلمات إستراتيجية فك التشفير
3. حفظ تكوين التوليد الخاص بك باستخدام [`GenerationConfig.save_pretrained`]، والتأكد من ترك حجته `config_file_name` فارغة
4. قم بتعيين `push_to_hub` على `True` لتحميل تكوينك إلى مستودع النموذج

```python
>>> from transformers import AutoModelForCausalLM, GenerationConfig

>>> model = AutoModelForCausalLM.from_pretrained("my_account/my_model")  # doctest: +SKIP
>>> generation_config = GenerationConfig(
...     max_new_tokens=50, do_sample=True, top_k=50, eos_token_id=model.config.eos_token_id
... )
>>> generation_config.save_pretrained("my_account/my_model", push_to_hub=True)  # doctest: +SKIP
```

يمكنك أيضًا تخزين العديد من تكوينات التوليد في دليل واحد، باستخدام حجة `config_file_name` في [`GenerationConfig.save_pretrained`]. يمكنك لاحقًا استدعاء مثيلاتها باستخدام [`GenerationConfig.from_pretrained`]. هذا مفيد إذا كنت تريد تخزين العديد من تكوينات التوليد لنموذج واحد (على سبيل المثال، واحد لتوليد نص إبداعي بالنمذجة، وآخر للتلخيص بالبحث الشعاعي). يجب أن يكون لديك الأذونات الصحيحة لـ Hub لإضافة ملفات تكوين إلى نموذج.

```python
>>> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig

>>> tokenizer = AutoTokenizer.from_pretrained("google-t5/t5-small")
>>> model = AutoModelForSeq2SeqLM.from_pretrained("google-t5/t5-small")

>>> translation_generation_config = GenerationConfig(
...     num_beams=4,
...     early_stopping=True,
...     decoder_start_token_id=0,
...     eos_token_id=model.config.eos_token_id,
...     pad_token=model.config.pad_token_id,
... )

>>> # Tip: add `push_to_hub=True` to push to the Hub
>>> translation_generation_config.save_pretrained("/tmp", "translation_generation_config.json")

>>> # You could then use the named generation config file to parameterize generation
>>> generation_config = GenerationConfig.from_pretrained("/tmp", "translation_generation_config.json")
>>> inputs = tokenizer("translate English to French: Configuration files are easy to use!", return_tensors="pt")
>>> outputs = model.generate(**inputs, generation_config=generation_config)
>>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True))
['Les fichiers de configuration sont faciles à utiliser!']
```

## البث

تدعم طريقة `generate()` البث، من خلال إدخالها `streamer`. مدخلات `streamer` متوافقة مع أي مثيل من فئة تحتوي على الطرق التالية: `put()` و`end()`. داخليًا، يتم استخدام `put()` لدفع الرموز المميزة الجديدة ويتم استخدام `end()` للإشارة إلى نهاية توليد النص.

<Tip warning={true}>
لا يزال API لفئات البث قيد التطوير وقد يتغير في المستقبل.
</Tip>

من الناحية العملية، يمكنك إنشاء فئة بث مخصصة لجميع أنواع الأغراض! لدينا أيضًا فئات بث أساسية جاهزة للاستخدام. على سبيل المثال، يمكنك استخدام فئة [`TextStreamer`] لبث إخراج `generate()` إلى شاشتك، كلمة واحدة في كل مرة:

```python
>>> from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer

>>> tok = AutoTokenizer.from_pretrained("openai-community/gpt2")
>>> model = AutoModelForCausalLM.from_pretrained("openai-community/gpt2")
>>> inputs = tok(["An increasing sequence: one,"], return_tensors="pt")
>>> streamer = TextStreamer(tok)

>>> # Despite returning the usual output, the streamer will also print the generated text to stdout.
>>> _ = model.generate(**inputs, streamer=streamer, max_new_tokens=20)
An increasing sequence: one, two, three, four, five, six, seven, eight, nine, ten, eleven,
```
بالتأكيد، سأقوم بترجمة النص الموجود في الفقرات والعناوين مع الالتزام بالتعليمات التي قدمتها.

## تكميم ذاكرة التخزين المؤقت للمفاتيح والقيم (KV Cache Quantization)
تدعم طريقة `generate()` تخزين المفاتيح والقيم المؤقتة لتعزيز الكفاءة وتجنب إعادة الحسابات. ومع ذلك، يمكن لذاكرة التخزين المؤقت للمفاتيح والقيم أن تشغل جزءًا كبيرًا من الذاكرة، مما يصبح عنق زجاجة لتوليد السياقات الطويلة، خاصة بالنسبة للنماذج اللغوية الكبيرة. يمكن لتكميم ذاكرة التخزين المؤقت عند استخدام `generate()` أن يقلل بشكل كبير من متطلبات الذاكرة على حساب السرعة.

يستلهم تكميم ذاكرة التخزين المؤقت للمفاتيح والقيم في مكتبة "ترانسفورمرز" إلى حد كبير من الورقة البحثية [KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache] (KIVI: تكميم غير متماثل 2-بت لذاكرة التخزين المؤقت للمفاتيح والقيم دون الحاجة إلى ضبط). وتدعم حاليًا "كوانتو" (quanto) و"HQQ" كخلفيات. لمزيد من المعلومات حول آلية العمل الداخلية، يرجى الرجوع إلى الورقة البحثية.

لتمكين تكميم ذاكرة التخزين المؤقت للمفاتيح والقيم، يجب الإشارة إلى `cache_implementation="quantized"` في `generation_config`. يجب تمرير الحجج المتعلقة بالتكميم إلى `generation_config` إما كقاموس أو كحالة لفئة [`QuantizedCacheConfig`]. يجب عليك الإشارة إلى خلفية التكميم التي تريد استخدامها في [`QuantizedCacheConfig`]، والخلفية الافتراضية هي "كوانتو".

<Tip warning={true}>
يمكن أن يكون تكميم ذاكرة التخزين المؤقت ضارًا إذا كان طول السياق قصيرًا وكانت هناك ذاكرة وصول عشوائي GPU VRAM كافية للتشغيل بدون تكميم ذاكرة التخزين المؤقت.
</Tip>

## إضافة العلامات المائية (Watermarking)
تدعم طريقة `generate()` إضافة العلامات المائية إلى النص المولد عن طريق وضع علامة عشوائية على جزء من الرموز كـ "خضراء". عندما يتم توليد النص، يتم إضافة قيمة "انحياز" صغيرة إلى احتمالات الرموز "الخضراء"، مما يزيد من احتمالية توليدها. يمكن اكتشاف النص الذي يحمل علامة مائية عن طريق حساب نسبة الرموز "الخضراء" في النص وتقدير مدى احتمالية الحصول على تلك الكمية من الرموز "الخضراء" في النص الذي يولده الإنسان. تم اقتراح هذه الاستراتيجية لإضافة العلامات المائية في الورقة البحثية ["On the Reliability of Watermarks for Large Language Models"] (حول موثوقية العلامات المائية للنماذج اللغوية الكبيرة). لمزيد من المعلومات حول آلية عمل إضافة العلامات المائية، يوصى بالرجوع إلى الورقة البحثية.

يمكن استخدام إضافة العلامات المائية مع أي نموذج توليدي في "ترانسفورمرز" ولا يتطلب نموذج تصنيف إضافيًا للكشف عن النص الذي يحمل علامة مائية. لتفعيل إضافة العلامات المائية، قم بتمرير [`WatermarkingConfig`] مع الحجج اللازمة مباشرة إلى طريقة `.generate()` أو أضفها إلى [`GenerationConfig`]. يمكن لاحقًا اكتشاف النص الذي يحمل علامة مائية باستخدام [`WatermarkDetector`].

<Tip warning={true}>
يعتمد "واترمارك ديتكتور" (WatermarkDetector) داخليًا على نسبة الرموز "الخضراء"، وما إذا كان النص المولد يتبع نمط التلوين. لهذا السبب، يوصى بإزالة نص المطالبة إذا كان أطول بكثير من النص المولد. قد يكون لهذا أيضًا تأثير عندما يكون أحد التسلسلات في الدفعة أطول، مما يتسبب في حشو الصفوف الأخرى. بالإضافة إلى ذلك، يجب تهيئة الكاشف **باستخدام نفس حجج تكوين العلامة المائية المستخدمة أثناء التوليد**.
</Tip>

دعونا نولد بعض النصوص مع إضافة العلامات المائية. في مقتطف الكود أدناه، نقوم بتعيين الانحياز إلى 2.5 وهي قيمة سيتم إضافتها إلى احتمالات الرموز "الخضراء". بعد توليد النص الذي يحمل علامة مائية، يمكننا تمريره مباشرة إلى `WatermarkDetector` للتحقق مما إذا كان النص مولدًا بواسطة آلة (يخرج القيمة `True` للنص المولد بواسطة آلة والقيمة `False` للنص المولد بواسطة إنسان).

## استراتيجيات فك التشفير (Decoding Strategies)
يمكن استخدام مجموعات معينة من معلمات طريقة `generate()`، وفي النهاية `generation_config`، لتمكين استراتيجيات فك التشفير المحددة. إذا كنت جديدًا على هذا المفهوم، نوصي بقراءة [هذه التدوينة التي توضح كيفية عمل استراتيجيات فك التشفير الشائعة](https://huggingface.co/blog/how-to-generate).

هنا، سنعرض بعض المعلمات التي تتحكم في استراتيجيات فك التشفير ونوضح كيفية استخدامها.

### البحث الجشع (Greedy Search)
يستخدم [`generate`] بشكل افتراضي استراتيجية فك التشفير بالبحث الجشع، لذلك لا يلزم تمرير أي معلمات لتمكينها. وهذا يعني أن معلمة `num_beams` مضبوطة على 1 و`do_sample=False`.

### البحث التبايني (Contrastive Search)
تم اقتراح استراتيجية فك التشفير بالبحث التبايني في الورقة البحثية لعام 2022 بعنوان [A Contrastive Framework for Neural Text Generation] (إطار تبايني لتوليد النص العصبي). وقد أظهرت نتائج متفوقة لتوليد مخرجات طويلة متماسكة وغير متكررة. لمعرفة كيفية عمل البحث التبايني، يرجى الاطلاع على [هذه التدوينة](https://huggingface.co/blog/introducing-csearch).

المعلمتان الرئيسيتان اللتان تمكنان من التحكم في سلوك البحث التبايني هما `penalty_alpha` و`top_k`:

### أخذ العينات متعددة الحدود (Multinomial Sampling)
على عكس البحث الجشع الذي يختار دائمًا الرمز ذو الاحتمالية الأعلى كرموز التالي، فإن أخذ العينات متعددة الحدود (المعروف أيضًا باسم أخذ العينات الأسلافي) يختار الرمز التالي بشكل عشوائي بناءً على توزيع الاحتمالية على المفردات بالكامل التي يقدمها النموذج. يحظى كل رمز باحتمالية غير صفرية بفرصة للاختيار، مما يقلل من خطر التكرار.

لتمكين أخذ العينات متعددة الحدود، قم بضبط `do_sample=True` و`num_beams=1`.
### فك تشفير البحث الشعاعي
على عكس البحث الجشع، يحتفظ فك تشفير البحث الشعاعي بعدة فرضيات في كل خطوة زمنية ويختار في النهاية الفرضية التي لها أعلى احتمال إجمالي للتسلسل بأكمله. تتمثل ميزة ذلك في تحديد تسلسلات عالية الاحتمال تبدأ برموز أولية أقل احتمالًا والتي كان من الممكن تجاهلها بواسطة البحث الجشع.

يمكنك تصور كيفية عمل فك تشفير البحث الشعاعي في [هذا العرض التوضيحي التفاعلي](https://huggingface.co/spaces/m-ric/beam_search_visualizer): اكتب جملتك، وتلاعب بالمعلمات لمشاهدة كيفية تغيير أشعة فك التشفير.

لتمكين استراتيجية فك التشفير هذه، حدد `num_beams` (المعروف أيضًا باسم عدد الفرضيات التي يجب تتبعها) والتي تكون أكبر من 1.

### البحث الشعاعي لعينات متعددة
كما يوحي الاسم، تجمع استراتيجية فك التشفير هذه بين البحث الشعاعي والعينات متعددة الحدود. تحتاج إلى تحديد `num_beams` أكبر من 1، وتعيين `do_sample=True` لاستخدام استراتيجية فك التشفير هذه.

### البحث الشعاعي المتنوع
استراتيجية فك تشفير البحث الشعاعي المتنوع هي امتداد لاستراتيجية البحث الشعاعي التي تتيح توليد مجموعة أكثر تنوعًا من تسلسلات الشعاع للاختيار من بينها. لمعرفة كيفية عملها، راجع [البحث الشعاعي المتنوع: فك تشفير الحلول المتنوعة من نماذج التسلسل العصبي](https://arxiv.org/pdf/1610.02424.pdf).

لدى هذا النهج ثلاثة معلمات رئيسية: `num_beams`، و`num_beam_groups`، و`diversity_penalty`.

تضمن عقوبة التنوع تميز الإخراج عبر المجموعات، ويتم استخدام البحث الشعاعي داخل كل مجموعة.

### فك التشفير التخميني
فك التشفير التخميني (المعروف أيضًا باسم فك التشفير بمساعدة) هو تعديل لاستراتيجيات فك التشفير المذكورة أعلاه، والتي تستخدم نموذج مساعد (يفضل أن يكون أصغر بكثير) بنفس برنامج فك التشفير، لتوليد بعض الرموز المرشحة. ثم يتحقق النموذج الرئيسي من الرموز المرشحة في تمرير أمامي واحد، والذي يسرع عملية فك التشفير. إذا كان `do_sample=True`، فسيتم استخدام التحقق من الرمز مع إعادة أخذ العينات المقدمة في [ورقة فك التشفير التخميني](https://arxiv.org/pdf/2211.17192.pdf).

حاليًا، يتم دعم البحث الجشع والعينات فقط مع فك التشفير بمساعدة، ولا يدعم فك التشفير بمساعدة الإدخالات المجمعة.

لمعرفة المزيد حول فك التشفير بمساعدة، راجع [منشور المدونة هذا](https://huggingface.co/blog/assisted-generation).

لتمكين فك التشفير بمساعدة، قم بتعيين وسيط `assistant_model` بنموذج.

عند استخدام فك التشفير بمساعدة مع طرق العينات، يمكنك استخدام وسيط "درجة الحرارة" للتحكم في العشوائية، تمامًا كما هو الحال في العينات متعددة الحدود. ومع ذلك، في فك التشفير بمساعدة، قد يساعد تقليل درجة الحرارة في تحسين وقت الاستجابة.

بدلاً من ذلك، يمكنك أيضًا تعيين `prompt_lookup_num_tokens` لتشغيل فك التشفير بمساعدة القائم على n-gram، على عكس فك التشفير بمساعدة القائم على النموذج. يمكنك قراءة المزيد عنه [هنا](https://twitter.com/joao_gante/status/1747322413006643259).