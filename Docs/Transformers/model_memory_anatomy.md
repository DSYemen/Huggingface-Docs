# تشريح عملية تدريب النموذج

لفهم تقنيات تحسين الأداء التي يمكن تطبيقها لتحسين كفاءة تدريب النماذج من حيث السرعة واستخدام الذاكرة، من المفيد التعرف على كيفية استخدام وحدة معالجة الرسوميات (GPU) أثناء التدريب، وكيف يختلف الحمل الحسابي حسب العملية المنفذة.

لنبدأ باستكشاف مثال توضيحي على استخدام وحدة معالجة الرسوميات (GPU) وتشغيل تدريب نموذج. ولأغراض التوضيح، سنحتاج إلى تثبيت بعض المكتبات:

```bash
pip install transformers datasets accelerate nvidia-ml-py3
```

تسمح لنا مكتبة `nvidia-ml-py3` بمراقبة استخدام الذاكرة في النماذج من داخل بايثون. قد تكون معتادًا على أمر `nvidia-smi` في المحطة الطرفية - تسمح هذه المكتبة بالوصول إلى نفس المعلومات مباشرة في بايثون.

بعد ذلك، نقوم بإنشاء بعض البيانات الوهمية: رموز تعريف عشوائية بين 100 و30000 وتصنيفات ثنائية لمصنف. في المجموع، نحصل على 512 تسلسلًا، كل منها بطول 512، ونخزنها في [`~datasets.Dataset`] بتنسيق PyTorch.

```py
>>> import numpy as np
>>> from datasets import Dataset

>>> seq_len, dataset_size = 512, 512
>>> dummy_data = {
...     "input_ids": np.random.randint(100, 30000, (dataset_size, seq_len)),
...     "labels": np.randomMultiplier
... }
>>> ds = Dataset.from_dict(dummy_data)
>>> ds.set_format("pt")
```

لطباعة إحصائيات الملخص لاستخدام وحدة معالجة الرسوميات (GPU) وتشغيل التدريب مع [`Trainer`]، نقوم بتعريف دالتين مساعدتين:

```py
>>> from pynvml import *

>>> def print_gpu_utilization():
...     nvmlInit()
...     handle = nvmlDeviceGetHandleByIndex(0)
...     info = nvmlDeviceGetMemoryInfo(handle)
...     print(f"GPU memory occupied: {info.used//1024**2} MB.")

>>> def print_summary(result):
...     print(f"Time: {result.metrics['train_runtime']:.2f}")
...     print(f"Samples/second: {result.metrics['train_samples_per_second']:.2f}")
...     print_gpu_utilization()
```

دعونا نتأكد من أننا نبدأ بذاكرة وحدة معالجة الرسوميات (GPU) فارغة:

```py
>>> print_gpu_utilization()
GPU memory occupied: 0 MB.
```

يبدو ذلك جيدًا: ذاكرة وحدة معالجة الرسوميات (GPU) غير مشغولة كما هو متوقع قبل تحميل أي نماذج. إذا لم يكن الأمر كذلك على جهازك، فتأكد من إيقاف جميع العمليات التي تستخدم ذاكرة وحدة معالجة الرسوميات (GPU). ومع ذلك، لا يمكن للمستخدم استخدام كل ذاكرة وحدة معالجة الرسوميات (GPU) الفارغة. عندما يتم تحميل نموذج إلى وحدة معالجة الرسوميات (GPU)، يتم أيضًا تحميل النواة، والتي يمكن أن تشغل 1-2 جيجابايت من الذاكرة. لمعرفة مقدارها، نقوم بتحميل مصفوفة صغيرة إلى وحدة معالجة الرسوميات (GPU) والتي تؤدي أيضًا إلى تحميل النواة.

```py
>>> import torch

>>> torch.ones((1, 1)).to("cuda")
>>> print_gpu_utilization()
GPU memory occupied: 1343 MB.
```

نلاحظ أن النواة وحدها تشغل 1.3 جيجابايت من ذاكرة وحدة معالجة الرسوميات (GPU). الآن دعونا نرى مقدار المساحة التي يستخدمها النموذج.

## تحميل النموذج

أولاً، نقوم بتحميل نموذج `google-bert/bert-large-uncased`. نقوم بتحميل أوزان النموذج مباشرة إلى وحدة معالجة الرسوميات (GPU) حتى نتمكن من التحقق من مقدار المساحة التي تشغلها الأوزان فقط.

```py
>>> from transformers import AutoModelForSequenceClassification

>>> model = AutoModelForSequenceClassification.from_pretrained("google-bert/bert-large-uncased").to("cuda")
>>> print_gpu_utilization()
GPU memory occupied: 2631 MB.
```

يمكننا أن نرى أن أوزان النموذج وحدها تشغل 1.3 جيجابايت من ذاكرة وحدة معالجة الرسوميات (GPU). يعتمد الرقم الدقيق على وحدة معالجة الرسوميات (GPU) المحددة التي تستخدمها. لاحظ أنه في وحدات معالجة الرسوميات (GPU) الأحدث، قد يشغل النموذج في بعض الأحيان مساحة أكبر نظرًا لأن الأوزان يتم تحميلها بطريقة محسنة تسرع من استخدام النموذج. الآن يمكننا أيضًا التحقق بسرعة مما إذا كنا نحصل على نفس النتيجة كما هو الحال مع `nvidia-smi` CLI:

```bash
nvidia-smi
```

```bash
Tue Jan 11 08:58:05 2022
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  On   | 00000000:00:04.0 Off |                    0 |
| N/A   37C    P0    39W / 300W |   2631MiB / 16160MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      3721      C   ...nvs/codeparrot/bin/python     2629MiB |
+-----------------------------------------------------------------------------+
```

نحصل على نفس الرقم كما هو الحال قبل، ويمكنك أيضًا أن ترى أننا نستخدم وحدة معالجة الرسوميات (GPU) V100 بذاكرة سعة 16 جيجابايت. لذلك الآن يمكننا بدء تدريب النموذج ورؤية كيف يتغير استهلاك ذاكرة وحدة معالجة الرسوميات (GPU). أولاً، نقوم بضبط بعض وسائط التدريب القياسية:

```py
default_args = {
"output_dir": "tmp",
"eval_strategy": "steps",
"num_train_epochs": 1,
"log_level": "error",
"report_to": "none",
}
```

<Tip>

إذا كنت تخطط لتشغيل تجارب متعددة، فمن أجل مسح الذاكرة بشكل صحيح بين التجارب، قم بإعادة تشغيل نواة بايثون بين التجارب.

</Tip>

## استخدام الذاكرة في التدريب الأساسي

دعونا نستخدم [`Trainer`] وندرب النموذج دون استخدام أي تقنيات تحسين أداء وحدة معالجة الرسوميات (GPU) وبحجم دفعة يبلغ 4:

```py
>>> from transformers import TrainingArguments, Trainer, logging

>>> logging.set_verbosity_error()

>>> training_args = TrainingArguments(per_device_train_batch_size=4, **default_args)
>>> trainer = Trainer(model=model, args=training_args, train_dataset=ds)
>>> result = trainer.train()
>>> print_summary(result)
```

```
Time: 57.82
Samples/second: 8.86
GPU memory occupied: 14949 MB.
```

نلاحظ أن حجم دفعة صغير نسبيًا يملأ تقريبًا ذاكرة وحدة معالجة الرسوميات (GPU) بالكامل. ومع ذلك، غالبًا ما يؤدي حجم الدفعة الأكبر إلى تقارب النموذج بشكل أسرع أو أداء أفضل. لذلك، من الناحية المثالية، نريد ضبط حجم الدفعة حسب احتياجات النموذج وليس حسب قيود وحدة معالجة الرسوميات (GPU). ما يثير الاهتمام هو أننا نستخدم ذاكرة أكثر بكثير من حجم النموذج. لفهم السبب في ذلك بشكل أفضل، دعونا نلقي نظرة على عمليات النموذج واحتياجات الذاكرة.

## تشريح عمليات النموذج

تتضمن بنية المحولات ثلاث مجموعات رئيسية من العمليات مجمعة أدناه حسب كثافة الحساب.

1. **عمليات الضرب المصفوفي**

الطبقات الخطية ومكونات الاهتمام متعدد الرؤوس جميعها تقوم بعمليات ضرب المصفوفات المصفوفة. هذه العمليات هي الجزء الأكثر كثافة في الحساب من تدريب المحول.

2. **التطبيع الإحصائي**

Softmax والتطبيع الطبقي أقل كثافة في الحساب من عمليات الضرب المصفوفي، وتنطوي على عملية تقليل واحدة أو أكثر، يتم تطبيق نتيجتها بعد ذلك عبر خريطة.

3. **المشغلون العنصريون**

هذه هي المشغلين المتبقين: **التحيزات، والإسقاط، وتنشيط، والاتصالات المتبقية**. هذه هي العمليات الأقل كثافة في الحساب.

يمكن أن تكون هذه المعرفة مفيدة عند تحليل اختناقات الأداء.

تم اشتقاق هذا الملخص من [Data Movement Is All You Need: A Case Study on Optimizing Transformers 2020](https://arxiv.org/abs/2007.00072)
## تشريح ذاكرة النموذج

لقد رأينا أن تدريب النموذج يستخدم ذاكرة أكثر بكثير من مجرد وضع النموذج على وحدة معالجة الرسومات (GPU). ويرجع ذلك إلى وجود العديد من المكونات أثناء التدريب التي تستخدم ذاكرة GPU. المكونات الموجودة في ذاكرة GPU هي كما يلي:

1. أوزان النموذج
2. حالات المحسن
3. التدرجات
4. تنشيطات للأمام محفوظة لحساب التدرجات
5. المخازن المؤقتة
6. الذاكرة الخاصة بوظيفة معينة

يتطلب النموذج النموذجي الذي يتم تدريبه بدقة مختلطة باستخدام AdamW 18 بايت لكل معلمة نموذج بالإضافة إلى ذاكرة التنشيط. بالنسبة للاستدلال، لا توجد حالات محسن وتدرجات، لذا يمكننا طرح تلك. وبالتالي، ينتهي بنا الأمر بـ 6 بايتات لكل معلمة نموذج للتنبؤ بدقة مختلطة، بالإضافة إلى ذاكرة التنشيط.

دعنا نلقي نظرة على التفاصيل.

**أوزان النموذج:**

- 4 بايتات * عدد المعلمات للتدريب fp32
- 6 بايتات * عدد المعلمات للتدريب الدقيق المختلط (يحافظ على نموذج واحد في fp32 وآخر في fp16 في الذاكرة)

**حالات المحسن:**

- 8 بايتات * عدد المعلمات لمحسن AdamW العادي (يحافظ على حالتين)
- 2 بايت * عدد المعلمات لمحسنات AdamW ذات 8 بتات مثل [bitsandbytes](https://github.com/TimDettmers/bitsandbytes)
- 4 بايتات * عدد المعلمات لمحسنات مثل SGD مع زخم (يحافظ فقط على حالة واحدة)

**التدرجات:**

- 4 بايتات * عدد المعلمات للتدريب fp32 أو الدقة المختلطة (تظل التدرجات دائمًا في fp32)

**التنشيطات للأمام:**

- يعتمد الحجم على العديد من العوامل، والعوامل الرئيسية هي طول التسلسل والحجم المخفي وحجم الدفعة.
هناك المدخلات والمخرجات التي يتم تمريرها وإرجاعها بواسطة الوظائف للأمام والخلف وتنشيطات للأمام محفوظة لحساب التدرجات.

**الذاكرة المؤقتة:**

بالإضافة إلى ذلك، هناك جميع أنواع المتغيرات المؤقتة التي يتم تحريرها بمجرد الانتهاء من الحساب، ولكن في هذه اللحظة قد تتطلب هذه المتغيرات ذاكرة إضافية وقد تدفع إلى نفاد الذاكرة. لذلك، عند الترميز، من المهم التفكير بشكل استراتيجي في مثل هذه المتغيرات المؤقتة وفي بعض الأحيان تحريرها بشكل صريح بمجرد عدم الحاجة إليها.

**الذاكرة الخاصة بوظيفة معينة:**

بعد ذلك، قد يكون لبرنامجك متطلبات ذاكرة خاصة. على سبيل المثال، عند توليد النص باستخدام البحث الشعاعي، يحتاج البرنامج إلى الاحتفاظ بنسخ متعددة من الإدخالات والمخرجات.

**سرعة التنفيذ للأمام مقابل الخلف:**

بالنسبة للطبقات التلافيفية والخطية، هناك 2x فلوب في الخلف مقارنة بالأمام، والتي تترجم عمومًا إلى ~2x أبطأ (في بعض الأحيان أكثر، لأن الأحجام في الخلف تميل إلى أن تكون أكثر إرهاقًا). عادة ما تكون التنشيطات محدودة النطاق الترددي، ومن المعتاد أن يتعين على التنشيط قراءة المزيد من البيانات في الخلف أكثر من الأمام (على سبيل المثال، يقرأ التنشيط للأمام مرة واحدة، ويكتب مرة واحدة، ويقرأ التنشيط للخلف مرتين، gradOutput وخرج للأمام، ويكتب مرة واحدة، gradInput).

كما ترون، هناك بعض الأماكن التي يمكننا فيها توفير ذاكرة GPU أو تسريع العمليات.

الآن بعد أن فهمت ما يؤثر على استخدام GPU وسرعة الحساب، راجع صفحة [أساليب وأدوات التدريب الفعال على GPU واحد](perf_train_gpu_one) للتعرف على تقنيات تحسين الأداء.