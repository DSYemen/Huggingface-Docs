# ุงูุงุณุชุฏูุงู ุจุงุณุชุฎุฏุงู ูุญุฏุฉ ูุนุงูุฌุฉ ุงูุฑุณูููุงุช (GPU)

ุชุนุฏ ูุญุฏุงุช ูุนุงูุฌุฉ ุงูุฑุณูููุงุช (GPU) ุงูุฎูุงุฑ ุงูููุงุณู ูุฃุฌูุฒุฉ ุชุนูู ุงูุขูุฉุ ุนูู ุนูุณ ูุญุฏุงุช ุงููุนุงูุฌุฉ ุงููุฑูุฒูุฉ (CPU)ุ ูุฃููุง ููุญุณููุฉ ูุนุฑุถ ูุทุงู ุงูุฐุงูุฑุฉ ูุงูุชูุงุฒู. ููููุงูุจุฉ ุงูุฃุญุฌุงู ุงูุฃูุจุฑ ููููุงุฐุฌ ุงูุญุฏูุซุฉ ุฃู ูุชุดุบูู ูุฐู ุงูููุงุฐุฌ ุงููุจูุฑุฉ ุนูู ุงูุฃุฌูุฒุฉ ุงูููุฌูุฏุฉ ูุงููุฏููุฉุ ููุงู ุงูุนุฏูุฏ ูู ุงูุชุญุณููุงุช ุงูุชู ููููู ุงุณุชุฎุฏุงููุง ูุชุณุฑูุน ุงูุงุณุชุฏูุงู ุจุงุณุชุฎุฏุงู ูุญุฏุฉ ูุนุงูุฌุฉ ุงูุฑุณูููุงุช. ูู ูุฐุง ุงูุฏูููุ ุณุชุชุนูู ููููุฉ ุงุณุชุฎุฏุงู FlashAttention-2 (ุขููุฉ ุงูุชูุงู ุฃูุซุฑ ููุงุกุฉ ูู ุงุณุชุฎุฏุงู ุงูุฐุงูุฑุฉ)ุ ูBetterTransformer (ูุณุงุฑ ุชูููุฐ ุณุฑูุน ุฃุตูู ูู PyTorch)ุ ูbitsandbytes ูุถุบุท ูููุฐุฌู ุฅูู ุฏูุฉ ุฃูู. ูุฃุฎูุฑูุงุ ุชุนุฑู ุนูู ููููุฉ ุงุณุชุฎุฏุงู ๐ค Optimum ูุชุณุฑูุน ุงูุงุณุชุฏูุงู ุจุงุณุชุฎุฏุงู ONNX Runtime ุนูู ูุญุฏุงุช ูุนุงูุฌุฉ ุงูุฑุณูููุงุช Nvidia ูAMD.

<Tip>

ุชูุทุจู ูุนุธู ุงูุชุญุณููุงุช ุงูููุถุญุฉ ููุง ุฃูุถูุง ุนูู ุฅุนุฏุงุฏุงุช ูุชุนุฏุฏุฉ ููุญุฏุงุช ูุนุงูุฌุฉ ุงูุฑุณูููุงุช!

</Tip>

## FlashAttention-2

<Tip>

FlashAttention-2 ุชุฌุฑูุจู ููุฏ ูุชุบูุฑ ุจุดูู ูุจูุฑ ูู ุงูุฅุตุฏุงุฑุงุช ุงููุณุชูุจููุฉ.

</Tip>

[FlashAttention-2](https://huggingface.co/papers/2205.14135) ูู ุชูููุฐ ุฃุณุฑุน ูุฃูุซุฑ ููุงุกุฉ ูุขููุฉ ุงูุงูุชูุงู ุงูููุงุณูุฉ ุงูุชู ูููู ุฃู ุชุณุฑุน ุงูุงุณุชุฏูุงู ุจุดูู ูุจูุฑ ูู ุฎูุงู:

1. ููุงุฒุงุฉ ุญุณุงุจ ุงูุงูุชูุงู ุจุดูู ุฅุถุงูู ุนูู ุทูู ุงูุชุณูุณู.
2. ุชูุณูู ุงูุนูู ุจูู ุฎููุท ูุญุฏุฉ ูุนุงูุฌุฉ ุงูุฑุณูููุงุช ูุชูููู ุงูุชูุงุตู ููุฑุงุกุงุช/ูุชุงุจุงุช ุงูุฐุงูุฑุฉ ุงููุดุชุฑูุฉ ุจูููุง.

ุชุฏุนู FlashAttention-2 ุญุงูููุง ุงููุนูุงุฑูุงุช ุงูุชุงููุฉ:

- [Bark](https://huggingface.co/docs/transformers/model_doc/bark#transformers.BarkModel)
- [Bart](https://huggingface.co/docs/transformers/model_doc/bart#transformers.BartModel)
- [Cohere](https://huggingface.co/docs/transformers/model_doc/cohere#transformers.CohereModel)
- [Dbrx](https://huggingface.co/docs/transformers/model_doc/dbrx#transformers.DbrxModel)
- [DistilBert](https://huggingface.co/docs/transformers/model_doc/distilbert#transformers.DistilBertModel)
- [Gemma](https://huggingface.co/docs/transformers/model_doc/gemma#transformers.GemmaModel)
- [GPT2](https://huggingface.co/docs/transformers/model_doc/gpt2)
- [GPTBigCode](https://huggingface.co/docs/transformers/model_doc/gpt_bigcode#transformers.GPTBigCodeModel)
- [GPTNeo](https://huggingface.co/docs/transformers/model_doc/gpt_neo#transformers.GPTNeoModel)
- [GPTNeoX](https://huggingface.co/docs/transformers/model_doc/gpt_neox#transformers.GPTNeoXModel)
- [GPT-J](https://huggingface.co/docs/transformers/model_doc/gptj#transformers.GPTJModel)
- [Idefics2](https://huggingface.co/docs/transformers/model_doc/idefics2#transformers.Idefics2Model)
- [Falcon](https://huggingface.co/docs/transformers/model_doc/falcon#transformers.FalconModel)
- [JetMoe](https://huggingface.co/docs/transformers/model_doc/jetmoe#transformers.JetMoeModel)
- [Jamba](https://huggingface.co/%D8%A7%D9%84%D9%85%D9%84ูุงุช/model_doc/jamba#transformers.JambaModel)
- [Llama](https://huggingface.co/docs/transformers/model_doc/llama#transformers.LlamaModel)
- [Llava](https://huggingface.co/docs/transformers/model_doc/llava)
- [Llava-NeXT](https://huggingface.co/docs/transformers/model_doc/llava_next)
- [VipLlava](https://huggingface.co/docs/transformers/model_doc/vipllava)
- [VideoLlava](https://huggingface.co/docs/transformers/model_doc/video_llava)
- [M2M100](https://huggingface.co/docs/transformers/model_doc/m2m_100)
- [MBart](https://huggingface.co/docs/transformers/model_doc/mbart#transformers.MBartModel)
- [Mistral](https://huggingface.co/docs/transformers/model_doc/mistral#transformers.MistralModel)
- [Mixtral](https://huggingface.co/docs/transformers/model_doc/mixtral#transformers.MixtralModel)
- [Musicgen](https://huggingface.co/docs/transformers/model_doc/musicgen#transformers.MusicgenModel)
- [MusicGen Melody](https://huggingface.co/docs/transformers/model_doc/musicgen_melody#transformers.MusicgenMelodyModel)
- [NLLB](https://huggingface.co/docs/transformers/model_doc/nllb)
- [OLMo](https://huggingface.co/docs/transformers/model_doc/olmo#transformers.OlmoModel)
- [OPT](https://huggingface.co/docs/transformers/model_doc/opt#transformers.OPTModel)
- [Phi](https://huggingface.co/docs/transformers/model_doc/phi#transformers.PhiModel)
- [Phi3](https://huggingface.co/docs/transformers/model_doc/phi3#transformers.Phi3Model)
- [StableLm](https://huggingface.co/docs/transformers/model_doc/stablelm#transformers.StableLmModel)
- [Starcoder2](https://huggingface.co/docs/transformers/model_doc/starcoder2#transformers.Starcoder2Model)
- [Qwen2](https://huggingface.co/docs/transformers/model_doc/qwen2#transformers.Qwen2Model)
- [Qwen2MoE](https://huggingface.co/docs/transformers/model_doc/qwen2_moe#transformers.Qwen2MoeModel)
- [Whisper](https://huggingface.co/docs/transformers/model_doc/whisper#transformers.WhisperModel)
- [Wav2Vec2](https://huggingface.co/docs/transformers/model_doc/wav2vec2#transformers.Wav2Vec2Model)
- [Hubert](https://huggingface.co/docs/transformers/model_doc/hubert#transformers.HubertModel)
- [data2vec_audio](https://huggingface.co/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecAudioModel)
- [Sew](https://huggingface.co/docs/transformers/main/en/model_doc/sew#transformers.SEWModel)
- [UniSpeech](https://huggingface.co/docs/transformers/v4.39.3/en/model_doc/unispeech#transformers.UniSpeechModel)
- [unispeech_sat](https://huggingface.co/docs/transformers/v4.39.3/en/model_doc/unispeech-sat#transformers.UniSpeechSatModel)

ููููู ุทูุจ ุฅุถุงูุฉ ุฏุนู FlashAttention-2 ููููุฐุฌ ุขุฎุฑ ูู ุฎูุงู ูุชุญ ูุดููุฉ ุฃู ุทูุจ ุณุญุจ ุนูู GitHub.

ูุจู ุงูุจุฏุกุ ุชุฃูุฏ ูู ุชุซุจูุช FlashAttention-2.

ูุชูููู FlashAttention-2ุ ูุฑุฑ ุงูุญุฌุฉ `attn_implementation="flash_attention_2"` ุฅูู [`~AutoModelForCausalLM.from_pretrained`]:

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaForCausalLM

model_id = "tiiuae/falcon-7b"
tokenizer = AutoTokenizer.from_pretrained(model_id)

model = AutoModelForCausalLM.from_pretrained(
model_id,
torch_dtype=torch.bfloat16,
attn_implementation="flash_attention_2",
)
```

<Tip>

ูููู ุงุณุชุฎุฏุงู FlashAttention-2 ููุท ุนูุฏูุง ูููู ููุน ุจูุงูุงุช ุงููููุฐุฌ `fp16` ุฃู `bf16`. ุชุฃูุฏ ูู ุชุญููู ูููุฐุฌู ุฅูู ููุน ุงูุจูุงูุงุช ุงูููุงุณุจ ูุชุญูููู ุนูู ุฌูุงุฒ ูุฏุนูู ูุจู ุงุณุชุฎุฏุงู FlashAttention-2.

<br>

ููููู ุฃูุถูุง ุชุนููู `use_flash_attention_2=True` ูุชูููู FlashAttention-2 ูููููุง ุฃุตุจุญุช ููููุฉ ูุตุงูุญ `attn_implementation="flash_attention_2"`.

</Tip>

ูููู ุงูุฌูุน ุจูู FlashAttention-2 ูุชูููุงุช ุงูุชุญุณูู ุงูุฃุฎุฑู ูุซู ุงูุถุบุท ูุฒูุงุฏุฉ ุชุณุฑูุน ุงูุงุณุชุฏูุงู. ุนูู ุณุจูู ุงููุซุงูุ ููููู ุงูุฌูุน ุจูู FlashAttention-2 ูุงูุถุบุท 8-ุจุช ุฃู 4-ุจุช:

```py
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaForCausalLM

model_id = "tiiuae/falcon-7b"
tokenizer = AutoTokenizer.from_pretrained(model_id)

# load in 8bit
model = AutoModelForCausalLM.from_pretrained(
model_id,
load_in_8bit=True,
attn_implementation="flash_attention_2",
)

# load in 4bit
model = AutoModelForCausingLM.from_pretrained(
model_id,
load_in_4bit=True,
attn_implementation="flash_attention_2",
)
```
### ุชุณุฑูุน ุงูุฃุฏุงุก ุงููุชููุน

ููููู ุงูุงุณุชูุงุฏุฉ ูู ุชุณุฑูุน ุงูุฃุฏุงุก ุจุดูู ูุจูุฑ ุนูุฏ ุงูุงุณุชูุชุงุฌุ ุฎุงุตุฉ ุจุงููุณุจุฉ ูููุฏุฎูุงุช ุฐุงุช ุงูุชุณูุณูุงุช ุงูุทูููุฉ. ููุน ุฐููุ ูุธุฑูุง ูุฃู FlashAttention-2 ูุง ูุฏุนู ุญุณุงุจ ุฏุฑุฌุงุช ุงูุงูุชูุงู ูุน ุฑููุฒ ุงูุญุดูุ ูุฌุจ ุนููู ูุฏูููุง ุญุดู/ุฅูุบุงุก ุญุดู ุฏุฑุฌุงุช ุงูุงูุชูุงู ููุงุณุชูุชุงุฌ ุงููุฌูุน ุนูุฏูุง ุชุญุชูู ุงูุชุณูุณู ุนูู ุฑููุฒ ุญุดู. ูุคุฏู ูุฐุง ุฅูู ุชุจุงุทุค ูุจูุฑ ูู ุงูุฃุฌูุงู ุงููุฌูุนุฉ ูุน ุฑููุฒ ุงูุญุดู.

ูุชุฌุงูุฒ ุฐููุ ูุฌุจ ุงุณุชุฎุฏุงู FlashAttention-2 ุจุฏูู ุฑููุฒ ุญุดู ูู ุงูุชุณูุณู ุฃุซูุงุก ุงูุชุฏุฑูุจ (ุนู ุทุฑูู ุญุฒู ูุฌููุนุฉ ุจูุงูุงุช ุฃู [ุฑุจุท ุงูุชุณูุณูุงุช](https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py#L516) ุญุชู ุงููุตูู ุฅูู ุทูู ุงูุชุณูุณู ุงูุฃูุตู).

ุจุงููุณุจุฉ ููุฑูุฑ ุฃูุงูู ูุงุญุฏ ุนูู [tiiuae/falcon-7b](https://hf.co/tiiuae/falcon-7b) ุจุทูู ุชุณูุณู ูุจูุบ 4096 ูุฃุญุฌุงู ุฏูุนุงุช ูุฎุชููุฉ ุจุฏูู ุฑููุฒ ุญุดูุ ูููู ุชุณุฑูุน ุงูุฃุฏุงุก ุงููุชููุน ุนูู ุงููุญู ุงูุชุงูู:

ุจุงููุณุจุฉ ููุฑูุฑ ุฃูุงูู ูุงุญุฏ ุนูู [meta-llama/Llama-7b-hf](https://hf.co/meta-llama/Llama-7b-hf) ุจุทูู ุชุณูุณู ูุจูุบ 4096 ูุฃุญุฌุงู ุฏูุนุงุช ูุฎุชููุฉ ุจุฏูู ุฑููุฒ ุญุดูุ ูููู ุชุณุฑูุน ุงูุฃุฏุงุก ุงููุชููุน ุนูู ุงููุญู ุงูุชุงูู:

ุจุงููุณุจุฉ ููุชุณูุณูุงุช ุงูุชู ุชุญุชูู ุนูู ุฑููุฒ ุญุดู (ุชูููุฏ ุจุงุณุชุฎุฏุงู ุฑููุฒ ุญุดู)ุ ูุฌุจ ุฅูุบุงุก ุญุดู/ุญุดู ุชุณูุณูุงุช ุงููุฏุฎูุงุช ูุญุณุงุจ ุฏุฑุฌุงุช ุงูุงูุชูุงู ุจุดูู ุตุญูุญ. ุจุงุณุชุฎุฏุงู ุทูู ุชุณูุณู ุตุบูุฑ ูุณุจููุงุ ูุคุฏู ุงููุฑูุฑ ุงูุฃูุงูู ุงููุงุญุฏ ุฅูู ุฒูุงุฏุฉ ุงูุนุจุก ุชุคุฏู ุฅูู ุชุณุฑูุน ุงูุฃุฏุงุก ุงูุทููู (ูู ุงููุซุงู ุฃุฏูุงูุ ูุชู ููุก 30% ูู ุงูุฅุฏุฎุงู ุจุฑููุฒ ุงูุญุดู):

ูููู ุจุงููุณุจุฉ ูุฃุทูุงู ุงูุชุณูุณูุงุช ุงูุฃูุจุฑุ ููููู ุชููุน ููุงุฆุฏ ุฃูุจุฑ ูู ุชุณุฑูุน ุงูุฃุฏุงุก:

> ูุนุฏ FlashAttention ุฃูุซุฑ ููุงุกุฉ ูู ุงุณุชุฎุฏุงู ุงูุฐุงูุฑุฉุ ููุง ูุนูู ุฃูู ููููู ุงูุชุฏุฑูุจ ุนูู ุฃุทูุงู ุชุณูุณูุงุช ุฃูุจุฑ ุจูุซูุฑ ุฏูู ููุงุฌูุฉ ูุดููุงุช ููุงุฏ ุงูุฐุงูุฑุฉ. ููููู ุชูููู ุงุณุชุฎุฏุงู ุงูุฐุงูุฑุฉ ุจูุณุจุฉ ุชุตู ุฅูู 20 ุถุนููุง ูุฃุทูุงู ุงูุชุณูุณูุงุช ุงูุฃูุจุฑ. ุงุทูุน ุนูู ูุณุชูุฏุน [flash-attention](https://github.com/Dao-AILab/flash-attention) ููุฒูุฏ ูู ุงูุชูุงุตูู.

ูุฏุนู PyTorch [`torch.nn.functional.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html) (SDPA) ุฃูุถูุง ุงุณุชุฏุนุงุก FlashAttention ูููุงุฉ ุงูุงูุชูุงู ุงููููุฑุฉ ููุฐุงูุฑุฉ ูู ุงูุฎูููุฉ. ูุฌุฑู ุญุงูููุง ุฅุถุงูุฉ ุฏุนู SDPA ุจุดูู ุฃุตูู ูู Transformers ููุชู ุงุณุชุฎุฏุงูู ุจุดูู ุงูุชุฑุงุถู ูู `torch>=2.1.1` ุนูุฏ ุชููุฑ ุงูุชูููุฐ. ููููู ุฃูุถูุง ุชุนููู `attn_implementation="sdpa"` ูู `from_pretrained()` ูุทูุจ ุงุณุชุฎุฏุงู SDPA ุจุดูู ุตุฑูุญ.

ูู ุงูููุช ุงูุญุงููุ ูุฏุนู Transformers ุงูุงุณุชุฏูุงู ูุงูุชุฏุฑูุจ SDPA ููุนูุงุฑุงุช ุงูุชุงููุฉ:

- [Audio Spectrogram Transformer](https://huggingface.co/docs/transformers/model_doc/audio-spectrogram-transformer#transformers.ASTModel)
- [Bart](https://huggingface.co/docs/transformers/model_doc/bart#transformers.BartModel)
- [Bert](https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertModel)
- [Cohere](https://huggingface.co/docs/transformers/model_doc/cohere#transformers.CohereModel)
- [Dbrx](https://huggingface.co/docs/transformers/model_doc/dbrx#transformers.DbrxModel)
- [DeiT](https://huggingface.co/docs/transformers/model_doc/deit#transformers.DeiTModel)
- [Dpr](https://huggingface.co/docs/transformers/model_doc/dpr#transformers.DprReader)
- [Falcon](https://huggingface.co/docs/transformers/model_doc/falcon#transformers.FalconModel)
- [Gemma](https://huggingface.co/docs/transformers/model_doc/gemma#transformers.GemmaModel)
- [GPTBigCode](https://huggingface.co/docs/transformers/model_doc/gpt_bigcode#transformers.GPTBigCodeModel)
- [JetMoe](https://huggingface.co/docs/transformers/model_doc/jetmoe#transformers.JetMoeModel)
- [Jamba](https://huggingface.co/docs/transformers/model_doc/jamba#transformers.JambaModel)
- [Llama](https://huggingface.co/docs/transformers/model_doc/llama#transformers.LlamaModel)
- [OLMo](https://huggingface.co/docs/transformers/model_doc/olmo#transformers.OlmoModel)
- [PaliGemma](https://huggingface.co/docs/transformers/model_doc/paligemma#transformers.PaliGemmaForConditionalGeneration)
- [Phi](https://huggingface.co/docs/transformers/model_doc/phi#transformers.PhiModel)
- [Idefics](https://huggingface.co/docs/transformers/model_doc/idefics#transformers.IdeficsModel)
- [Whisper](https://huggingface.co/docs/transformers/model_doc/whisper#transformers.WhisperModel)
- [Mistral](https://huggingface.co/docs/transformers/model_doc/mistral#transformers.MistralModel)
- [Mixtral](https://huggingface.co/docs/transformers/model_doc/mixtral#transformers.MixtralModel)
- [StableLm](https://huggingface.co/docs/transformers/model_doc/stablelm#transformers.StableLmModel)
- [Starcoder2](https://huggingface.co/docs/transformers/model_doc/starcoder2#transformers.Starcoder2Model)
- [Qwen2](https://huggingface.co/docs/transformers/model_doc/qwen2#transformers.Qwen2Model)
- [Qwen2MoE](https://huggingface.co/docs/transformers/model_doc/qwen2_moe#transformers.Qwen2MoeModel)
- [Musicgen](https://huggingface.co/docs/transformers/model_doc/musicgen#transformers.MusicgenModel)
- [MusicGen Melody](https://huggingface.co/docs/transformers/model_doc/musicgen_melody#transformers.MusicgenMelodyModel)
- [ViT](https://huggingface.co/docs/transformers/model_doc/vit#transformers.ViTModel)
- [ViTHybrid](https://huggingface.co/docs/transformers/model_doc/vit_hybrid#transformers.ViTHybridModel)
- [ViTMAE](https://huggingface.co/docs/transformers/model_doc/vit_mae#transformers.ViTMAEModel)
- [ViTMSN](https://huggingface.co/docs/transformers/model_doc/vit_msn#transformers.ViTMSNModel)
- [VideoMAE](https://huggingface.co/docs/transformers/model_doc/videomae#transformers.VideoMAEModell)
- [wav2vec2](https://huggingface.co/docs/transformers/model_doc/wav2vec2#transformers.Wav2Vec2Model)
- [Hubert](https://huggingface.co/docs/transformers/model_doc/hubert#transformers.HubertModel)
- [data2vec_audio](https://huggingface.co/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecAudioModel)
- [Sew](https://huggingface.co/docs/transformers/main/en/model_doc/sew#transformers.SEWModel)
- [UniSpeech](https://huggingface.co/docs/transformers/v4.39.3/en/model_doc/unispeech#transformers.UniSpeechModel)
- [unispeech_sat](https://huggingface.co/docs/transformers/v4.39.3/en/model_doc/unispeech-sat#transformers.UniSpeechSatModel)
- [YOLOS](https://huggingface.co/docs/transformers/model_doc/yolos#transformers.YolosModel)

> ูููู ุงุณุชุฎุฏุงู FlashAttention ููุท ููููุงุฐุฌ ุฐุงุช ุงูููุน `fp16` ุฃู `bf16` torchุ ูุฐุง ุชุฃูุฏ ูู ุชุญููู ูููุฐุฌู ุฅูู ุงูููุน ุงูููุงุณุจ ุฃููุงู. ูููู ูููุตุฉ ุงูุงูุชูุงู ุงููููุฑุฉ ููุฐุงูุฑุฉ ุงูุชุนุงูู ูุน ููุงุฐุฌ `fp32`.

> ูุง ูุฏุนู SDPA ูุฌููุนุงุช ูุนููุฉ ูู ูุนููุงุช ุงูุงูุชูุงูุ ูุซู `head_mask` ู`output_attentions=True`. ูู ูุฐู ุงูุญุงูุฉุ ูุฌุจ ุฃู ุชุดุงูุฏ ุฑุณุงูุฉ ุชุญุฐูุฑ ูุณูุนูุฏ ุฅูู ุงูุชูููุฐ (ุงูุฃุจุทุฃ).

ุจุดูู ุงูุชุฑุงุถูุ ูุฎุชุงุฑ SDPA ููุงุฉ ุงูุฃุฏุงุก ุงูุฃูุซุฑ ููุงุกุฉ ุงููุชุงุญุฉุ ูููู ููููู ุงูุชุญูู ููุง ุฅุฐุง ูุงูุช ููุตุฉ ูุชููุฑุฉ ูู ุฅุนุฏุงุฏ ูุนูู (ุงูุฃุฌูุฒุฉุ ูุญุฌู ุงููุดููุฉ) ุจุงุณุชุฎุฏุงู [`torch.backends.cuda.sdp_kernel`](https://pytorch.org/docs/master/backends.html#torch.backends.cuda.sdp_kernel) ููุฏูุฑ ุณูุงู:

ุฅุฐุง ุฑุฃูุช ุฎุทุฃ ูุน ุชุชุจุน ุงูููุฏุณ ุฃุฏูุงูุ ูุฌุฑูุจ ุงุณุชุฎุฏุงู ุงูุฅุตุฏุงุฑ ุงููููู ูู PyTorch ุงูุฐู ูุฏ ูููู ูู ุชุบุทูุฉ ุฃูุณุน ูู FlashAttention:

```bash
RuntimeError: No available kernel. Aborting execution.

# install PyTorch nightly
pip3 install -U --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu118
```
## BetterTransformer

ูููุฑ BetterTransformer ุชุณุฑูุนูุง ููุงุณุชุฏูุงู ูู ุฎูุงู ุชูููุฐ fastpath (ุชูููุฐ ูุชุฎุตุต ูู PyTorch ุงูุฃุตูู ููุธุงุฆู Transformer). ููุงู ุชุญุณููุงู ูู ุชูููุฐ fastpath:

1. ุงูุงูุฏูุงุฌุ ุงูุฐู ูุฌูุน ุจูู ุนุฏุฉ ุนูููุงุช ูุชุชุงููุฉ ูู "kernel" ูุงุญุฏ ูุชูููู ุนุฏุฏ ุฎุทูุงุช ุงูุญุณุงุจ.
2. ุชุฎุทู ูุฏุฑุฉ ุงูุชููููุฒ ุงููุงุฑุบุฉ ุงููุชุฃุตูุฉ ูุชุฌูุจ ุงูุญุณุงุจุงุช ุบูุฑ ุงูุถุฑูุฑูุฉ ูุน ุงูุชูุณูุฑุงุช ุงูููุนุดููุดุฉ.

ูุญูู BetterTransformer ุฃูุถูุง ุฌููุน ุนูููุงุช ุงูุงูุชุจุงู ูุงุณุชุฎุฏุงู [scaled dot product attention (SDPA)](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention) ุงูุฃูุซุฑ ููุงุกุฉ ูู ุงูุฐุงูุฑุฉุ ููุง ุฃูู ูุณุชุฏุนู ููุฑููุงุช ููุญุณูููุฉ ูุซู [FlashAttention](https://huggingface.co/papers/2205.14135) ูู ุงูุฎูููุฉ.

ูุจู ุฃู ุชุจุฏุฃุ ุชุฃูุฏ ูู ุชุซุจูุช ๐ค Optimum [installed](https://huggingface.co/docs/optimum/installation).

ุจุนุฏ ุฐููุ ููููู ุชูููู BetterTransformer ุจุงุณุชุฎุฏุงู ุทุฑููุฉ [`PreTrainedModel.to_bettertransformer`]:

```python
model = model.to_bettertransformer()
```

ููููู ุฅุนุงุฏุฉ ูููุฐุฌ Transformers ุงูุฃุตูู ุจุงุณุชุฎุฏุงู ุทุฑููุฉ [`~PreTrainedModel.reverse_bettertransformer`]. ูุฌุจ ุงุณุชุฎุฏุงู ูุฐุง ูุจู ุญูุธ ูููุฐุฌู ูุงุณุชุฎุฏุงู ุงูููุฐุฌุฉ ุงูููุงุณูุฉ ูู Transformers:

```py
model = model.reverse_bettertransformer()
model.save_pretrained("saved_model")
```

## bitsandbytes

bitsandbytes ูู ููุชุจุฉ ููุชุญููู ุงูููู ุชุชุถูู ุฏุนููุง ููุชุญููู ุงูููู 4-ุจุช ู8-ุจุช. ูููู ุงูุชุญููู ุงูููู ุญุฌู ูููุฐุฌู ููุงุฑูุฉ ุจุฅุตุฏุงุฑู ุงููุงูู ุงูุฏูุฉ ุงูุฃุตููุ ููุง ูุณูู ูุถุน ุงูููุงุฐุฌ ุงููุจูุฑุฉ ุนูู ูุญุฏุงุช ูุนุงูุฌุฉ ุงูุฑุณููุงุช (GPU) ุฐุงุช ุงูุฐุงูุฑุฉ ุงููุญุฏูุฏุฉ.

ุชุฃูุฏ ูู ุชุซุจูุช bitsandbytes ู๐ค Accelerate:

```bash
# ูุฐู ุงูุฅุตุฏุงุฑุงุช ุชุฏุนู 8-ุจุช ู4-ุจุช
pip install bitsandbytes>=0.39.0 accelerate>=0.20.0

# ุชุซุจูุช Transformers
pip install transformers
```

### 4-ุจุช

ูุชุญููู ูููุฐุฌ ูู 4-ุจุช ููุงุณุชุฏูุงูุ ุงุณุชุฎุฏู ูุนููุฉ `load_in_4bit`. ูุนููุฉ `device_map` ุงุฎุชูุงุฑูุฉุ ูููู ููุตู ุจุชุนููููุง ุฅูู `"auto"` ููุณูุงุญ ูู ๐ค Accelerate ุจุชุฎุตูุต ุงููููุฐุฌ ุชููุงุฆููุง ูุจููุงุกุฉ ุจุงููุธุฑ ุฅูู ุงูููุงุฑุฏ ุงููุชุงุญุฉ ูู ุงูุจูุฆุฉ.

```py
from transformers import AutoModelForCausalLM

model_name = "bigscience/bloom-2b5"
model_4bit = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", load_in_4bit=True)
```

ูุชุญููู ูููุฐุฌ ูู 4-ุจุช ููุงุณุชุฏูุงู ุจุงุณุชุฎุฏุงู ูุญุฏุงุช ูุนุงูุฌุฉ ุฑุณููุงุช ูุชุนุฏุฏุฉุ ููููู ุงูุชุญูู ูู ููุฏุงุฑ ุฐุงูุฑุฉ GPU ุงูุชู ุชุฑูุฏ ุชุฎุตูุตูุง ููู GPU. ุนูู ุณุจูู ุงููุซุงูุ ูุชูุฒูุน 600 ููุฌุงุจุงูุช ูู ุงูุฐุงูุฑุฉ ุนูู GPU ุงูุฃูู ู1 ุฌูุฌุงุจุงูุช ูู ุงูุฐุงูุฑุฉ ุนูู GPU ุงูุซุงูู:

```py
max_memory_mapping = {0: "600MB", 1: "1GB"}
model_name = "bigscience/bloom-3b"
model_4bit = AutoModelForCausalLM.from_pretrained(
model_name, device_map="auto", load_in_4bit=True, max_memory=max_memory_mapping
)
```

### 8-ุจุช

ุฅุฐุง ููุช ูุถููููุง ูููุชููุง ุจูุนุฑูุฉ ุงููุฒูุฏ ุนู ุงูููุงููู ุงูุฃุณุงุณูุฉ ููุชุญููู ุงูููู 8-ุจุชุ ูุงูุฑุฃ ููุดูุฑ ุงููุฏููุฉ [Gentle Introduction to 8-bit Matrix Multiplication for transformers at scale using Hugging Face Transformers, Accelerate and bitsandbytes](https://huggingface.co/blog/hf-bitsandbytes-integration).

ูุชุญููู ูููุฐุฌ ูู 8-ุจุช ููุงุณุชุฏูุงูุ ุงุณุชุฎุฏู ูุนููุฉ `load_in_8bit`. ูุนููุฉ `device_map` ุงุฎุชูุงุฑูุฉุ ูููู ููุตู ุจุชุนููููุง ุฅูู `"auto"` ููุณูุงุญ ูู ๐ค Accelerate ุจุชุฎุตูุต ุงููููุฐุฌ ุชููุงุฆููุง ูุจููุงุกุฉ ุจุงููุธุฑ ุฅูู ุงูููุงุฑุฏ ุงููุชุงุญุฉ ูู ุงูุจูุฆุฉ:

```py
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

model_name = "bigscience/bloom-2b5"
model_8bit = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=BitsAndBytesConfig(load_in_8bit=True))
```

ุฅุฐุง ููุช ุชููู ุจุชุญููู ูููุฐุฌ ูู 8-ุจุช ูุชูููุฏ ุงููุตุ ููุฌุจ ุงุณุชุฎุฏุงู ุทุฑููุฉ [`~transformers.GenerationMixin.generate`] ุจุฏูุงู ูู ูุธููุฉ [`Pipeline`] ุงูุชู ูุง ุชููู ููุญุณูููุฉ ูููุงุฐุฌ 8-ุจุช ูุณุชููู ุฃุจุทุฃ. ูุง ุชุฏุนู ุจุนุถ ุงุณุชุฑุงุชูุฌูุงุช ุฃุฎุฐ ุงูุนููุงุชุ ูุซู ุฃุฎุฐ ุงูุนููุงุช ุงูููููุฉุ ุจูุงุณุทุฉ [`Pipeline`] ูููุงุฐุฌ 8-ุจุช. ูุฌุจ ุฃูุถูุง ูุถุน ุฌููุน ุงููุฏุฎูุงุช ุนูู ููุณ ุงูุฌูุงุฒ ูุซู ุงููููุฐุฌ:

```py
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

model_name = "bigscience/bloom-2b5"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model_8bit = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=BitsAndBytesConfig(load_in_8bit=True))

prompt = "Hello, my llama is cute"
inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
generated_ids = model.generate(**inputs)
outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
```

ูุชุญููู ูููุฐุฌ ูู 4-ุจุช ููุงุณุชุฏูุงู ุจุงุณุชุฎุฏุงู ูุญุฏุงุช ูุนุงูุฌุฉ ุฑุณููุงุช ูุชุนุฏุฏุฉุ ููููู ุงูุชุญูู ูู ููุฏุงุฑ ุฐุงูุฑุฉ GPU ุงูุชู ุชุฑูุฏ ุชุฎุตูุตูุง ููู GPU. ุนูู ุณุจูู ุงููุซุงูุ ูุชูุฒูุน 1 ุฌูุฌุงุจุงูุช ูู ุงูุฐุงูุฑุฉ ุนูู GPU ุงูุฃูู ู2 ุฌูุฌุงุจุงูุช ูู ุงูุฐุงูุฑุฉ ุนูู GPU ุงูุซุงูู:

```py
max_memory_mapping = {0: "1GB", 1: "2GB"}
model_name = "bigscience/bloom-3b"
model_8bit = AutoModelForCausalLM.from_pretrained(
model_name, device_map="auto", load_in_8bit=True, max_memory=max_memory_mapping
)
```

ุฌุฑุจ ุชุดุบูู ูููุฐุฌ T5 ุจุญุฌู 11 ูููุงุฑ ูุนููุฉ [T5 model] (https://colab.research.google.com/drive/1YORPWx4okIHXnjW7MSAidXN29mPVNT7F?usp=sharing) ุฃู ูููุฐุฌ BLOOM ุจุญุฌู 3 ูููุงุฑุงุช ูุนููุฉ [BLOOM model] (https://colab.research.google.com/drive/1qOjXfQIAULfKvZqwCen8-MoWKGdSatZ4?usp=sharing) ููุงุณุชุฏูุงู ุนูู ูุญุฏุงุช ูุนุงูุฌุฉ ุงูุฑุณููุงุช (GPU) ูู ุงููุณุชูู ุงููุฌุงูู ูู Google Colab!

## ๐ค Optimum

ููุนุฑูุฉ ุงููุฒูุฏ ูู ุงูุชูุงุตูู ุญูู ุงุณุชุฎุฏุงู ORT ูุน ๐ค Optimumุ ุฑุงุฌุน ุฃุฏูุฉ [Accelerated inference on NVIDIA GPUs](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/gpu#accelerated-inference-on-nvidia-gpus) ู[Accelerated inference on AMD GPUs](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/amdgpu#accelerated-inference-on-amd-gpus). ููุฏู ูุฐุง ุงููุณู ููุท ูุซุงููุง ููุฌุฒูุง ูุจุณูุทูุง.

ONNX Runtime (ORT) ูู ูุณุฑุน ูููุฐุฌ ูุฏุนู ุงูุงุณุชุฏูุงู ุงููุนุฌู ุนูู ูุญุฏุงุช ูุนุงูุฌุฉ ุงูุฑุณููุงุช (GPU) ูู Nvidiaุ ููุญุฏุงุช ูุนุงูุฌุฉ ุงูุฑุณููุงุช (GPU) ูู AMD ุงูุชู ุชุณุชุฎุฏู [ROCm](https://www.amd.com/en/products/software/rocm.html) stack. ูุณุชุฎุฏู ORT ุชูููุงุช ุงูุชุญุณูู ูุซู ุฏูุฌ ุงูุนูููุงุช ุงูุดุงุฆุนุฉ ูู ุนูุฏุฉ ูุงุญุฏุฉ ูุทู ุงูุซูุงุจุช ูุชูููู ุนุฏุฏ ุงูุญุณุงุจุงุช ุงูุชู ูุชู ุฅุฌุฑุงุคูุง ูุชุณุฑูุน ุงูุงุณุชุฏูุงู. ููุง ูุถุน ORT ุงูุนูููุงุช ุงูุฃูุซุฑ ูุซุงูุฉ ุญุณุงุจูุฉ ุนูู ูุญุฏุฉ ูุนุงูุฌุฉ ุงูุฑุณููุงุช (GPU) ูุจููุฉ ุงูุนูููุงุช ุนูู ูุญุฏุฉ ุงููุนุงูุฌุฉ ุงููุฑูุฒูุฉ (CPU) ูุชูุฒูุน ุนุจุก ุงูุนูู ุจูู ุงูุฌูุงุฒูู ุจุฐูุงุก.

ูุฏุนู ๐ค Optimum ุงุณุชุฎุฏุงู ONNX Runtimeุ ูุงูุฐู ูููู ุงุณุชุฎุฏุงูู ูู ๐ค Transformers. ุณุชุญุชุงุฌ ุฅูู ุงุณุชุฎุฏุงู [`~optimum.onnxruntime.ORTModel`] ูููููุฉ ุงูุชู ุชุญุงูู ุญููุงุ ูุชุญุฏูุฏ ูุนููุฉ `provider` ุงูุชู ูููู ุชุนููููุง ุฅูู [`CUDAExecutionProvider`]ุ ุฃู [`ROCMExecutionProvider`]ุ ุฃู [`TensorrtExecutionProvider`]. ุฅุฐุง ููุช ุชุฑูุฏ ุชุญููู ูููุฐุฌ ูู ูุชู ุชุตุฏูุฑู ุจุนุฏ ุฅูู ONNXุ ูููููู ุชุนููู `export=True` ูุชุญููู ูููุฐุฌู ุฃุซูุงุก ุงูุชููู ุฅูู ุชูุณูู ONNX:

```py
from optimum.onnxruntime import ORTModelForSequenceClassification

ort_model = ORTModelForSequenceClassification.from_pretrained(
"distilbert/distilbert-base-uncased-finetuned-sst-2-english",
export=True,
provider="CUDAExecutionProvider",
)
```

ุงูุขู ููููู ุงุณุชุฎุฏุงู ุงููููุฐุฌ ููุงุณุชุฏูุงู:

```py
from optimum.pipelines import pipeline
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("distilbert/distilbert-base-uncased-finetuned-sst-2-english")

pipeline = pipeline(task="text-classification", model=ort_model, tokenizer=tokenizer, device="cuda:0")
result = pipeline("Both the music and visual were astounding, not to mention the actors performance.")
```

## ุงูุฌูุน ุจูู ุงูุชุญุณููุงุช

ุบุงูุจูุง ูุง ูููู ูู ุงููููู ุงูุฌูุน ุจูู ุนุฏุฉ ุชูููุงุช ุชุญุณูู ููุตููุฉ ุฃุนูุงู ููุญุตูู ุนูู ุฃูุถู ุฃุฏุงุก ุงุณุชุฏูุงูู ูููู ููููุฐุฌู. ุนูู ุณุจูู ุงููุซุงูุ ููููู ุชุญููู ูููุฐุฌ ูู 4-ุจุชุ ุซู ุชูููู BetterTransformer ูุน FlashAttention:

```py
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

# ุชุญููู ุงููููุฐุฌ ูู 4-ุจุช
quantization_config = BitsAndBytesConfig(
load_in_4bit=True,
bnb_4bit_compute_dtype=torch.float16
)

tokenizer = AutoTokenizer.from_pretrained("facebook/opt-350m")
model = AutoModelForCausalLM.from_pretrained("facebook/opt-350m", quantization_config=quantization_config)

# ุชูููู BetterTransformer
model = model.to_bettertransformer()

input_text = "Hello my dog is cute and"
inputs = tokenizer(input_text, return_tensors="pt").to("cuda")

# ุชูููู FlashAttention
with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):
outputs = model.generate(**inputs)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```