# Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ­Ø¯Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³ÙˆÙ…ÙŠØ§Øª (GPU)

ØªØ¹Ø¯ ÙˆØ­Ø¯Ø§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³ÙˆÙ…ÙŠØ§Øª (GPU) Ø§Ù„Ø®ÙŠØ§Ø± Ø§Ù„Ù‚ÙŠØ§Ø³ÙŠ Ù„Ø£Ø¬Ù‡Ø²Ø© ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„Ø©ØŒ Ø¹Ù„Ù‰ Ø¹ÙƒØ³ ÙˆØ­Ø¯Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø±ÙƒØ²ÙŠØ© (CPU)ØŒ Ù„Ø£Ù†Ù‡Ø§ Ù…ÙØ­Ø³Ù‘Ù†Ø© Ù„Ø¹Ø±Ø¶ Ù†Ø·Ø§Ù‚ Ø§Ù„Ø°Ø§ÙƒØ±Ø© ÙˆØ§Ù„ØªÙˆØ§Ø²ÙŠ. ÙˆÙ„Ù…ÙˆØ§ÙƒØ¨Ø© Ø§Ù„Ø£Ø­Ø¬Ø§Ù… Ø§Ù„Ø£ÙƒØ¨Ø± Ù„Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø­Ø¯ÙŠØ«Ø© Ø£Ùˆ Ù„ØªØ´ØºÙŠÙ„ Ù‡Ø°Ù‡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ÙƒØ¨ÙŠØ±Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¬Ù‡Ø²Ø© Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙˆØ§Ù„Ù‚Ø¯ÙŠÙ…Ø©ØŒ Ù‡Ù†Ø§Ùƒ Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„ØªÙŠ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ù„ØªØ³Ø±ÙŠØ¹ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ­Ø¯Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³ÙˆÙ…ÙŠØ§Øª. ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø¯Ù„ÙŠÙ„ØŒ Ø³ØªØªØ¹Ù„Ù… ÙƒÙŠÙÙŠØ© Ø§Ø³ØªØ®Ø¯Ø§Ù… FlashAttention-2 (Ø¢Ù„ÙŠØ© Ø§Ù‡ØªÙ…Ø§Ù… Ø£ÙƒØ«Ø± ÙƒÙØ§Ø¡Ø© ÙÙŠ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø©)ØŒ ÙˆBetterTransformer (Ù…Ø³Ø§Ø± ØªÙ†ÙÙŠØ° Ø³Ø±ÙŠØ¹ Ø£ØµÙ„ÙŠ ÙÙŠ PyTorch)ØŒ Ùˆbitsandbytes Ù„Ø¶ØºØ· Ù†Ù…ÙˆØ°Ø¬Ùƒ Ø¥Ù„Ù‰ Ø¯Ù‚Ø© Ø£Ù‚Ù„. ÙˆØ£Ø®ÙŠØ±Ù‹Ø§ØŒ ØªØ¹Ø±Ù Ø¹Ù„Ù‰ ÙƒÙŠÙÙŠØ© Ø§Ø³ØªØ®Ø¯Ø§Ù… ğŸ¤— Optimum Ù„ØªØ³Ø±ÙŠØ¹ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ONNX Runtime Ø¹Ù„Ù‰ ÙˆØ­Ø¯Ø§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³ÙˆÙ…ÙŠØ§Øª Nvidia ÙˆAMD.

<Tip>

ØªÙ†Ø·Ø¨Ù‚ Ù…Ø¹Ø¸Ù… Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ù…ÙˆØ¶Ø­Ø© Ù‡Ù†Ø§ Ø£ÙŠØ¶Ù‹Ø§ Ø¹Ù„Ù‰ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø© Ù„ÙˆØ­Ø¯Ø§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³ÙˆÙ…ÙŠØ§Øª!

</Tip>

## FlashAttention-2

<Tip>

FlashAttention-2 ØªØ¬Ø±ÙŠØ¨ÙŠ ÙˆÙ‚Ø¯ ÙŠØªØºÙŠØ± Ø¨Ø´ÙƒÙ„ ÙƒØ¨ÙŠØ± ÙÙŠ Ø§Ù„Ø¥ØµØ¯Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ©.

</Tip>

[FlashAttention-2](https://huggingface.co/papers/2205.14135) Ù‡Ùˆ ØªÙ†ÙÙŠØ° Ø£Ø³Ø±Ø¹ ÙˆØ£ÙƒØ«Ø± ÙƒÙØ§Ø¡Ø© Ù„Ø¢Ù„ÙŠØ© Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ø§Ù„Ù‚ÙŠØ§Ø³ÙŠØ© Ø§Ù„ØªÙŠ ÙŠÙ…ÙƒÙ† Ø£Ù† ØªØ³Ø±Ø¹ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø¨Ø´ÙƒÙ„ ÙƒØ¨ÙŠØ± Ù…Ù† Ø®Ù„Ø§Ù„:

1. Ù…ÙˆØ§Ø²Ø§Ø© Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ø¨Ø´ÙƒÙ„ Ø¥Ø¶Ø§ÙÙŠ Ø¹Ù„Ù‰ Ø·ÙˆÙ„ Ø§Ù„ØªØ³Ù„Ø³Ù„.
2. ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¹Ù…Ù„ Ø¨ÙŠÙ† Ø®ÙŠÙˆØ· ÙˆØ­Ø¯Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³ÙˆÙ…ÙŠØ§Øª Ù„ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„ØªÙˆØ§ØµÙ„ ÙˆÙ‚Ø±Ø§Ø¡Ø§Øª/ÙƒØªØ§Ø¨Ø§Øª Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø´ØªØ±ÙƒØ© Ø¨ÙŠÙ†Ù‡Ø§.

ØªØ¯Ø¹Ù… FlashAttention-2 Ø­Ø§Ù„ÙŠÙ‹Ø§ Ø§Ù„Ù…Ø¹Ù…Ø§Ø±ÙŠØ§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©:

- [Bark](https://huggingface.co/docs/transformers/model_doc/bark#transformers.BarkModel)
- [Bart](https://huggingface.co/docs/transformers/model_doc/bart#transformers.BartModel)
- [Cohere](https://huggingface.co/docs/transformers/model_doc/cohere#transformers.CohereModel)
- [Dbrx](https://huggingface.co/docs/transformers/model_doc/dbrx#transformers.DbrxModel)
- [DistilBert](https://huggingface.co/docs/transformers/model_doc/distilbert#transformers.DistilBertModel)
- [Gemma](https://huggingface.co/docs/transformers/model_doc/gemma#transformers.GemmaModel)
- [GPT2](https://huggingface.co/docs/transformers/model_doc/gpt2)
- [GPTBigCode](https://huggingface.co/docs/transformers/model_doc/gpt_bigcode#transformers.GPTBigCodeModel)
- [GPTNeo](https://huggingface.co/docs/transformers/model_doc/gpt_neo#transformers.GPTNeoModel)
- [GPTNeoX](https://huggingface.co/docs/transformers/model_doc/gpt_neox#transformers.GPTNeoXModel)
- [GPT-J](https://huggingface.co/docs/transformers/model_doc/gptj#transformers.GPTJModel)
- [Idefics2](https://huggingface.co/docs/transformers/model_doc/idefics2#transformers.Idefics2Model)
- [Falcon](https://huggingface.co/docs/transformers/model_doc/falcon#transformers.FalconModel)
- [JetMoe](https://huggingface.co/docs/transformers/model_doc/jetmoe#transformers.JetMoeModel)
- [Jamba](https://huggingface.co/%D8%A7%D9%84%D9%85%D9%84ÙØ§Øª/model_doc/jamba#transformers.JambaModel)
- [Llama](https://huggingface.co/docs/transformers/model_doc/llama#transformers.LlamaModel)
- [Llava](https://huggingface.co/docs/transformers/model_doc/llava)
- [Llava-NeXT](https://huggingface.co/docs/transformers/model_doc/llava_next)
- [VipLlava](https://huggingface.co/docs/transformers/model_doc/vipllava)
- [VideoLlava](https://huggingface.co/docs/transformers/model_doc/video_llava)
- [M2M100](https://huggingface.co/docs/transformers/model_doc/m2m_100)
- [MBart](https://huggingface.co/docs/transformers/model_doc/mbart#transformers.MBartModel)
- [Mistral](https://huggingface.co/docs/transformers/model_doc/mistral#transformers.MistralModel)
- [Mixtral](https://huggingface.co/docs/transformers/model_doc/mixtral#transformers.MixtralModel)
- [Musicgen](https://huggingface.co/docs/transformers/model_doc/musicgen#transformers.MusicgenModel)
- [MusicGen Melody](https://huggingface.co/docs/transformers/model_doc/musicgen_melody#transformers.MusicgenMelodyModel)
- [NLLB](https://huggingface.co/docs/transformers/model_doc/nllb)
- [OLMo](https://huggingface.co/docs/transformers/model_doc/olmo#transformers.OlmoModel)
- [OPT](https://huggingface.co/docs/transformers/model_doc/opt#transformers.OPTModel)
- [Phi](https://huggingface.co/docs/transformers/model_doc/phi#transformers.PhiModel)
- [Phi3](https://huggingface.co/docs/transformers/model_doc/phi3#transformers.Phi3Model)
- [StableLm](https://huggingface.co/docs/transformers/model_doc/stablelm#transformers.StableLmModel)
- [Starcoder2](https://huggingface.co/docs/transformers/model_doc/starcoder2#transformers.Starcoder2Model)
- [Qwen2](https://huggingface.co/docs/transformers/model_doc/qwen2#transformers.Qwen2Model)
- [Qwen2MoE](https://huggingface.co/docs/transformers/model_doc/qwen2_moe#transformers.Qwen2MoeModel)
- [Whisper](https://huggingface.co/docs/transformers/model_doc/whisper#transformers.WhisperModel)
- [Wav2Vec2](https://huggingface.co/docs/transformers/model_doc/wav2vec2#transformers.Wav2Vec2Model)
- [Hubert](https://huggingface.co/docs/transformers/model_doc/hubert#transformers.HubertModel)
- [data2vec_audio](https://huggingface.co/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecAudioModel)
- [Sew](https://huggingface.co/docs/transformers/main/en/model_doc/sew#transformers.SEWModel)
- [UniSpeech](https://huggingface.co/docs/transformers/v4.39.3/en/model_doc/unispeech#transformers.UniSpeechModel)
- [unispeech_sat](https://huggingface.co/docs/transformers/v4.39.3/en/model_doc/unispeech-sat#transformers.UniSpeechSatModel)

ÙŠÙ…ÙƒÙ†Ùƒ Ø·Ù„Ø¨ Ø¥Ø¶Ø§ÙØ© Ø¯Ø¹Ù… FlashAttention-2 Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¢Ø®Ø± Ù…Ù† Ø®Ù„Ø§Ù„ ÙØªØ­ Ù…Ø´ÙƒÙ„Ø© Ø£Ùˆ Ø·Ù„Ø¨ Ø³Ø­Ø¨ Ø¹Ù„Ù‰ GitHub.

Ù‚Ø¨Ù„ Ø§Ù„Ø¨Ø¯Ø¡ØŒ ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª FlashAttention-2.

Ù„ØªÙ…ÙƒÙŠÙ† FlashAttention-2ØŒ Ù…Ø±Ø± Ø§Ù„Ø­Ø¬Ø© `attn_implementation="flash_attention_2"` Ø¥Ù„Ù‰ [`~AutoModelForCausalLM.from_pretrained`]:

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaForCausalLM

model_id = "tiiuae/falcon-7b"
tokenizer = AutoTokenizer.from_pretrained(model_id)

model = AutoModelForCausalLM.from_pretrained(
model_id,
torch_dtype=torch.bfloat16,
attn_implementation="flash_attention_2",
)
```

<Tip>

ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… FlashAttention-2 ÙÙ‚Ø· Ø¹Ù†Ø¯Ù…Ø§ ÙŠÙƒÙˆÙ† Ù†ÙˆØ¹ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ `fp16` Ø£Ùˆ `bf16`. ØªØ£ÙƒØ¯ Ù…Ù† ØªØ­ÙˆÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬Ùƒ Ø¥Ù„Ù‰ Ù†ÙˆØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨ ÙˆØªØ­Ù…ÙŠÙ„Ù‡ Ø¹Ù„Ù‰ Ø¬Ù‡Ø§Ø² Ù…Ø¯Ø¹ÙˆÙ… Ù‚Ø¨Ù„ Ø§Ø³ØªØ®Ø¯Ø§Ù… FlashAttention-2.

<br>

ÙŠÙ…ÙƒÙ†Ùƒ Ø£ÙŠØ¶Ù‹Ø§ ØªØ¹ÙŠÙŠÙ† `use_flash_attention_2=True` Ù„ØªÙ…ÙƒÙŠÙ† FlashAttention-2 ÙˆÙ„ÙƒÙ†Ù‡Ø§ Ø£ØµØ¨Ø­Øª Ù…Ù‡Ù…Ù„Ø© Ù„ØµØ§Ù„Ø­ `attn_implementation="flash_attention_2"`.

</Tip>

ÙŠÙ…ÙƒÙ† Ø§Ù„Ø¬Ù…Ø¹ Ø¨ÙŠÙ† FlashAttention-2 ÙˆØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø®Ø±Ù‰ Ù…Ø«Ù„ Ø§Ù„Ø¶ØºØ· Ù„Ø²ÙŠØ§Ø¯Ø© ØªØ³Ø±ÙŠØ¹ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„. Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¬Ù…Ø¹ Ø¨ÙŠÙ† FlashAttention-2 ÙˆØ§Ù„Ø¶ØºØ· 8-Ø¨Øª Ø£Ùˆ 4-Ø¨Øª:

```py
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaForCausalLM

model_id = "tiiuae/falcon-7b"
tokenizer = AutoTokenizer.from_pretrained(model_id)

# load in 8bit
model = AutoModelForCausalLM.from_pretrained(
model_id,
load_in_8bit=True,
attn_implementation="flash_attention_2",
)

# load in 4bit
model = AutoModelForCausingLM.from_pretrained(
model_id,
load_in_4bit=True,
attn_implementation="flash_attention_2",
)
```
### ØªØ³Ø±ÙŠØ¹ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹

ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø§Ø³ØªÙØ§Ø¯Ø© Ù…Ù† ØªØ³Ø±ÙŠØ¹ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø¨Ø´ÙƒÙ„ ÙƒØ¨ÙŠØ± Ø¹Ù†Ø¯ Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬ØŒ Ø®Ø§ØµØ© Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ø°Ø§Øª Ø§Ù„ØªØ³Ù„Ø³Ù„Ø§Øª Ø§Ù„Ø·ÙˆÙŠÙ„Ø©. ÙˆÙ…Ø¹ Ø°Ù„ÙƒØŒ Ù†Ø¸Ø±Ù‹Ø§ Ù„Ø£Ù† FlashAttention-2 Ù„Ø§ ÙŠØ¯Ø¹Ù… Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø§Øª Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ù…Ø¹ Ø±Ù…ÙˆØ² Ø§Ù„Ø­Ø´ÙˆØŒ ÙŠØ¬Ø¨ Ø¹Ù„ÙŠÙƒ ÙŠØ¯ÙˆÙŠÙ‹Ø§ Ø­Ø´Ùˆ/Ø¥Ù„ØºØ§Ø¡ Ø­Ø´Ùˆ Ø¯Ø±Ø¬Ø§Øª Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ù„Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬ Ø§Ù„Ù…Ø¬Ù…Ø¹ Ø¹Ù†Ø¯Ù…Ø§ ØªØ­ØªÙˆÙŠ Ø§Ù„ØªØ³Ù„Ø³Ù„ Ø¹Ù„Ù‰ Ø±Ù…ÙˆØ² Ø­Ø´Ùˆ. ÙŠØ¤Ø¯ÙŠ Ù‡Ø°Ø§ Ø¥Ù„Ù‰ ØªØ¨Ø§Ø·Ø¤ ÙƒØ¨ÙŠØ± ÙÙŠ Ø§Ù„Ø£Ø¬ÙŠØ§Ù„ Ø§Ù„Ù…Ø¬Ù…Ø¹Ø© Ù…Ø¹ Ø±Ù…ÙˆØ² Ø§Ù„Ø­Ø´Ùˆ.

Ù„ØªØ¬Ø§ÙˆØ² Ø°Ù„ÙƒØŒ ÙŠØ¬Ø¨ Ø§Ø³ØªØ®Ø¯Ø§Ù… FlashAttention-2 Ø¨Ø¯ÙˆÙ† Ø±Ù…ÙˆØ² Ø­Ø´Ùˆ ÙÙŠ Ø§Ù„ØªØ³Ù„Ø³Ù„ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ (Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø­Ø²Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø£Ùˆ [Ø±Ø¨Ø· Ø§Ù„ØªØ³Ù„Ø³Ù„Ø§Øª](https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py#L516) Ø­ØªÙ‰ Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø·ÙˆÙ„ Ø§Ù„ØªØ³Ù„Ø³Ù„ Ø§Ù„Ø£Ù‚ØµÙ‰).

Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù…Ø±ÙˆØ± Ø£Ù…Ø§Ù…ÙŠ ÙˆØ§Ø­Ø¯ Ø¹Ù„Ù‰ [tiiuae/falcon-7b](https://hf.co/tiiuae/falcon-7b) Ø¨Ø·ÙˆÙ„ ØªØ³Ù„Ø³Ù„ ÙŠØ¨Ù„Øº 4096 ÙˆØ£Ø­Ø¬Ø§Ù… Ø¯ÙØ¹Ø§Øª Ù…Ø®ØªÙ„ÙØ© Ø¨Ø¯ÙˆÙ† Ø±Ù…ÙˆØ² Ø­Ø´ÙˆØŒ ÙŠÙƒÙˆÙ† ØªØ³Ø±ÙŠØ¹ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Ø­Ùˆ Ø§Ù„ØªØ§Ù„ÙŠ:

Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù…Ø±ÙˆØ± Ø£Ù…Ø§Ù…ÙŠ ÙˆØ§Ø­Ø¯ Ø¹Ù„Ù‰ [meta-llama/Llama-7b-hf](https://hf.co/meta-llama/Llama-7b-hf) Ø¨Ø·ÙˆÙ„ ØªØ³Ù„Ø³Ù„ ÙŠØ¨Ù„Øº 4096 ÙˆØ£Ø­Ø¬Ø§Ù… Ø¯ÙØ¹Ø§Øª Ù…Ø®ØªÙ„ÙØ© Ø¨Ø¯ÙˆÙ† Ø±Ù…ÙˆØ² Ø­Ø´ÙˆØŒ ÙŠÙƒÙˆÙ† ØªØ³Ø±ÙŠØ¹ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Ø­Ùˆ Ø§Ù„ØªØ§Ù„ÙŠ:

Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù„ØªØ³Ù„Ø³Ù„Ø§Øª Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø±Ù…ÙˆØ² Ø­Ø´Ùˆ (ØªÙˆÙ„ÙŠØ¯ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø±Ù…ÙˆØ² Ø­Ø´Ùˆ)ØŒ ÙŠØ¬Ø¨ Ø¥Ù„ØºØ§Ø¡ Ø­Ø´Ùˆ/Ø­Ø´Ùˆ ØªØ³Ù„Ø³Ù„Ø§Øª Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ù„Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø§Øª Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­. Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø·ÙˆÙ„ ØªØ³Ù„Ø³Ù„ ØµØºÙŠØ± Ù†Ø³Ø¨ÙŠÙ‹Ø§ØŒ ÙŠØ¤Ø¯ÙŠ Ø§Ù„Ù…Ø±ÙˆØ± Ø§Ù„Ø£Ù…Ø§Ù…ÙŠ Ø§Ù„ÙˆØ§Ø­Ø¯ Ø¥Ù„Ù‰ Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø¹Ø¨Ø¡ ØªØ¤Ø¯ÙŠ Ø¥Ù„Ù‰ ØªØ³Ø±ÙŠØ¹ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø·ÙÙŠÙ (ÙÙŠ Ø§Ù„Ù…Ø«Ø§Ù„ Ø£Ø¯Ù†Ø§Ù‡ØŒ ÙŠØªÙ… Ù…Ù„Ø¡ 30% Ù…Ù† Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ Ø¨Ø±Ù…ÙˆØ² Ø§Ù„Ø­Ø´Ùˆ):

ÙˆÙ„ÙƒÙ† Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ø£Ø·ÙˆØ§Ù„ Ø§Ù„ØªØ³Ù„Ø³Ù„Ø§Øª Ø§Ù„Ø£ÙƒØ¨Ø±ØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªÙˆÙ‚Ø¹ ÙÙˆØ§Ø¦Ø¯ Ø£ÙƒØ¨Ø± Ù…Ù† ØªØ³Ø±ÙŠØ¹ Ø§Ù„Ø£Ø¯Ø§Ø¡:

> ÙŠØ¹Ø¯ FlashAttention Ø£ÙƒØ«Ø± ÙƒÙØ§Ø¡Ø© ÙÙŠ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø©ØŒ Ù…Ù…Ø§ ÙŠØ¹Ù†ÙŠ Ø£Ù†Ù‡ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¹Ù„Ù‰ Ø£Ø·ÙˆØ§Ù„ ØªØ³Ù„Ø³Ù„Ø§Øª Ø£ÙƒØ¨Ø± Ø¨ÙƒØ«ÙŠØ± Ø¯ÙˆÙ† Ù…ÙˆØ§Ø¬Ù‡Ø© Ù…Ø´ÙƒÙ„Ø§Øª Ù†ÙØ§Ø¯ Ø§Ù„Ø°Ø§ÙƒØ±Ø©. ÙŠÙ…ÙƒÙ†Ùƒ ØªÙ‚Ù„ÙŠÙ„ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø¨Ù†Ø³Ø¨Ø© ØªØµÙ„ Ø¥Ù„Ù‰ 20 Ø¶Ø¹ÙÙ‹Ø§ Ù„Ø£Ø·ÙˆØ§Ù„ Ø§Ù„ØªØ³Ù„Ø³Ù„Ø§Øª Ø§Ù„Ø£ÙƒØ¨Ø±. Ø§Ø·Ù„Ø¹ Ø¹Ù„Ù‰ Ù…Ø³ØªÙˆØ¯Ø¹ [flash-attention](https://github.com/Dao-AILab/flash-attention) Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªÙØ§ØµÙŠÙ„.

ÙŠØ¯Ø¹Ù… PyTorch [`torch.nn.functional.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html) (SDPA) Ø£ÙŠØ¶Ù‹Ø§ Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ FlashAttention ÙˆÙ†ÙˆØ§Ø© Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ø§Ù„Ù…ÙˆÙØ±Ø© Ù„Ù„Ø°Ø§ÙƒØ±Ø© ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ©. ÙŠØ¬Ø±ÙŠ Ø­Ø§Ù„ÙŠÙ‹Ø§ Ø¥Ø¶Ø§ÙØ© Ø¯Ø¹Ù… SDPA Ø¨Ø´ÙƒÙ„ Ø£ØµÙ„ÙŠ ÙÙŠ Transformers ÙˆÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ø¨Ø´ÙƒÙ„ Ø§ÙØªØ±Ø§Ø¶ÙŠ Ù„Ù€ `torch>=2.1.1` Ø¹Ù†Ø¯ ØªÙˆÙØ± Ø§Ù„ØªÙ†ÙÙŠØ°. ÙŠÙ…ÙƒÙ†Ùƒ Ø£ÙŠØ¶Ù‹Ø§ ØªØ¹ÙŠÙŠÙ† `attn_implementation="sdpa"` ÙÙŠ `from_pretrained()` Ù„Ø·Ù„Ø¨ Ø§Ø³ØªØ®Ø¯Ø§Ù… SDPA Ø¨Ø´ÙƒÙ„ ØµØ±ÙŠØ­.

ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ø­Ø§Ù„ÙŠØŒ ÙŠØ¯Ø¹Ù… Transformers Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ ÙˆØ§Ù„ØªØ¯Ø±ÙŠØ¨ SDPA Ù„Ù„Ø¹Ù…Ø§Ø±Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©:

- [Audio Spectrogram Transformer](https://huggingface.co/docs/transformers/model_doc/audio-spectrogram-transformer#transformers.ASTModel)
- [Bart](https://huggingface.co/docs/transformers/model_doc/bart#transformers.BartModel)
- [Bert](https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertModel)
- [Cohere](https://huggingface.co/docs/transformers/model_doc/cohere#transformers.CohereModel)
- [Dbrx](https://huggingface.co/docs/transformers/model_doc/dbrx#transformers.DbrxModel)
- [DeiT](https://huggingface.co/docs/transformers/model_doc/deit#transformers.DeiTModel)
- [Dpr](https://huggingface.co/docs/transformers/model_doc/dpr#transformers.DprReader)
- [Falcon](https://huggingface.co/docs/transformers/model_doc/falcon#transformers.FalconModel)
- [Gemma](https://huggingface.co/docs/transformers/model_doc/gemma#transformers.GemmaModel)
- [GPTBigCode](https://huggingface.co/docs/transformers/model_doc/gpt_bigcode#transformers.GPTBigCodeModel)
- [JetMoe](https://huggingface.co/docs/transformers/model_doc/jetmoe#transformers.JetMoeModel)
- [Jamba](https://huggingface.co/docs/transformers/model_doc/jamba#transformers.JambaModel)
- [Llama](https://huggingface.co/docs/transformers/model_doc/llama#transformers.LlamaModel)
- [OLMo](https://huggingface.co/docs/transformers/model_doc/olmo#transformers.OlmoModel)
- [PaliGemma](https://huggingface.co/docs/transformers/model_doc/paligemma#transformers.PaliGemmaForConditionalGeneration)
- [Phi](https://huggingface.co/docs/transformers/model_doc/phi#transformers.PhiModel)
- [Idefics](https://huggingface.co/docs/transformers/model_doc/idefics#transformers.IdeficsModel)
- [Whisper](https://huggingface.co/docs/transformers/model_doc/whisper#transformers.WhisperModel)
- [Mistral](https://huggingface.co/docs/transformers/model_doc/mistral#transformers.MistralModel)
- [Mixtral](https://huggingface.co/docs/transformers/model_doc/mixtral#transformers.MixtralModel)
- [StableLm](https://huggingface.co/docs/transformers/model_doc/stablelm#transformers.StableLmModel)
- [Starcoder2](https://huggingface.co/docs/transformers/model_doc/starcoder2#transformers.Starcoder2Model)
- [Qwen2](https://huggingface.co/docs/transformers/model_doc/qwen2#transformers.Qwen2Model)
- [Qwen2MoE](https://huggingface.co/docs/transformers/model_doc/qwen2_moe#transformers.Qwen2MoeModel)
- [Musicgen](https://huggingface.co/docs/transformers/model_doc/musicgen#transformers.MusicgenModel)
- [MusicGen Melody](https://huggingface.co/docs/transformers/model_doc/musicgen_melody#transformers.MusicgenMelodyModel)
- [ViT](https://huggingface.co/docs/transformers/model_doc/vit#transformers.ViTModel)
- [ViTHybrid](https://huggingface.co/docs/transformers/model_doc/vit_hybrid#transformers.ViTHybridModel)
- [ViTMAE](https://huggingface.co/docs/transformers/model_doc/vit_mae#transformers.ViTMAEModel)
- [ViTMSN](https://huggingface.co/docs/transformers/model_doc/vit_msn#transformers.ViTMSNModel)
- [VideoMAE](https://huggingface.co/docs/transformers/model_doc/videomae#transformers.VideoMAEModell)
- [wav2vec2](https://huggingface.co/docs/transformers/model_doc/wav2vec2#transformers.Wav2Vec2Model)
- [Hubert](https://huggingface.co/docs/transformers/model_doc/hubert#transformers.HubertModel)
- [data2vec_audio](https://huggingface.co/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecAudioModel)
- [Sew](https://huggingface.co/docs/transformers/main/en/model_doc/sew#transformers.SEWModel)
- [UniSpeech](https://huggingface.co/docs/transformers/v4.39.3/en/model_doc/unispeech#transformers.UniSpeechModel)
- [unispeech_sat](https://huggingface.co/docs/transformers/v4.39.3/en/model_doc/unispeech-sat#transformers.UniSpeechSatModel)
- [YOLOS](https://huggingface.co/docs/transformers/model_doc/yolos#transformers.YolosModel)

> ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… FlashAttention ÙÙ‚Ø· Ù„Ù„Ù†Ù…Ø§Ø°Ø¬ Ø°Ø§Øª Ø§Ù„Ù†ÙˆØ¹ `fp16` Ø£Ùˆ `bf16` torchØŒ Ù„Ø°Ø§ ØªØ£ÙƒØ¯ Ù…Ù† ØªØ­ÙˆÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬Ùƒ Ø¥Ù„Ù‰ Ø§Ù„Ù†ÙˆØ¹ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨ Ø£ÙˆÙ„Ø§Ù‹. ÙŠÙ…ÙƒÙ† Ù„Ù…Ù†ØµØ© Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ø§Ù„Ù…ÙˆÙØ±Ø© Ù„Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ù†Ù…Ø§Ø°Ø¬ `fp32`.

> Ù„Ø§ ÙŠØ¯Ø¹Ù… SDPA Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ù…Ø¹ÙŠÙ†Ø© Ù…Ù† Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…ØŒ Ù…Ø«Ù„ `head_mask` Ùˆ`output_attentions=True`. ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ ÙŠØ¬Ø¨ Ø£Ù† ØªØ´Ø§Ù‡Ø¯ Ø±Ø³Ø§Ù„Ø© ØªØ­Ø°ÙŠØ± ÙˆØ³Ù†Ø¹ÙˆØ¯ Ø¥Ù„Ù‰ Ø§Ù„ØªÙ†ÙÙŠØ° (Ø§Ù„Ø£Ø¨Ø·Ø£).

Ø¨Ø´ÙƒÙ„ Ø§ÙØªØ±Ø§Ø¶ÙŠØŒ ÙŠØ®ØªØ§Ø± SDPA Ù†ÙˆØ§Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø£ÙƒØ«Ø± ÙƒÙØ§Ø¡Ø© Ø§Ù„Ù…ØªØ§Ø­Ø©ØŒ ÙˆÙ„ÙƒÙ† ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…Ù†ØµØ© Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ø¥Ø¹Ø¯Ø§Ø¯ Ù…Ø¹ÙŠÙ† (Ø§Ù„Ø£Ø¬Ù‡Ø²Ø©ØŒ ÙˆØ­Ø¬Ù… Ø§Ù„Ù…Ø´ÙƒÙ„Ø©) Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`torch.backends.cuda.sdp_kernel`](https://pytorch.org/docs/master/backends.html#torch.backends.cuda.sdp_kernel) ÙƒÙ…Ø¯ÙŠØ± Ø³ÙŠØ§Ù‚:

Ø¥Ø°Ø§ Ø±Ø£ÙŠØª Ø®Ø·Ø£ Ù…Ø¹ ØªØªØ¨Ø¹ Ø§Ù„Ù…ÙƒØ¯Ø³ Ø£Ø¯Ù†Ø§Ù‡ØŒ ÙØ¬Ø±Ù‘Ø¨ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¥ØµØ¯Ø§Ø± Ø§Ù„Ù„ÙŠÙ„ÙŠ Ù…Ù† PyTorch Ø§Ù„Ø°ÙŠ Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ù„Ù‡ ØªØºØ·ÙŠØ© Ø£ÙˆØ³Ø¹ Ù„Ù€ FlashAttention:

```bash
RuntimeError: No available kernel. Aborting execution.

# install PyTorch nightly
pip3 install -U --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu118
```
## BetterTransformer

ÙŠÙˆÙØ± BetterTransformer ØªØ³Ø±ÙŠØ¹Ù‹Ø§ Ù„Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ù…Ù† Ø®Ù„Ø§Ù„ ØªÙ†ÙÙŠØ° fastpath (ØªÙ†ÙÙŠØ° Ù…ØªØ®ØµØµ Ù„Ù€ PyTorch Ø§Ù„Ø£ØµÙ„ÙŠ Ù„ÙˆØ¸Ø§Ø¦Ù Transformer). Ù‡Ù†Ø§Ùƒ ØªØ­Ø³ÙŠÙ†Ø§Ù† ÙÙŠ ØªÙ†ÙÙŠØ° fastpath:

1. Ø§Ù„Ø§Ù†Ø¯Ù…Ø§Ø¬ØŒ Ø§Ù„Ø°ÙŠ ÙŠØ¬Ù…Ø¹ Ø¨ÙŠÙ† Ø¹Ø¯Ø© Ø¹Ù…Ù„ÙŠØ§Øª Ù…ØªØªØ§Ù„ÙŠØ© ÙÙŠ "kernel" ÙˆØ§Ø­Ø¯ Ù„ØªÙ‚Ù„ÙŠÙ„ Ø¹Ø¯Ø¯ Ø®Ø·ÙˆØ§Øª Ø§Ù„Ø­Ø³Ø§Ø¨.
2. ØªØ®Ø·ÙŠ Ù†Ø¯Ø±Ø© Ø§Ù„ØªÙˆÙƒÙŠÙ†Ø² Ø§Ù„ÙØ§Ø±ØºØ© Ø§Ù„Ù…ØªØ£ØµÙ„Ø© Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ø­Ø³Ø§Ø¨Ø§Øª ØºÙŠØ± Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ø¹ Ø§Ù„ØªÙ†Ø³ÙˆØ±Ø§Øª Ø§Ù„Ù…ÙØ¹Ø´ÙÙ‘Ø´Ø©.

ÙŠØ­ÙˆÙ„ BetterTransformer Ø£ÙŠØ¶Ù‹Ø§ Ø¬Ù…ÙŠØ¹ Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡ Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… [scaled dot product attention (SDPA)](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention) Ø§Ù„Ø£ÙƒØ«Ø± ÙƒÙØ§Ø¡Ø© ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©ØŒ ÙƒÙ…Ø§ Ø£Ù†Ù‡ ÙŠØ³ØªØ¯Ø¹ÙŠ ÙƒÙŠØ±Ù†Ù„Ø§Øª Ù…ÙØ­Ø³ÙÙ‘Ù†Ø© Ù…Ø«Ù„ [FlashAttention](https://huggingface.co/papers/2205.14135) ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ©.

Ù‚Ø¨Ù„ Ø£Ù† ØªØ¨Ø¯Ø£ØŒ ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª ğŸ¤— Optimum [installed](https://huggingface.co/docs/optimum/installation).

Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªÙ…ÙƒÙŠÙ† BetterTransformer Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø·Ø±ÙŠÙ‚Ø© [`PreTrainedModel.to_bettertransformer`]:

```python
model = model.to_bettertransformer()
```

ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø¹Ø§Ø¯Ø© Ù†Ù…ÙˆØ°Ø¬ Transformers Ø§Ù„Ø£ØµÙ„ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø·Ø±ÙŠÙ‚Ø© [`~PreTrainedModel.reverse_bettertransformer`]. ÙŠØ¬Ø¨ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‡Ø°Ø§ Ù‚Ø¨Ù„ Ø­ÙØ¸ Ù†Ù…ÙˆØ°Ø¬Ùƒ Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…Ø°Ø¬Ø© Ø§Ù„Ù‚ÙŠØ§Ø³ÙŠØ© Ù„Ù€ Transformers:

```py
model = model.reverse_bettertransformer()
model.save_pretrained("saved_model")
```

## bitsandbytes

bitsandbytes Ù‡ÙŠ Ù…ÙƒØªØ¨Ø© Ù„Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙƒÙ…ÙŠ ØªØªØ¶Ù…Ù† Ø¯Ø¹Ù…Ù‹Ø§ Ù„Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙƒÙ…ÙŠ 4-Ø¨Øª Ùˆ8-Ø¨Øª. ÙŠÙ‚Ù„Ù„ Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙƒÙ…ÙŠ Ø­Ø¬Ù… Ù†Ù…ÙˆØ°Ø¬Ùƒ Ù…Ù‚Ø§Ø±Ù†Ø© Ø¨Ø¥ØµØ¯Ø§Ø±Ù‡ Ø§Ù„ÙƒØ§Ù…Ù„ Ø§Ù„Ø¯Ù‚Ø© Ø§Ù„Ø£ØµÙ„ÙŠØŒ Ù…Ù…Ø§ ÙŠØ³Ù‡Ù„ ÙˆØ¶Ø¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ÙƒØ¨ÙŠØ±Ø© Ø¹Ù„Ù‰ ÙˆØ­Ø¯Ø§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³ÙˆÙ…Ø§Øª (GPU) Ø°Ø§Øª Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø­Ø¯ÙˆØ¯Ø©.

ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª bitsandbytes ÙˆğŸ¤— Accelerate:

```bash
# Ù‡Ø°Ù‡ Ø§Ù„Ø¥ØµØ¯Ø§Ø±Ø§Øª ØªØ¯Ø¹Ù… 8-Ø¨Øª Ùˆ4-Ø¨Øª
pip install bitsandbytes>=0.39.0 accelerate>=0.20.0

# ØªØ«Ø¨ÙŠØª Transformers
pip install transformers
```

### 4-Ø¨Øª

Ù„ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ 4-Ø¨Øª Ù„Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ù…Ø¹Ù„Ù…Ø© `load_in_4bit`. Ù…Ø¹Ù„Ù…Ø© `device_map` Ø§Ø®ØªÙŠØ§Ø±ÙŠØ©ØŒ ÙˆÙ„ÙƒÙ† ÙŠÙˆØµÙ‰ Ø¨ØªØ¹ÙŠÙŠÙ†Ù‡Ø§ Ø¥Ù„Ù‰ `"auto"` Ù„Ù„Ø³Ù…Ø§Ø­ Ù„Ù€ ğŸ¤— Accelerate Ø¨ØªØ®ØµÙŠØµ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ ÙˆØ¨ÙƒÙØ§Ø¡Ø© Ø¨Ø§Ù„Ù†Ø¸Ø± Ø¥Ù„Ù‰ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ù…ØªØ§Ø­Ø© ÙÙŠ Ø§Ù„Ø¨ÙŠØ¦Ø©.

```py
from transformers import AutoModelForCausalLM

model_name = "bigscience/bloom-2b5"
model_4bit = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", load_in_4bit=True)
```

Ù„ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ 4-Ø¨Øª Ù„Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ­Ø¯Ø§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø±Ø³ÙˆÙ…Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø©ØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„ØªØ­ÙƒÙ… ÙÙŠ Ù…Ù‚Ø¯Ø§Ø± Ø°Ø§ÙƒØ±Ø© GPU Ø§Ù„ØªÙŠ ØªØ±ÙŠØ¯ ØªØ®ØµÙŠØµÙ‡Ø§ Ù„ÙƒÙ„ GPU. Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ù„ØªÙˆØ²ÙŠØ¹ 600 Ù…ÙŠØ¬Ø§Ø¨Ø§ÙŠØª Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø¹Ù„Ù‰ GPU Ø§Ù„Ø£ÙˆÙ„ Ùˆ1 Ø¬ÙŠØ¬Ø§Ø¨Ø§ÙŠØª Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø¹Ù„Ù‰ GPU Ø§Ù„Ø«Ø§Ù†ÙŠ:

```py
max_memory_mapping = {0: "600MB", 1: "1GB"}
model_name = "bigscience/bloom-3b"
model_4bit = AutoModelForCausalLM.from_pretrained(
model_name, device_map="auto", load_in_4bit=True, max_memory=max_memory_mapping
)
```

### 8-Ø¨Øª

Ø¥Ø°Ø§ ÙƒÙ†Øª ÙØ¶ÙˆÙ„ÙŠÙ‹Ø§ ÙˆÙ…Ù‡ØªÙ…Ù‹Ø§ Ø¨Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…Ø²ÙŠØ¯ Ø¹Ù† Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù„Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙƒÙ…ÙŠ 8-Ø¨ØªØŒ ÙØ§Ù‚Ø±Ø£ Ù…Ù†Ø´ÙˆØ± Ø§Ù„Ù…Ø¯ÙˆÙ†Ø© [Gentle Introduction to 8-bit Matrix Multiplication for transformers at scale using Hugging Face Transformers, Accelerate and bitsandbytes](https://huggingface.co/blog/hf-bitsandbytes-integration).

Ù„ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ 8-Ø¨Øª Ù„Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ù…Ø¹Ù„Ù…Ø© `load_in_8bit`. Ù…Ø¹Ù„Ù…Ø© `device_map` Ø§Ø®ØªÙŠØ§Ø±ÙŠØ©ØŒ ÙˆÙ„ÙƒÙ† ÙŠÙˆØµÙ‰ Ø¨ØªØ¹ÙŠÙŠÙ†Ù‡Ø§ Ø¥Ù„Ù‰ `"auto"` Ù„Ù„Ø³Ù…Ø§Ø­ Ù„Ù€ ğŸ¤— Accelerate Ø¨ØªØ®ØµÙŠØµ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ ÙˆØ¨ÙƒÙØ§Ø¡Ø© Ø¨Ø§Ù„Ù†Ø¸Ø± Ø¥Ù„Ù‰ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ù…ØªØ§Ø­Ø© ÙÙŠ Ø§Ù„Ø¨ÙŠØ¦Ø©:

```py
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

model_name = "bigscience/bloom-2b5"
model_8bit = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=BitsAndBytesConfig(load_in_8bit=True))
```

Ø¥Ø°Ø§ ÙƒÙ†Øª ØªÙ‚ÙˆÙ… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ 8-Ø¨Øª Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµØŒ ÙÙŠØ¬Ø¨ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø·Ø±ÙŠÙ‚Ø© [`~transformers.GenerationMixin.generate`] Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† ÙˆØ¸ÙŠÙØ© [`Pipeline`] Ø§Ù„ØªÙŠ Ù„Ø§ ØªÙƒÙˆÙ† Ù…ÙØ­Ø³ÙÙ‘Ù†Ø© Ù„Ù†Ù…Ø§Ø°Ø¬ 8-Ø¨Øª ÙˆØ³ØªÙƒÙˆÙ† Ø£Ø¨Ø·Ø£. Ù„Ø§ ØªØ¯Ø¹Ù… Ø¨Ø¹Ø¶ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø£Ø®Ø° Ø§Ù„Ø¹ÙŠÙ†Ø§ØªØŒ Ù…Ø«Ù„ Ø£Ø®Ø° Ø§Ù„Ø¹ÙŠÙ†Ø§Øª Ø§Ù„Ù†ÙˆÙˆÙŠØ©ØŒ Ø¨ÙˆØ§Ø³Ø·Ø© [`Pipeline`] Ù„Ù†Ù…Ø§Ø°Ø¬ 8-Ø¨Øª. ÙŠØ¬Ø¨ Ø£ÙŠØ¶Ù‹Ø§ ÙˆØ¶Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ø¹Ù„Ù‰ Ù†ÙØ³ Ø§Ù„Ø¬Ù‡Ø§Ø² Ù…Ø«Ù„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬:

```py
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

model_name = "bigscience/bloom-2b5"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model_8bit = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=BitsAndBytesConfig(load_in_8bit=True))

prompt = "Hello, my llama is cute"
inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
generated_ids = model.generate(**inputs)
outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
```

Ù„ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ 4-Ø¨Øª Ù„Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ­Ø¯Ø§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø±Ø³ÙˆÙ…Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø©ØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„ØªØ­ÙƒÙ… ÙÙŠ Ù…Ù‚Ø¯Ø§Ø± Ø°Ø§ÙƒØ±Ø© GPU Ø§Ù„ØªÙŠ ØªØ±ÙŠØ¯ ØªØ®ØµÙŠØµÙ‡Ø§ Ù„ÙƒÙ„ GPU. Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ù„ØªÙˆØ²ÙŠØ¹ 1 Ø¬ÙŠØ¬Ø§Ø¨Ø§ÙŠØª Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø¹Ù„Ù‰ GPU Ø§Ù„Ø£ÙˆÙ„ Ùˆ2 Ø¬ÙŠØ¬Ø§Ø¨Ø§ÙŠØª Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø¹Ù„Ù‰ GPU Ø§Ù„Ø«Ø§Ù†ÙŠ:

```py
max_memory_mapping = {0: "1GB", 1: "2GB"}
model_name = "bigscience/bloom-3b"
model_8bit = AutoModelForCausalLM.from_pretrained(
model_name, device_map="auto", load_in_8bit=True, max_memory=max_memory_mapping
)
```

Ø¬Ø±Ø¨ ØªØ´ØºÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ T5 Ø¨Ø­Ø¬Ù… 11 Ù…Ù„ÙŠØ§Ø± Ù…Ø¹Ù„Ù…Ø© [T5 model] (https://colab.research.google.com/drive/1YORPWx4okIHXnjW7MSAidXN29mPVNT7F?usp=sharing) Ø£Ùˆ Ù†Ù…ÙˆØ°Ø¬ BLOOM Ø¨Ø­Ø¬Ù… 3 Ù…Ù„ÙŠØ§Ø±Ø§Øª Ù…Ø¹Ù„Ù…Ø© [BLOOM model] (https://colab.research.google.com/drive/1qOjXfQIAULfKvZqwCen8-MoWKGdSatZ4?usp=sharing) Ù„Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø¹Ù„Ù‰ ÙˆØ­Ø¯Ø§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³ÙˆÙ…Ø§Øª (GPU) Ù…Ù† Ø§Ù„Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ù…Ø¬Ø§Ù†ÙŠ ÙÙŠ Google Colab!

## ğŸ¤— Optimum

Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø­ÙˆÙ„ Ø§Ø³ØªØ®Ø¯Ø§Ù… ORT Ù…Ø¹ ğŸ¤— OptimumØŒ Ø±Ø§Ø¬Ø¹ Ø£Ø¯Ù„Ø© [Accelerated inference on NVIDIA GPUs](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/gpu#accelerated-inference-on-nvidia-gpus) Ùˆ[Accelerated inference on AMD GPUs](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/amdgpu#accelerated-inference-on-amd-gpus). ÙŠÙ‚Ø¯Ù… Ù‡Ø°Ø§ Ø§Ù„Ù‚Ø³Ù… ÙÙ‚Ø· Ù…Ø«Ø§Ù„Ù‹Ø§ Ù…ÙˆØ¬Ø²Ù‹Ø§ ÙˆØ¨Ø³ÙŠØ·Ù‹Ø§.

ONNX Runtime (ORT) Ù‡Ùˆ Ù…Ø³Ø±Ø¹ Ù†Ù…ÙˆØ°Ø¬ ÙŠØ¯Ø¹Ù… Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø§Ù„Ù…Ø¹Ø¬Ù„ Ø¹Ù„Ù‰ ÙˆØ­Ø¯Ø§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³ÙˆÙ…Ø§Øª (GPU) Ù…Ù† NvidiaØŒ ÙˆÙˆØ­Ø¯Ø§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³ÙˆÙ…Ø§Øª (GPU) Ù…Ù† AMD Ø§Ù„ØªÙŠ ØªØ³ØªØ®Ø¯Ù… [ROCm](https://www.amd.com/en/products/software/rocm.html) stack. ÙŠØ³ØªØ®Ø¯Ù… ORT ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„ØªØ­Ø³ÙŠÙ† Ù…Ø«Ù„ Ø¯Ù…Ø¬ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© ÙÙŠ Ø¹Ù‚Ø¯Ø© ÙˆØ§Ø­Ø¯Ø© ÙˆØ·ÙŠ Ø§Ù„Ø«ÙˆØ§Ø¨Øª Ù„ØªÙ‚Ù„ÙŠÙ„ Ø¹Ø¯Ø¯ Ø§Ù„Ø­Ø³Ø§Ø¨Ø§Øª Ø§Ù„ØªÙŠ ÙŠØªÙ… Ø¥Ø¬Ø±Ø§Ø¤Ù‡Ø§ ÙˆØªØ³Ø±ÙŠØ¹ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„. ÙƒÙ…Ø§ ÙŠØ¶Ø¹ ORT Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø£ÙƒØ«Ø± ÙƒØ«Ø§ÙØ© Ø­Ø³Ø§Ø¨ÙŠØ© Ø¹Ù„Ù‰ ÙˆØ­Ø¯Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³ÙˆÙ…Ø§Øª (GPU) ÙˆØ¨Ù‚ÙŠØ© Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø¹Ù„Ù‰ ÙˆØ­Ø¯Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø±ÙƒØ²ÙŠØ© (CPU) Ù„ØªÙˆØ²ÙŠØ¹ Ø¹Ø¨Ø¡ Ø§Ù„Ø¹Ù…Ù„ Ø¨ÙŠÙ† Ø§Ù„Ø¬Ù‡Ø§Ø²ÙŠÙ† Ø¨Ø°ÙƒØ§Ø¡.

ÙŠØ¯Ø¹Ù… ğŸ¤— Optimum Ø§Ø³ØªØ®Ø¯Ø§Ù… ONNX RuntimeØŒ ÙˆØ§Ù„Ø°ÙŠ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ ÙÙŠ ğŸ¤— Transformers. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø§Ø³ØªØ®Ø¯Ø§Ù… [`~optimum.onnxruntime.ORTModel`] Ù„Ù„Ù…Ù‡Ù…Ø© Ø§Ù„ØªÙŠ ØªØ­Ø§ÙˆÙ„ Ø­Ù„Ù‡Ø§ØŒ ÙˆØªØ­Ø¯ÙŠØ¯ Ù…Ø¹Ù„Ù…Ø© `provider` Ø§Ù„ØªÙŠ ÙŠÙ…ÙƒÙ† ØªØ¹ÙŠÙŠÙ†Ù‡Ø§ Ø¥Ù„Ù‰ [`CUDAExecutionProvider`]ØŒ Ø£Ùˆ [`ROCMExecutionProvider`]ØŒ Ø£Ùˆ [`TensorrtExecutionProvider`]. Ø¥Ø°Ø§ ÙƒÙ†Øª ØªØ±ÙŠØ¯ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ù„Ù… ÙŠØªÙ… ØªØµØ¯ÙŠØ±Ù‡ Ø¨Ø¹Ø¯ Ø¥Ù„Ù‰ ONNXØŒ ÙÙŠÙ…ÙƒÙ†Ùƒ ØªØ¹ÙŠÙŠÙ† `export=True` Ù„ØªØ­ÙˆÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬Ùƒ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªÙ†Ù‚Ù„ Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ ONNX:

```py
from optimum.onnxruntime import ORTModelForSequenceClassification

ort_model = ORTModelForSequenceClassification.from_pretrained(
"distilbert/distilbert-base-uncased-finetuned-sst-2-english",
export=True,
provider="CUDAExecutionProvider",
)
```

Ø§Ù„Ø¢Ù† ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„:

```py
from optimum.pipelines import pipeline
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("distilbert/distilbert-base-uncased-finetuned-sst-2-english")

pipeline = pipeline(task="text-classification", model=ort_model, tokenizer=tokenizer, device="cuda:0")
result = pipeline("Both the music and visual were astounding, not to mention the actors performance.")
```

## Ø§Ù„Ø¬Ù…Ø¹ Ø¨ÙŠÙ† Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª

ØºØ§Ù„Ø¨Ù‹Ø§ Ù…Ø§ ÙŠÙƒÙˆÙ† Ù…Ù† Ø§Ù„Ù…Ù…ÙƒÙ† Ø§Ù„Ø¬Ù…Ø¹ Ø¨ÙŠÙ† Ø¹Ø¯Ø© ØªÙ‚Ù†ÙŠØ§Øª ØªØ­Ø³ÙŠÙ† Ù…ÙˆØµÙˆÙØ© Ø£Ø¹Ù„Ø§Ù‡ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£ÙØ¶Ù„ Ø£Ø¯Ø§Ø¡ Ø§Ø³ØªØ¯Ù„Ø§Ù„ÙŠ Ù…Ù…ÙƒÙ† Ù„Ù†Ù…ÙˆØ°Ø¬Ùƒ. Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ 4-Ø¨ØªØŒ Ø«Ù… ØªÙ…ÙƒÙŠÙ† BetterTransformer Ù…Ø¹ FlashAttention:

```py
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ 4-Ø¨Øª
quantization_config = BitsAndBytesConfig(
load_in_4bit=True,
bnb_4bit_compute_dtype=torch.float16
)

tokenizer = AutoTokenizer.from_pretrained("facebook/opt-350m")
model = AutoModelForCausalLM.from_pretrained("facebook/opt-350m", quantization_config=quantization_config)

# ØªÙ…ÙƒÙŠÙ† BetterTransformer
model = model.to_bettertransformer()

input_text = "Hello my dog is cute and"
inputs = tokenizer(input_text, return_tensors="pt").to("cuda")

# ØªÙ…ÙƒÙŠÙ† FlashAttention
with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):
outputs = model.generate(**inputs)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```