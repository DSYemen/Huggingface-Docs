## ุงูููุฐุฌุฉ ุงููุบููุฉ ุงููููุนุฉ

ุชุชูุจุฃ ุงูููุฐุฌุฉ ุงููุบููุฉ ุงููููุนุฉ ุจุฑูุฒ ูููุน ูู ุชุณูุณูุ ููููู ูููููุฐุฌ ุงูุงูุชูุงู ุจุงูุฑููุฒ ุซูุงุฆู ุงูุงุชุฌุงู. ููุฐุง ูุนูู ุฃู ุงููููุฐุฌ ูุฏูู ุฅููุงููุฉ ุงููุตูู ุงููุงูู ุฅูู ุงูุฑููุฒ ุงูููุฌูุฏุฉ ุนูู ุงููุณุงุฑ ูุงููููู. ุชุนุฏ ุงูููุฐุฌุฉ ุงููุบููุฉ ุงููููุนุฉ ุฑุงุฆุนุฉ ููููุงู ุงูุชู ุชุชุทูุจ ููููุง ุณูุงูููุง ุฌูุฏูุง ูุชุณูุณู ูุงูู. BERT ูู ูุซุงู ุนูู ูููุฐุฌ ุงููุบุฉ ุงููููุน.

ุณููุถุญ ูุฐุง ุงูุฏููู ููููุฉ:

1. ุถุจุท ูููุฐุฌ [DistilRoBERTa](https://huggingface.co/distilbert/distilroberta-base) ุงูุฏููู ุนูู ุงูุฌุฒุก ุงููุฑุนู [r/askscience](https://www.reddit.com/r/askscience/) ููุฌููุนุฉ ุจูุงูุงุช [ELI5](https://huggingface.co/datasets/eli5).
2. ุงุณุชุฎุฏุงู ูููุฐุฌู ุงูุฏููู ููุงุณุชูุชุงุฌ.

ูุจู ุฃู ุชุจุฏุฃุ ุชุฃูุฏ ูู ุชุซุจูุช ุฌููุน ุงูููุชุจุงุช ุงูุถุฑูุฑูุฉ:

```bash
pip install transformers datasets evaluate
```

ูุญู ูุดุฌุนู ุนูู ุชุณุฌูู ุงูุฏุฎูู ุฅูู ุญุณุงุจ Hugging Face ุงูุฎุงุต ุจู ุญุชู ุชุชููู ูู ุชุญููู ูููุฐุฌู ููุดุงุฑูุชู ูุน ุงููุฌุชูุน. ุนูุฏูุง ููุทูุจ ููู ุฐููุ ุฃุฏุฎู ุฑูุฒู ููุชุณุฌูู:

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

## ุชุญููู ูุฌููุนุฉ ุจูุงูุงุช ELI5

ุงุจุฏุฃ ุจุชุญููู ุฃูู 5000 ูุซุงู ูู ูุฌููุนุฉ ุจูุงูุงุช [ELI5-Category](https://huggingface.co/datasets/eli5_category) ุจุงุณุชุฎุฏุงู ููุชุจุฉ Datasets ๐ค. ุณูุชูุญ ูู ูุฐุง ูุฑุตุฉ ุงูุชุฌุฑุจุฉ ูุงูุชุฃูุฏ ูู ุฃู ูู ุดูุก ูุนูู ูุจู ูุถุงุก ุงููุฒูุฏ ูู ุงูููุช ูู ุงูุชุฏุฑูุจ ุนูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุงููุงููุฉ.

```py
>>> from datasets import load_dataset

>>> eli5 = load_dataset("eli5_category", split="train[:5000]")
```

ูุณููู ูุฌููุนุฉ ุงูุจูุงูุงุช ุฅูู ูุฌููุนุชูู ูุฑุนูุชูู ููุชุฏุฑูุจ ูุงูุงุฎุชุจุงุฑ ุจุงุณุชุฎุฏุงู ุทุฑููุฉ [`~datasets.Dataset.train_test_split`]:

```py
>>> eli5 = eli5.train_test_split(test_size=0.2)
```

ุซู ุงูู ูุธุฑุฉ ุนูู ูุซุงู:

```py
>>> eli5["train"][0]
{'q_id': '7h191n',
'title': 'ูุง ุงูุฐู ูุนููู ูุดุฑูุน ูุงููู ุงูุถุฑุงุฆุจ ุงูุฐู ุชู ุชูุฑูุฑู ุงููููุ ููู ุณูุคุซุฑ ุนูู ุงูุฃูุฑููููู ูู ูู ุดุฑูุญุฉ ุถุฑูุจูุฉุ',
'selftext': '',
'category': 'ุงูุชุตุงุฏ',
'subreddit': 'explainlikeimfive',
'answers': {'a_id': ['dqnds8l', 'dqnd1jl', 'dqng3i1', 'dqnku5x'],
'text': ["ูุดุฑูุน ูุงููู ุงูุถุฑุงุฆุจ ุนุจุงุฑุฉ ุนู 500 ุตูุญุฉ ููุงูุช ููุงู ุงููุซูุฑ ูู ุงูุชุบููุฑุงุช ุงูุชู ูุง ุชุฒุงู ูุณุชูุฑุฉ ุญุชู ุงูููุงูุฉ. ุงูุฃูุฑ ูุง ููุชุตุฑ ุนูู ุชุนุฏูู ุงูุดุฑุงุฆุญ ุงูุถุฑูุจูุฉ ุนูู ุงูุฏุฎูุ ุจู ูู ูุฌููุนุฉ ูุงููุฉ ูู ุงูุชุบููุฑุงุช. ูุจุงูุชุงูู ูุง ููุฌุฏ ุฅุฌุงุจุฉ ุฌูุฏุฉ ุนูู ุณุคุงูู. ุงูููุงุท ุงูุฑุฆูุณูุฉ ูู: - ููุงู ุชุฎููุถ ูุจูุฑ ูู ูุนุฏู ุถุฑูุจุฉ ุงูุดุฑูุงุช ููุง ุณูุฌุนู ุงูุดุฑูุงุช ุงููุจุฑู ุณุนูุฏุฉ ููุบุงูุฉ. - ุณูุคุฏู ุชุบููุฑ ูุนุฏู ุงููุฑูุฑ ุฅูู ุณุนุงุฏุฉ ุจุนุถ ุฃููุงุท ุงูุฃุนูุงู (ููุงุชุจ ุงููุญุงูุงุฉุ ูุตูุงุฏูู ุงูุชุญูุท) ุจุดูู ูุจูุฑ - ุชุนุฏููุงุช ุงูุฏุฎู ุงูุถุฑูุจู ูุนุชุฏูุฉุ ููู ุงูููุฑุฑ ุฃู ุชูุชูู (ุนูู ุงูุฑุบู ูู ุฃููุง ุงูููุน ุงูุฐู ูุฏ ูุชู ุฅุนุงุฏุฉ ุชุทุจููู ุฏุงุฆููุง ุฏูู ุฌุนูู ุฏุงุฆููุง) - ูุฎุณุฑ ุงูุฃุดุฎุงุต ูู ุงูููุงูุงุช ุฐุงุช ุงูุถุฑุงุฆุจ ุงููุฑุชูุนุฉ (ูุงููููุฑููุงุ ูููููุฑู)ุ ููุฏ ููุชูู ุจูู ุงูุฃูุฑ ุจุฑูุน ุงูุถุฑุงุฆุจ.",
'ูู ูุชู ุจุนุฏ. ูุฌุจ ุงูุชูููู ุจููู ูุจูู ูุดุฑูุน ูุงููู ูุฌูุณ ุงูููุงุจ ุงููุฎุชูู ุชูุงููุง ุซู ุฅูุฑุงุฑู ูุฑุฉ ุฃุฎุฑู.',
'ุฃูุถุง: ูู ููุทุจู ูุฐุง ุนูู ุถุฑุงุฆุจ ุนุงู 2017ุ ุฃู ุฃูู ูุจุฏุฃ ุจุถุฑุงุฆุจ ุนุงู 2018ุ',
'ุชูุถุญ ูุฐู ุงูููุงูุฉ ููุง ูู ูุดุฑูุนู ูุฌูุณู ุงูููุงุจ ูุงูุดููุฎุ ุจูุง ูู ุฐูู ุงูุชุบููุฑุงุช ุงูููุชุฑุญุฉ ุนูู ุถุฑุงุฆุจ ุงูุฏุฎู ุงูุฎุงุตุฉ ุจู ุจูุงุกู ุนูู ูุณุชูู ุฏุฎูู. URL_0'],
'score': [21ุ 19ุ 5ุ 3]ุ
'text_urls': [[],
[]ุ
[]ุ
['https://www.investopedia.com/news/trumps-tax-reform-what-can-be-done/']]},
'title_urls': ['url'],
'selftext_urls': ['url']}
```

ุนูู ุงูุฑุบู ูู ุฃู ูุฐุง ูุฏ ูุจุฏู ูุซูุฑูุงุ ุฅูุง ุฃูู ููุชู ุญููุง ุจุญูู `ุงููุต`. ูุง ูู ุฑุงุฆุน ุญูู ููุงู ููุฐุฌุฉ ุงููุบุฉ ูู ุฃูู ูุง ุชุญุชุงุฌ ุฅูู ุชุณููุงุช (ุชูุนุฑู ุฃูุถูุง ุจุงุณู ุงููููุฉ ุบูุฑ ุงูุฎุงุถุนุฉ ููุฅุดุฑุงู) ูุฃู ุงููููุฉ ุงูุชุงููุฉ *ูู* ุงูุชุณููุฉ.

## ูุนุงูุฌุฉ ูุณุจูุฉ

ุจุงููุณุจุฉ ููููุฐุฌุฉ ุงููุบููุฉ ุงููููุนุฉุ ุชุชูุซู ุงูุฎุทูุฉ ุงูุชุงููุฉ ูู ุชุญููู ุจุฑูุงูุฌ Tokenizer DistilRoBERTa ููุนุงูุฌุฉ ุญูู `ุงููุต` ุงููุฑุนู:

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("distilbert/distilroberta-base")
```

ุณุชูุงุญุธ ูู ุงููุซุงู ุฃุนูุงูุ ุฃู ุญูู `ุงููุต` ููุฌูุฏ ุจุงููุนู ุฏุงุฎู `ุงูุฅุฌุงุจุงุช`. ููุฐุง ูุนูู ุฃูู ุณุชุญุชุงุฌ ุฅูู ุงุณุชุฎุฑุงุฌ ุญูู `ุงููุต` ูู ููููู ุงููุถูู ุจุงุณุชุฎุฏุงู ุทุฑููุฉ [`flatten`](https://huggingface.co/docs/datasets/process#flatten):

```py
>>> eli5 = eli5.flatten()
>>> eli5["train"][0]
{'q_id': '7h191n',
'title': 'ูุง ุงูุฐู ูุนููู ูุดุฑูุน ูุงููู ุงูุถุฑุงุฆุจ ุงูุฐู ุชู ุชูุฑูุฑู ุงููููุ ููู ุณูุคุซุฑ ุนูู ุงูุฃูุฑููููู ูู ูู ุดุฑูุญุฉ ุถุฑูุจูุฉุ',
'selftext': '',
'category': 'ุงูุชุตุงุฏ',
'subreddit': 'explainlikeimfive',
'answers.a_id': ['dqnds8l', 'dqnd1jl', 'dqng3i1', 'dqnku5x'],
'answers.text': ["ูุดุฑูุน ูุงููู ุงูุถุฑุงุฆุจ ุนุจุงุฑุฉ ุนู 500 ุตูุญุฉ ููุงูุช ููุงู ุงููุซูุฑ ูู ุงูุชุบููุฑุงุช ุงูุชู ูุง ุชุฒุงู ูุณุชูุฑุฉ ุญุชู ุงูููุงูุฉ. ุงูุฃูุฑ ูุง ููุชุตุฑ ุนูู ุชุนุฏูู ุงูุดุฑุงุฆุญ ุงูุถุฑูุจูุฉ ุนูู ุงูุฏุฎูุ ุจู ูู ูุฌููุนุฉ ูุงููุฉ ูู ุงูุชุบููุฑุงุช. ูุจุงูุชุงูู ูุง ููุฌุฏ ุฅุฌุงุจุฉ ุฌูุฏุฉ ุนูู ุณุคุงูู. ุงูููุงุท ุงูุฑุฆูุณูุฉ ูู: - ููุงู ุชุฎููุถ ูุจูุฑ ูู ูุนุฏู ุถุฑูุจุฉ ุงูุดุฑูุงุช ููุง ุณูุฌุนู ุงูุดุฑูุงุช ุงููุจุฑู ุณุนูุฏุฉ ููุบุงูุฉ. - ุณูุคุฏู ุชุบููุฑ ูุนุฏู ุงููุฑูุฑ ุฅูู ุณุนุงุฏุฉ ุจุนุถ ุฃููุงุท ุงูุฃุนูุงู (ููุงุชุจ ุงููุญุงูุงุฉุ ูุตูุงุฏูู ุงูุชุญูุท) ุจุดูู ูุจูุฑ - ุชุนุฏููุงุช ุงูุฏุฎู ุงูุถุฑูุจู ูุนุชุฏูุฉุ ููู ุงูููุฑุฑ ุฃู ุชูุชูู (ุนูู ุงูุฑุบู ูู ุฃููุง ุงูููุน ุงูุฐู ูุฏ ูุชู ุฅุนุงุฏุฉ ุชุทุจููู ุฏุงุฆููุง ุฏูู ุฌุนูู ุฏุงุฆููุง) - ูุฎุณุฑ ุงูุฃุดุฎุงุต ูู ุงูููุงูุงุช ุฐุงุช ุงูุถุฑุงุฆุจ ุงููุฑุชูุนุฉ (ูุงููููุฑููุงุ ูููููุฑู)ุ ููุฏ ููุชูู ุจูู ุงูุฃูุฑ ุจุฑูุน ุงูุถุฑุงุฆุจ.",
'ูู ูุชู ุจุนุฏ. ูุฌุจ ุงูุชูููู ุจููู ูุจูู ูุดุฑูุน ูุงููู ูุฌูุณ ุงูููุงุจ ุงููุฎุชูู ุชูุงููุง ุซู ุฅูุฑุงุฑู ูุฑุฉ ุฃุฎุฑู.',
'ุฃูุถุง: ูู ููุทุจู ูุฐุง ุนูู ุถุฑุงุฆุจ ุนุงู 2017ุ ุฃู ุฃูู ูุจุฏุฃ ุจุถุฑุงุฆุจ ุนุงู 2018ุ',
'ุชูุถุญ ูุฐู ุงูููุงูุฉ ููุง ูู ูุดุฑูุนู ูุฌูุณู ุงูููุงุจ ูุงูุดููุฎุ ุจูุง ูู ุฐูู ุงูุชุบููุฑุงุช ุงูููุชุฑุญุฉ ุนูู ุถุฑุงุฆุจ ุงูุฏุฎู ุงูุฎุงุตุฉ ุจู ุจูุงุกู ุนูู ูุณุชูู ุฏุฎูู. URL_0'],
'answers.score': [21ุ 19ุ 5ุ 3]ุ
'answers.text_urls': [[],
[]ุ
[]ุ
['https://www.investopedia.com/news/trumps-tax-reform-what-can-be-done/']],
'title_urls': ['url'],
'selftext_urls': ['url']}
```

ูู ุญูู ูุฑุนู ูู ุงูุขู ุนููุฏ ูููุตู ููุง ูู ููุถุญ ุจุงูุชุณููุฉ `ุงูุฅุฌุงุจุงุช`ุ ูุญูู `ุงููุต` ูู ูุงุฆูุฉ ุงูุขู. ุจุฏูุงู ูู ุชูููููุฒ ูู ุฌููุฉ ุจุดูู ูููุตูุ ูู ุจุชุญููู ุงููุงุฆูุฉ ุฅูู ุณูุณูุฉ ุญุชู ุชุชููู ูู ุชูููููุฒูุง ุจุดูู ูุดุชุฑู.

ูุฐู ูู ุฏุงูุฉ ุงููุนุงูุฌุฉ ุงููุณุจูุฉ ุงูุฃููู ูุฏูุฌ ูุงุฆูุฉ ุงูุณูุงุณู ููู ูุซุงู ูุชููููุฒ ุงููุชูุฌุฉ:

```py
>>> def preprocess_function(examples):
...     return tokenizer([" ".join(x) for x in examples["answers.text"])]
```

ูุชุทุจูู ุฏุงูุฉ ุงููุนุงูุฌุฉ ุงููุณุจูุฉ ูุฐู ุนูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุจุฃููููุงุ ุงุณุชุฎุฏู ุทุฑููุฉ [`~datasets.Dataset.map`] ูู ููุชุจุฉ Datasets ๐ค. ููููู ุชุณุฑูุน ูุธููุฉ `map` ุนู ุทุฑูู ุชุนููู `batched=True` ููุนุงูุฌุฉ ุนูุงุตุฑ ูุชุนุฏุฏุฉ ูู ูุฌููุนุฉ ุงูุจูุงูุงุช ูู ููุช ูุงุญุฏุ ูุฒูุงุฏุฉ ุนุฏุฏ ุงูุนูููุงุช ุจุงุณุชุฎุฏุงู `num_proc`. ุงุญุฐู ุฃู ุฃุนูุฏุฉ ูุง ุชุญุชุงุฌูุง:

```py
>>> tokenized_eli5 = eli5.map(
...     preprocess_functionุ
...     batched=Trueุ
...     num_proc=4ุ
...     remove_columns=eli5["train"].column_namesุ
... )
```

ุชุญุชูู ูุฌููุนุฉ ุงูุจูุงูุงุช ูุฐู ุนูู ุชุณูุณูุงุช ุงูุฑููุฒุ ูููู ุจุนุถูุง ุฃุทูู ูู ุทูู ุงูุฅุฏุฎุงู ุงูุฃูุตู ูููููุฐุฌ.

ุงูุขู ููููู ุงุณุชุฎุฏุงู ุฏุงูุฉ ุงููุนุงูุฌุฉ ุงููุณุจูุฉ ุงูุซุงููุฉ ู:

- ุฏูุฌ ุฌููุน ุงูุชุณูุณูุงุช
- ุชูุณูู ุงูุชุณูุณูุงุช ุงููุฏูุฌุฉ ุฅูู ูุทุน ุฃูุตุฑ ูุญุฏุฏุฉ ุจูุงุณุทุฉ `block_size`ุ ูุงูุชู ูุฌุจ ุฃู ุชููู ุฃูุตุฑ ูู ุทูู ุงูุฅุฏุฎุงู ุงูุฃูุตู ููุตูุฑุฉ ุจุฏุฑุฌุฉ ูุงููุฉ ูุฐุงูุฑุฉ GPU RAM.

```py
>>> block_size = 128


>>> def group_texts(examples):
...     # ุฏูุฌ ุฌููุน ุงููุตูุต.
...     concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}
...     total_length = len(concatenated_examples[list(examples.keys())[0]])
...     # ูุชุฎูุต ูู ุงูุจุงูู ุงูุตุบูุฑุ ููููููุง ุฅุถุงูุฉ ูุณุงุฏุฉ ุฅุฐุง ูุงู ุงููููุฐุฌ ูุฏุนููุง ุจุฏูุงู ูู ูุฐุง ุงูุงูุฎูุงุถุ ููููู
...     # ูู ุจุชุฎุตูุต ูุฐุง ุงูุฌุฒุก ููููุง ูุงุญุชูุงุฌุงุชู.
...     if total_length >= block_size:
...         total_length = (total_length // block_size) * block_size
...     # ุชูุณูู ุญุณุจ ูุชู ูู block_size.
...     result = {
...         k: [t[i : i + block_size] for i in range(0, total_length, block_size)]
...         for k, t in concatenated_examples.items()
...     }
...     return result
```

ูู ุจุชุทุจูู ูุธููุฉ `group_texts` ุนูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุจุฃููููุง:

```py
>>> lm_dataset = tokenized_eli5.map(group_textsุ batched=Trueุ num_proc=4)
```

ุงูุขู ูู ุจุฅูุดุงุก ุฏูุนุฉ ูู ุงูุฃูุซูุฉ ุจุงุณุชุฎุฏุงู [`DataCollatorForLanguageModeling`]. ูู ุงูุฃูุซุฑ ููุงุกุฉ *ุญุดู* ุงูุฌูู ุฏููุงูููููุง ุฅูู ุฃุทูู ุทูู ูู ุฏูุนุฉ ุฃุซูุงุก ุงูุชุฌููุนุ ุจุฏูุงู ูู ุญุดู ูุฌููุนุฉ ุงูุจูุงูุงุช ุจุฃููููุง ุฅูู ุงูุทูู ุงูุฃูุตู.

<frameworkcontent>
<pt>

ุงุณุชุฎุฏู ุฑูุฒ ููุงูุฉ ุงูุชุณูุณู ูุฑููุฒ ุญุดู ูุญุฏุฏ `mlm_probability` ูุฅุฎูุงุก ุงูุฑููุฒ ุนุดูุงุฆููุง ูููุง ููุช ุจุงูุชููู ุฎูุงู ุงูุจูุงูุงุช:

```py
>>> from transformers import DataCollatorForLanguageModeling

>>> tokenizer.pad_token = tokenizer.eos_token
>>> data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)
```

</pt>
<tf>

ุงุณุชุฎุฏู ุฑูุฒ ููุงูุฉ ุงูุชุณูุณู ูุฑููุฒ ุญุดู ูุญุฏุฏ `mlm_probability` ูุฅุฎูุงุก ุงูุฑููุฒ ุนุดูุงุฆููุง ูููุง ููุช ุจุงูุชููู ุฎูุงู ุงูุจูุงูุงุช:

```py
>>> from transformers import DataCollatorForLanguageModeling

>>> data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15, return_tensors="tf")
```

</tf>

</frameworkcontent>
## ุงูุชุฏุฑูุจ

ุฅุฐุง ูู ุชูู ูุนุชุงุฏุงู ุนูู ุถุจุท ูููุฐุฌ ุจุงุณุชุฎุฏุงู [`Trainer`] ุ ูุฑุงุฌุน ุงูุจุฑูุงูุฌ ุงูุชุนูููู ุงูุฃุณุงุณู [ููุง] (../training # train-with-pytorch-trainer)ย!

ุฃูุช ุงูุขู ุนูู ุงุณุชุนุฏุงุฏ ูุจุฏุก ุชุฏุฑูุจ ูููุฐุฌู! ูู ุจุชุญููู DistilRoBERTa ูุน [`AutoModelForMaskedLM`]ย:

```python
>>> from transformers import AutoModelForMaskedLM

>>> model = AutoModelForMaskedLM.from_pretrained("distilbert/distilroberta-base")
```

ูู ูุฐู ุงููุฑุญูุฉุ ูู ูุชุจู ุณูู ุซูุงุซ ุฎุทูุงุช:

1. ุญุฏุฏ ูุฑุท ูุนููุงุช ุงูุชุฏุฑูุจ ุงูุฎุงุตุฉ ุจู ูู [`TrainingArguments`]. ุงููุนููุฉ ุงููุทููุจุฉ ุงููุญูุฏุฉ ูู `output_dir` ุงูุชู ุชุญุฏุฏ ููุงู ุญูุธ ูููุฐุฌู. ุณุชููู ุจุฏูุน ูุฐุง ุงููููุฐุฌ ุฅูู ุงููุฑูุฒ ุนู ุทุฑูู ุชุนููู `push_to_hub=True` (ูุฌุจ ุฃู ุชููู ูุฏ ุณุฌูุช ุงูุฏุฎูู ุฅูู Hugging Face ูุชุญููู ูููุฐุฌู).
2. ูู ุจุชูุฑูุฑ ูุฑุท ูุนููุงุช ุงูุชุฏุฑูุจ ุฅูู [`Trainer`] ุฌูุจูุง ุฅูู ุฌูุจ ูุน ุงููููุฐุฌ ููุฌููุนุงุช ุงูุจูุงูุงุช ููุฌูุน ุงูุจูุงูุงุช.
3. ุงุณุชุฏุนุงุก [`~ Trainer.train`] ูุถุจุท ูููุฐุฌู ุจุดูู ุฏููู.

```python
>>> training_args = TrainingArguments(
...     output_dir="my_awesome_eli5_mlm_model"ุ
...     eval_strategy="epoch"ุ
...     learning_rate=2e-5ุ
...     num_train_epochs=3ุ
...     weight_decay=0.01ุ
...     push_to_hub=Trueุ
... )

>>> trainer = Trainer(
...     model=modelุ
...     args=training_argsุ
...     train_dataset=lm_dataset ["train"]ุ
...     eval_dataset=lm_dataset ["test"]ุ
...     data_collator=data_collatorุ
... )

>>> trainer.train()
```

ุจูุฌุฑุฏ ุงูุชูุงู ุงูุชุฏุฑูุจุ ุงุณุชุฎุฏู ุทุฑููุฉ [`~ transformers.Trainer.evaluate`] ูุชูููู ูููุฐุฌู ูุงูุญุตูู ุนูู ุญูุฑุชู:

```python
>>> import math

>>> eval_results = trainer.evaluate()
>>> print (f "Perplexity: {math.exp (eval_results ['eval_loss']): .2f}")
ุญูุฑุฉ: 8.76
```

ุซู ุดุงุฑู ูููุฐุฌู ูู ุงููุฑูุฒ ุจุงุณุชุฎุฏุงู ุทุฑููุฉ [`~ transformers.Trainer.push_to_hub`] ุญุชู ูุชููู ุงูุฌููุน ูู ุงุณุชุฎุฏุงู ูููุฐุฌู:

```python
>>> trainer.push_to_hub()
```

ุฅุฐุง ูู ุชูู ูุนุชุงุฏูุง ุนูู ุถุจุท ูููุฐุฌ ุจุงุณุชุฎุฏุงู Kerasุ ูุฑุงุฌุน ุงูุจุฑูุงูุฌ ุงูุชุนูููู ุงูุฃุณุงุณู [ููุง] (../training # train-a-tensorflow-model-with-keras)ย!

ูุถุจุท ูููุฐุฌ ุฏููู ูู TensorFlowุ ุงุจุฏุฃ ุจุฅุนุฏุงุฏ ุฏุงูุฉ ูุญุณู ููุนุฏู ุชุนูู ูุฌุฏูู ุฒููู ูุจุนุถ ูุฑุท ูุนููุงุช ุงูุชุฏุฑูุจ:

```python
>>> from transformers import create_optimizerุ AdamWeightDecay

>>> optimizer = AdamWeightDecay (learning_rate=2e-5ุ weight_decay_rate=0.01)
```

ุจุนุฏ ุฐููุ ููููู ุชุญููู DistilRoBERTa ูุน [`TFAutoModelForMaskedLM`]ย:

```python
>>> from transformers import TFAutoModelForMaskedLM

>>> model = TFAutoModelForMaskedLM.from_pretrained("distilbert/distilroberta-base")
```

ูู ุจุชุญููู ูุฌููุนุงุช ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจู ุฅูู ุชูุณูู `tf.data.Dataset` ุจุงุณุชุฎุฏุงู [`~ transformers.TFPreTrainedModel.prepare_tf_dataset`]ย:

```python
>>> tf_train_set = model.prepare_tf_dataset (
...     lm_dataset ["train"]ุ
...     shuffle=Trueุ
...     batch_size=16ุ
...     collate_fn=data_collatorุ
... )

>>> tf_test_set = model.prepare_tf_dataset (
...     lm_dataset ["test"]ุ
...     shuffle=Falseุ
...     batch_size=16ุ
...     collate_fn=data_collatorุ
... )
```

ูู ุจุชูููู ุงููููุฐุฌ ููุชุฏุฑูุจ ูุน [`compile`] (https://keras.io/api/models/model_training_apis/#compile-method). ูุงุญุธ ุฃู ุฌููุน ููุงุฐุฌ Transformers ุจูุง ุฏุงูุฉ ุฎุณุงุฑุฉ ุฐุงุช ุตูุฉ ุจุงููููุฉ ุจุดูู ุงูุชุฑุงุถูุ ูุฐุง ูุฃูุช ูุณุช ุจุญุงุฌุฉ ุฅูู ุชุญุฏูุฏ ูุงุญุฏุฉ ูุง ูู ุชุฑุบุจ ูู ุฐูู:

```python
>>> import tensorflow as tf

>>> model.compile (optimizer=optimizer) # ูุง ุชูุฌุฏ ุญุฌุฉ ุงูุฎุณุงุฑุฉ!
```

ูููู ุงูููุงู ุจุฐูู ุนู ุทุฑูู ุชุญุฏูุฏ ุงูููุงู ุงูุฐู ุณุชุฏูุน ููู ูููุฐุฌู ููุนุงูุฌุชู ูู [`~ transformers.PushToHubCallback`]ย:

```python
>>> from transformers.keras_callbacks import PushToHubCallback

>>> callback = PushToHubCallback (
...     output_dir="my_awesome_eli5_mlm_model"ุ
...     tokenizer=tokenizerุ
... )
```

ุฃุฎูุฑูุงุ ุฃูุช ุนูู ุงุณุชุนุฏุงุฏ ูุจุฏุก ุชุฏุฑูุจ ูููุฐุฌู! ุงุณุชุฏุนุงุก [`fit`] (https://keras.io/api/models/model_training_apis/#fit-method) ูุน ูุฌููุนุงุช ุงูุจูุงูุงุช ุงูุชุฏุฑูุจูุฉ ูุงูุชุญูู ูู ุตุญุชูุงุ ูุนุฏุฏ ุงูุนุตูุฑุ ูุงุณุชุฏุนุงุก ุงูุฅุฑุฌุงุน ุงูุฎุงุต ุจู ูุถุจุท ุงููููุฐุฌ ุจุฏูุฉ:

```python
>>> model.fit (x=tf_train_setุ validation_data=tf_test_setุ epochs=3ุ callbacks = [callback])
```

ุจูุฌุฑุฏ ุงูุชูุงู ุงูุชุฏุฑูุจุ ูุชู ุชุญููู ูููุฐุฌู ุชููุงุฆููุง ุฅูู ุงููุฑูุฒ ุญุชู ูุชููู ุงูุฌููุน ูู ุงุณุชุฎุฏุงูู!

ููุญุตูู ุนูู ูุซุงู ุฃูุซุฑ ุชุนูููุง ุญูู ููููุฉ ุถุจุท ูููุฐุฌ ุจุฏูุฉ ููุถุน ููุฐุฌุฉ ุงููุบุฉ ุงููููุนุฉุ ุฑุงุฌุน ุงูุฏูุชุฑ ุงูููุงุจู
[ุฏูุชุฑ ููุงุญุธุงุช PyTorch] (https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb)
ุฃู [ุฏูุชุฑ TensorFlow] (https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb).

## ุงูุงุณุชูุชุงุฌ

ุฑุงุฆุนุ ุงูุขู ุจุนุฏ ุฃู ุถุจุทุช ูููุฐุฌูุง ุจุฏูุฉุ ููููู ุงุณุชุฎุฏุงูู ููุงุณุชูุชุงุฌ!

ููุฑ ูู ุจุนุถ ุงููุตูุต ุงูุชู ุชุฑูุฏูุง ุงููููุฐุฌ ูููุก ุงููุฑุงุบ ุจูุงุ ูุงุณุชุฎุฏู ุงูุฑูุฒ `<mask>` ููุฅุดุงุฑุฉ ุฅูู ุงููุฑุงุบ:

```python
>>> text = "The Milky Way is a <mask> galaxy."
```

ุฃุจุณุท ุทุฑููุฉ ูุชุฌุฑุจุฉ ูููุฐุฌู ุงูุฏููู ููุงุณุชูุชุงุฌ ูู ุงุณุชุฎุฏุงูู ูู [`pipeline`]. ูู ุจุชูููุฐ ูุซูู `pipeline` ูููุก ุงูููุงุน ุจุงุณุชุฎุฏุงู ูููุฐุฌูุ ููุฑุฑ ูุตู ุฅููู. ุฅุฐุง ููุช ุชุฑุบุจ ูู ุฐููุ ููููู ุงุณุชุฎุฏุงู ูุนููุฉ `top_k` ูุชุญุฏูุฏ ุนุฏุฏ ุงูุชูุจุคุงุช ุงูุชู ูุฌุจ ุฅุฑุฌุงุนูุง:

```python
>>> from transformers import pipeline

>>> mask_filler = pipeline ("fill-mask"ุ "username/my_awesome_eli5_mlm_model")
>>> mask_filler (textุ top_k=3)
[{'score': 0.5150994658470154ุ
'token': 21300ุ
'token_str': ' ูููุจูุฉ'ุ
'sequence': 'The Milky Way is a spiral galaxy.'}ุ
{'score': 0.07087188959121704ุ
'token': 2232ุ
'token_str': ' ุถุฎูุฉ'ุ
'sequence': 'The Milky Way is a massive galaxy.'}ุ
{'score': 0.06434620916843414ุ
'token': 650ุ
'token_str': ' ุตุบูุฑุฉ'ุ
'sequence': 'The Milky Way is a small galaxy. '}]
```

ูู ุจุฑูุฒูุฉ ุงููุต ูุฅุฑุฌุงุน `input_ids` ูุฑููุฒ PyTorch. ุณุชุญุชุงุฌ ุฃูุถูุง ุฅูู ุชุญุฏูุฏ ููุถุน ุงูุฑูุฒ `<mask>` :

```python
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained ("username/my_awesome_eli5_mlm_model")
>>> inputs = tokenizer (textุ return_tensors="pt")
>>> mask_token_index = torch.where (inputs ["input_ids"] == tokenizer.mask_token_id) [1]
```

ูุฑุฑ ุงููุฏุฎูุงุช ุฅูู ุงููููุฐุฌ ูุฃุนุฏ `logits` ููุฑูุฒ ุงููููุน:

```python
>>> from transformers import AutoModelForMaskedLM

>>> model = AutoModelForMaskedLM.from_pretrained ("username/my_awesome_eli5_mlm_model")
>>> logits = model (** inputs). logits
>>> mask_token_logits = logits [0ุ mask_token_indexุ:]
```

ุซู ูู ุจุฅุฑุฌุงุน ุงูุฑููุฒ ุงูุซูุงุซุฉ ุงููููุนุฉ ุฐุงุช ุงูุงุญุชูุงููุฉ ุงูุฃุนูู ูุทุจุงุนุชูุง:

```python
>>> top_3_tokens = torch.topk (mask_token_logitsุ 3ุ dim=1). indices [0]. tolist ()

>>> for token in top_3_tokens:
... print (ุงููุต.replace (ูุนุงูุฌ. ุฑูุฒ ุงูููุงุนุ ูุนุงูุฌ. ูู ุงูุชุดููุฑ ([ุงูุฑูุฒ])))
ุทุฑูู ุงููุจู ูู ูุฌุฑุฉ ูููุจูุฉ.
ุทุฑูู ุงููุจู ูู ูุฌุฑุฉ ุถุฎูุฉ.
ุทุฑูู ุงููุจู ูู ูุฌุฑุฉ ุตุบูุฑุฉ.
```

ูู ุจุฑูุฒูุฉ ุงููุต ูุฅุฑุฌุงุน `input_ids` ูุฑููุฒ TensorFlow. ุณุชุญุชุงุฌ ุฃูุถูุง ุฅูู ุชุญุฏูุฏ ููุถุน ุงูุฑูุฒ `<mask>` :

```python
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained ("username/my_awesome_eli5_mlm_model")
>>> inputs = tokenizer (textุ return_tensors="tf")
>>> mask_token_index = tf.where (inputs ["input_ids"] == tokenizer.mask_token_id) [0ุ 1]
```

ูุฑุฑ ุงููุฏุฎูุงุช ุฅูู ุงููููุฐุฌ ูุฃุนุฏ `logits` ููุฑูุฒ ุงููููุน:

```python
>>> from transformers import TFAutoModelForMaskedLM

>>> model = TFAutoModelForMaskedLM.from_pretrained ("username/my_awesome_eli5_mlm_model")
>>> logits = model (** inputs). logits
>>> mask_token_logits = logits [0ุ mask_token_indexุ:]
```

ุซู ูู ุจุฅุฑุฌุงุน ุงูุฑููุฒ ุงูุซูุงุซุฉ ุงููููุนุฉ ุฐุงุช ุงูุงุญุชูุงููุฉ ุงูุฃุนูู ูุทุจุงุนุชูุง:

```python
>>> top_3_tokens = tf.math.top_k (mask_token_logitsุ 3). indices. numpy ()

>>> for token in top_3_tokens:
... print (ุงููุต.replace (ูุนุงูุฌ. ุฑูุฒ ุงูููุงุนุ ูุนุงูุฌ. ูู ุงูุชุดููุฑ ([ุงูุฑูุฒ])))
ุทุฑูู ุงููุจู ูู ูุฌุฑุฉ ูููุจูุฉ.
ุทุฑูู ุงููุจู ูู ูุฌุฑุฉ ุถุฎูุฉ.
ุทุฑูู ุงููุจู ูู ูุฌุฑุฉ ุตุบูุฑุฉ.
```