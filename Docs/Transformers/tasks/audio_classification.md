## Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØªÙŠ

ÙŠÙØ¹ÙŠÙ‘Ù† Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØªÙŠ - ØªÙ…Ø§Ù…Ù‹Ø§ Ù…Ø«Ù„ Ø§Ù„Ù†Øµ - ØªØ³Ù…ÙŠØ© ÙØ¦Ø© Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬ Ù…Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„. ÙˆØ§Ù„ÙØ±Ù‚ Ø§Ù„ÙˆØ­ÙŠØ¯ Ù‡Ùˆ Ø£Ù†Ù‡ Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø¥Ø¯Ø®Ø§Ù„Ø§Øª Ø§Ù„Ù†ØµØŒ Ù„Ø¯ÙŠÙƒ Ø£Ø´ÙƒØ§Ù„ Ù…ÙˆØ¬ÙŠØ© ØµÙˆØªÙŠØ© Ø®Ø§Ù…. ÙˆØªØ´Ù…Ù„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ù„Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØªÙŠ ØªØ­Ø¯ÙŠØ¯ Ù†ÙŠØ© Ø§Ù„Ù…ØªØ­Ø¯Ø«ØŒ ÙˆØªØµÙ†ÙŠÙ Ø§Ù„Ù„ØºØ©ØŒ ÙˆØ­ØªÙ‰ Ø§Ù„Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø­ÙŠÙˆØ§Ù†ÙŠØ© Ù…Ù† Ø®Ù„Ø§Ù„ Ø£ØµÙˆØ§ØªÙ‡Ø§.

Ø³ÙŠÙˆØ¶Ø­ Ù„Ùƒ Ù‡Ø°Ø§ Ø§Ù„Ø¯Ù„ÙŠÙ„ ÙƒÙŠÙÙŠØ©:

1. Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ [Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base) Ø§Ù„Ø¯Ù‚ÙŠÙ‚ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14) Ù„ØªØµÙ†ÙŠÙ Ù†ÙŠØ© Ø§Ù„Ù…ØªØ­Ø¯Ø«.
2. Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬Ùƒ Ø§Ù„Ø¯Ù‚ÙŠÙ‚ Ù„Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬.

Ù‚Ø¨Ù„ Ø£Ù† ØªØ¨Ø¯Ø£ØŒ ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ©:

```bash
pip install transformers datasets evaluate
```

Ù†Ø­Ù† Ù†Ø´Ø¬Ø¹Ùƒ Ø¹Ù„Ù‰ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø¥Ù„Ù‰ Ø­Ø³Ø§Ø¨ Hugging Face Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ Ø­ØªÙ‰ ØªØªÙ…ÙƒÙ† Ù…Ù† ØªØ­Ù…ÙŠÙ„ ÙˆÙ…Ø´Ø§Ø±ÙƒØ© Ù†Ù…ÙˆØ°Ø¬Ùƒ Ù…Ø¹ Ø§Ù„Ù…Ø¬ØªÙ…Ø¹. Ø¹Ù†Ø¯Ù…Ø§ ÙŠÙØ·Ù„Ø¨ Ù…Ù†Ùƒ Ø°Ù„ÙƒØŒ Ø£Ø¯Ø®Ù„ Ø±Ù…Ø²Ùƒ Ù„Ù„ØªØ³Ø¬ÙŠÙ„:

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

## ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª MInDS-14

Ø§Ø¨Ø¯Ø£ Ø¨ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª MInDS-14 Ù…Ù† Ù…ÙƒØªØ¨Ø© Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ğŸ¤—:

```py
>>> from datasets import load_dataset, Audio

>>> minds = load_dataset("PolyAI/minds14", name="en-US", split="train")
```

Ù‚Ø³ÙÙ‘Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª ÙØ±Ø¹ÙŠØ© Ù„Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø·Ø±ÙŠÙ‚Ø© [`~datasets.Dataset.train_test_split`]. Ø³ÙŠØ¹Ø·ÙŠÙƒ Ù‡Ø°Ø§ ÙØ±ØµØ© Ù„Ù„ØªØ¬Ø±Ø¨Ø© ÙˆØ§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† ÙƒÙ„ Ø´ÙŠØ¡ ÙŠØ¹Ù…Ù„ Ù‚Ø¨Ù„ Ù‚Ø¶Ø§Ø¡ Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ÙˆÙ‚Øª ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙƒØ§Ù…Ù„Ø©.

```py
>>> minds = minds.train_test_split(test_size=0.2)
```

Ø«Ù… Ø§Ù„Ù‚ Ù†Ø¸Ø±Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:

```py
>>> minds
DatasetDict({
train: Dataset({
features: ['path', 'audio', 'transcription', 'english_transcription', 'intent_class', 'lang_id'],
num_rows: 450
})
test: Dataset({
features: ['path', 'audio', 'transcription', 'english_transcription', 'intent_class', 'lang_id'],
num_rows: 113
})
})
```

ÙÙŠ Ø­ÙŠÙ† Ø£Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„ÙƒØ«ÙŠØ± Ù…Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ÙÙŠØ¯Ø©ØŒ Ù…Ø«Ù„ `lang_id` Ùˆ`english_transcription`ØŒ Ø³ØªØ±ÙƒØ² Ø¹Ù„Ù‰ `audio` Ùˆ`intent_class` ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø¯Ù„ÙŠÙ„. Ù‚Ù… Ø¨Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø£Ø®Ø±Ù‰ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø·Ø±ÙŠÙ‚Ø© [`~datasets.Dataset.remove_columns`]:

```py
>>> minds = minds.remove_columns(["path", "transcription", "english_transcription", "lang_id"])
```

Ø§Ù„Ù‚ Ù†Ø¸Ø±Ø© Ø¹Ù„Ù‰ Ù…Ø«Ø§Ù„ Ø§Ù„Ø¢Ù†:

```py
>>> minds["train"][0]
{'audio': {'array': array([ 0.        ,  0.        ,  0.        , ..., -0.00048828,
-0.00024414, -0.00024414], dtype=float32),
'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602b9a5fbb1e6d0fbce91f52.wav',
'sampling_rate': 8000},
'intent_class': 2}
```

Ù‡Ù†Ø§Ùƒ Ø­Ù‚ÙˆÙ„Ø§Ù†:

- `audio`: Ù…ØµÙÙˆÙØ© Ø£Ø­Ø§Ø¯ÙŠØ© Ø§Ù„Ø¨Ø¹Ø¯ Ù„Ù„Ø¥Ø´Ø§Ø±Ø© Ø§Ù„ØµÙˆØªÙŠØ© Ø§Ù„ØªÙŠ ÙŠØ¬Ø¨ Ø§Ø³ØªØ¯Ø¹Ø§Ø¤Ù‡Ø§ Ù„ØªØ­Ù…ÙŠÙ„ ÙˆØ¥Ø¹Ø§Ø¯Ø© Ø£Ø®Ø° Ø¹ÙŠÙ†Ø§Øª Ù…Ù„Ù Ø§Ù„ØµÙˆØª.
- `intent_class`: ÙŠÙ…Ø«Ù„ Ù…Ø¹Ø±Ù ÙØ¦Ø© Ù†ÙŠØ© Ø§Ù„Ù…ØªØ­Ø¯Ø«.

ÙˆÙ„ØªØ³Ù‡ÙŠÙ„ Ø§Ù„Ø£Ù…Ø± Ø¹Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ø³Ù… Ø§Ù„ØªØ³Ù…ÙŠØ© Ù…Ù† Ù…Ø¹Ø±Ù Ø§Ù„ØªØ³Ù…ÙŠØ©ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ù…ÙˆØ³ ÙŠÙ‚ÙˆÙ… Ø¨ØªØ¹ÙŠÙŠÙ† Ø§Ø³Ù… Ø§Ù„ØªØ³Ù…ÙŠØ© Ø¥Ù„Ù‰ Ø±Ù‚Ù… ØµØ­ÙŠØ­ ÙˆØ§Ù„Ø¹ÙƒØ³ ØµØ­ÙŠØ­:

```py
>>> labels = minds["train"].features["intent_class"].names
>>> label2id, id2label = dict(), dict()
>>> for i, label in enumerate(labels):
...     label2id[label] = str(i)
...     id2label[str(i)] = label
```

Ø§Ù„Ø¢Ù† ÙŠÙ…ÙƒÙ†Ùƒ ØªØ­ÙˆÙŠÙ„ Ù…Ø¹Ø±Ù Ø§Ù„ØªØ³Ù…ÙŠØ© Ø¥Ù„Ù‰ Ø§Ø³Ù… Ø§Ù„ØªØ³Ù…ÙŠØ©:

```py
>>> id2label[str(2)]
'app_error'
```

## Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø³Ø¨Ù‚Ø©

Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„ØªØ§Ù„ÙŠØ© Ù‡ÙŠ ØªØ­Ù…ÙŠÙ„ Ù…Ø³ØªØ®Ø±Ø¬ Ù…ÙŠØ²Ø§Øª Wav2Vec2 Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ø§Ù„ØµÙˆØªÙŠØ©:

```py
>>> from transformers import AutoFeatureExtractor

>>> feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/wav2vec2-base")
```

ØªØ¨Ù„Øº Ø³Ø±Ø¹Ø© Ø£Ø®Ø° Ø§Ù„Ø¹ÙŠÙ†Ø§Øª ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª MInDS-14 8000 ÙƒÙŠÙ„Ùˆ Ù‡Ø±ØªØ² (ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙÙŠ [Ø¨Ø·Ø§Ù‚Ø© Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª](https://huggingface.co/datasets/PolyAI/minds14))ØŒ Ù…Ù…Ø§ ÙŠØ¹Ù†ÙŠ Ø£Ù†Ù‡ Ø³ÙŠØªØ¹ÙŠÙ† Ø¹Ù„ÙŠÙƒ Ø¥Ø¹Ø§Ø¯Ø© Ø£Ø®Ø° Ø¹ÙŠÙ†Ø§Øª Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ 16000 ÙƒÙŠÙ„Ùˆ Ù‡Ø±ØªØ² Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ Wav2Vec2 Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ù‹Ø§:

```py
>>> minds = minds.cast_column("audio", Audio(sampling_rate=16_000))
>>> minds["train"][0]
{'audio': {'array': array([ 2.2098757e-05,  4.6582241e-05, -2.2803260e-05, ...,
-2.8419291e-04, -2.3305941e-04, -1.1425107e-04], dtype=float32),
'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602b9a5fbb1e6d0fbce91f52.wav',
'sampling_rate': 1600Multiplier: 1000
>>> minds = minds.cast_column("audio", Audio(sampling_rate=16_000))
```

Ø§Ù„Ø¢Ù† Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ø¯Ø§Ù„Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø³Ø¨Ù‚Ø© ØªÙ‚ÙˆÙ… Ø¨Ù…Ø§ ÙŠÙ„ÙŠ:

1. Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø¹Ù…ÙˆØ¯ `audio` Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±ØŒ Ø¥Ø¹Ø§Ø¯Ø© Ø£Ø®Ø° Ø¹ÙŠÙ†Ø§Øª Ù…Ù„Ù Ø§Ù„ØµÙˆØª.
2. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…Ø¹Ø¯Ù„ Ø£Ø®Ø° Ø§Ù„Ø¹ÙŠÙ†Ø§Øª Ù„Ù…Ù„Ù Ø§Ù„ØµÙˆØª ÙŠØªØ·Ø§Ø¨Ù‚ Ù…Ø¹ Ù…Ø¹Ø¯Ù„ Ø£Ø®Ø° Ø§Ù„Ø¹ÙŠÙ†Ø§Øª Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØª Ø§Ù„ØªÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„ÙŠÙ‡Ø§ Ù…Ø³Ø¨Ù‚Ù‹Ø§. ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙÙŠ [Ø¨Ø·Ø§Ù‚Ø© Ù†Ù…ÙˆØ°Ø¬](https://huggingface.co/facebook/wav2vec2-base) Wav2Vec2.
3. Ù‚Ù… Ø¨ØªØ¹ÙŠÙŠÙ† Ø·ÙˆÙ„ Ø¥Ø¯Ø®Ø§Ù„ Ø£Ù‚ØµÙ‰ Ù„Ø¯ÙØ¹Ø§Øª Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ø§Ù„Ø£Ø·ÙˆÙ„ Ø¯ÙˆÙ† Ø§Ù‚ØªØ·Ø§Ø¹Ù‡Ø§.

```py
>>> def preprocess_function(examples):
...     audio_arrays = [x["array"] for x in examples["audio"]]
...     inputs = feature_extractor(
...         audio_arrays, sampling_rate=feature_extractor.sampling_rate, max_length=16000, truncation=True
...     )
...     return inputs
```

Ù„ØªØ·Ø¨ÙŠÙ‚ Ø¯Ø§Ù„Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³Ø¨Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø£ÙƒÙ…Ù„Ù‡Ø§ØŒ Ø§Ø³ØªØ®Ø¯Ù… ÙˆØ¸ÙŠÙØ© [`~datasets.Dataset.map`] ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ğŸ¤—. ÙŠÙ…ÙƒÙ†Ùƒ ØªØ³Ø±ÙŠØ¹ `map` Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªØ¹ÙŠÙŠÙ† `batched=True` Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¹Ù†Ø§ØµØ± Ù…ØªØ¹Ø¯Ø¯Ø© Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ ÙˆÙ‚Øª ÙˆØ§Ø­Ø¯. Ø£Ø²Ù„ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„ØªÙŠ Ù„Ø§ ØªØ­ØªØ§Ø¬Ù‡Ø§ØŒ ÙˆØ£Ø¹Ø¯ ØªØ³Ù…ÙŠØ© `intent_class` Ø¥Ù„Ù‰ `label` Ù„Ø£Ù† Ù‡Ø°Ø§ Ù‡Ùˆ Ø§Ù„Ø§Ø³Ù… Ø§Ù„Ø°ÙŠ ÙŠØªÙˆÙ‚Ø¹Ù‡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬:

```py
>>> encoded_minds = minds.map(preprocess_function, remove_columns="audio", batched=True)
>>> encoded_minds = encoded_minds.rename_column("intent_class", "label")
```

## ØªÙ‚ÙŠÙŠÙ…

ØºØ§Ù„Ø¨Ù‹Ø§ Ù…Ø§ ÙŠÙƒÙˆÙ† ØªØ¶Ù…ÙŠÙ† Ù…Ù‚ÙŠØ§Ø³ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù…ÙÙŠØ¯Ù‹Ø§ Ù„ØªÙ‚ÙŠÙŠÙ… Ø£Ø¯Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬Ùƒ. ÙŠÙ…ÙƒÙ†Ùƒ ØªØ­Ù…ÙŠÙ„ Ø·Ø±ÙŠÙ‚Ø© ØªÙ‚ÙŠÙŠÙ… Ø¨Ø³Ø±Ø¹Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙƒØªØ¨Ø© ğŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index). Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù‡Ø°Ù‡ Ø§Ù„Ù…Ù‡Ù…Ø©ØŒ Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù…Ù‚ÙŠØ§Ø³ [Ø§Ù„Ø¯Ù‚Ø©](https://huggingface.co/spaces/evaluate-metric/accuracy) (Ø±Ø§Ø¬Ø¹ Ø¬ÙˆÙ„Ø© ğŸ¤— Evaluate [Ø§Ù„Ø³Ø±ÙŠØ¹Ø©](https://huggingface.co/docs/evaluate/a_quick_tour) Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…Ø²ÙŠØ¯ Ø­ÙˆÙ„ ÙƒÙŠÙÙŠØ© ØªØ­Ù…ÙŠÙ„ ÙˆØ­Ø³Ø§Ø¨ Ù…Ù‚ÙŠØ§Ø³):

```py
>>> import evaluate

>>> accuracy = evaluate.load("accuracy")
```

Ø«Ù… Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ø¯Ø§Ù„Ø© ØªÙ…Ø±Ø± ØªÙ†Ø¨Ø¤Ø§ØªÙƒ ÙˆØªØ³Ù…ÙŠØ§ØªÙƒ Ø¥Ù„Ù‰ [`~evaluate.EvaluationModule.compute`] Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¯Ù‚Ø©:

```py
>>> import numpy as np


>>> def compute_metrics(eval_pred):
...     predictions = np.argmax(eval_pred.predictions, axis=1)
...     return accuracy.compute(predictions=predictions, references=eval_pred.label_ids)
```

ÙˆØ¸ÙŠÙØªÙƒ `compute_metrics` Ø¬Ø§Ù‡Ø²Ø© Ø§Ù„Ø¢Ù†ØŒ ÙˆØ³ØªØ¹ÙˆØ¯ Ø¥Ù„ÙŠÙ‡Ø§ Ø¹Ù†Ø¯Ù…Ø§ ØªÙ‚ÙˆÙ… Ø¨Ø¥Ø¹Ø¯Ø§Ø¯ ØªØ¯Ø±ÙŠØ¨Ùƒ.

## ØªØ¯Ø±ÙŠØ¨

<frameworkcontent>
<pt>
<Tip>
Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…Ø¹ØªØ§Ø¯Ù‹Ø§ Ø¹Ù„Ù‰ Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`Trainer`]]ØŒ ÙØ±Ø§Ø¬Ø¹ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ [Ù‡Ù†Ø§](../training#train-with-pytorch-trainer)!
</Tip>

Ø£Ù†Øª Ù…Ø³ØªØ¹Ø¯ Ø§Ù„Ø¢Ù† Ù„Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬Ùƒ! Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Wav2Vec2 Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`AutoModelForAudioClassification`] Ø¬Ù†Ø¨Ù‹Ø§ Ø¥Ù„Ù‰ Ø¬Ù†Ø¨ Ù…Ø¹ Ø¹Ø¯Ø¯ Ø§Ù„ØªØ³Ù…ÙŠØ§Øª Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©ØŒ ÙˆØ®Ø±Ø§Ø¦Ø· Ø§Ù„ØªØ³Ù…ÙŠØ§Øª:

```py
>>> from transformers import AutoModelForAudioClassification, TrainingArguments, Trainer

>>> num_labels = len(id2label)
>>> model = AutoModelForAudioClassification.from_pretrained(
...     "facebook/wav2vec2-base"ØŒ num_labels=num_labels, label2id=label2id, id2label=id2label
... )
```

ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø±Ø­Ù„Ø©ØŒ Ù‡Ù†Ø§Ùƒ Ø«Ù„Ø§Ø« Ø®Ø·ÙˆØ§Øª ÙÙ‚Ø·:

1. Ø­Ø¯Ø¯ ÙØ±Ø· Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ ÙÙŠ [`TrainingArguments`]. Ø§Ù„Ù…Ø¹Ù„Ù…Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ø§Ù„ÙˆØ­ÙŠØ¯Ø© Ù‡ÙŠ `output_dir` Ø§Ù„ØªÙŠ ØªØ­Ø¯Ø¯ Ù…ÙƒØ§Ù† Ø­ÙØ¸ Ù†Ù…ÙˆØ°Ø¬Ùƒ. Ø³ØªÙ‚ÙˆÙ… Ø¨Ø§Ù„Ø¯ÙØ¹ Ø¥Ù„Ù‰ Hub Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªØ¹ÙŠÙŠÙ† `push_to_hub=True` (ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ù…Ø³Ø¬Ù„ Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø¥Ù„Ù‰ Hugging Face Ù„ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬Ùƒ). ÙÙŠ Ù†Ù‡Ø§ÙŠØ© ÙƒÙ„ Ø­Ù‚Ø¨Ø©ØŒ Ø³ÙŠÙ‚ÙŠÙ… [`Trainer`] Ø§Ù„Ø¯Ù‚Ø© ÙˆÙŠØ­ÙØ¸ Ù†Ù‚Ø·Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨.
2. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø§Ù„Ø­Ø¬Ø¬ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ÙŠØ© Ø¥Ù„Ù‰ [`Trainer`] Ø¬Ù†Ø¨Ù‹Ø§ Ø¥Ù„Ù‰ Ø¬Ù†Ø¨ Ù…Ø¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙˆÙ…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŒ ÙˆÙ…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ø±Ù…ÙˆØ²ØŒ ÙˆÙ…Ù„Ù ØªØ¹Ø±ÙŠÙ Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø· Ù„Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŒ ÙˆÙˆØ¸ÙŠÙØ© `compute_metrics`.
3. Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ [`~Trainer.train`] Ù„Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬Ùƒ Ø¨Ø´ÙƒÙ„ Ø¯Ù‚ÙŠÙ‚.

```py
>>> training_args = TrainingArguments(
...     output_dir="my_awesome_mind_model"ØŒ
...     eval_strategy="epoch"ØŒ
...     save_strategy="epoch"ØŒ
...     learning_rate=3e-5ØŒ
...     per_device_train_batch_size=32ØŒ
...     gradient_accumulation_steps=4ØŒ
...     per_device_eval_batch_size=32ØŒ
...     num_train_epochs=10ØŒ
...     warmup_ratio=0.1ØŒ
...     logging_steps=10ØŒ
...     load_best_model_at_end=TrueØŒ
...     metric_for_best_model="accuracy"ØŒ
...     push_to_hub=TrueØŒ
... )

>>> trainer = Trainer(
...     model=modelØŒ
...     args=training_argsØŒ
...     train_dataset=encoded_minds["train"]ØŒ
...     eval_dataset=encoded_minds["test"]ØŒ
...     tokenizer=feature_extractorØŒ
...     compute_metrics=compute_metricsØŒ
... )

>>> trainer.train()
```

Ø¨Ù…Ø¬Ø±Ø¯ Ø§ÙƒØªÙ…Ø§Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ØŒ Ø´Ø§Ø±Ùƒ Ù†Ù…ÙˆØ°Ø¬Ùƒ Ø¹Ù„Ù‰ Hub Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø·Ø±ÙŠÙ‚Ø© [`~transformers.Trainer.push_to_hub`]] Ø­ØªÙ‰ ÙŠØªÙ…ÙƒÙ† Ø§Ù„Ø¬Ù…ÙŠØ¹ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬Ùƒ:

```py
>>> trainer.push_to_hub()
```

</pt>
</frameworkcontent>

<Tip>
Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø«Ø§Ù„ Ø£ÙƒØ«Ø± Ø´Ù…ÙˆÙ„Ø§Ù‹ Ø­ÙˆÙ„ ÙƒÙŠÙÙŠØ© Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ Ø¯Ù‚ÙŠÙ‚ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØªØŒ Ø±Ø§Ø¬Ø¹ Ø§Ù„Ø¯ÙØªØ± [Ø§Ù„Ù…ÙÙƒØ±Ø©](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/audio_classification.ipynb) Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„ Ù„Ù€ PyTorch.
</Tip>
## Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬
Ø±Ø§Ø¦Ø¹ØŒ Ø§Ù„Ø¢Ù† Ø¨Ø¹Ø¯ Ø£Ù† Ø¶Ø¨Ø·Øª Ù†Ù…ÙˆØ°Ø¬ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬!

Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù ØµÙˆØªÙŠ ØªØ±ÙŠØ¯ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬ Ø¹Ù„ÙŠÙ‡. ØªØ°ÙƒØ± Ø¥Ø¹Ø§Ø¯Ø© Ø£Ø®Ø° Ø¹ÙŠÙ†Ø§Øª Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø¹ÙŠÙ†Ø§Øª Ù„Ù…Ù„Ù Ø§Ù„ØµÙˆØª Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ù…Ø¹Ø¯Ù„ Ø¹ÙŠÙ†Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±!

Ø£Ø¨Ø³Ø· Ø·Ø±ÙŠÙ‚Ø© Ù„ØªØ¬Ø±Ø¨Ø© Ù†Ù…ÙˆØ°Ø¬Ùƒ Ø§Ù„Ù…Ø¶Ø¨ÙˆØ· Ù„Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬ Ù‡ÙŠ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ ÙÙŠ ["pipeline"]. Ù‚Ù… Ø¨ØªÙ†ÙÙŠØ° Ù†Ù…ÙˆØ°Ø¬ "pipeline" Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ÙƒØŒ ÙˆÙ…Ø±Ø± Ù…Ù„Ù Ø§Ù„ØµÙˆØª Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ Ø¥Ù„ÙŠÙ‡:

```py
>>> from transformers import pipeline

>>> classifier = pipeline("audio-classification", model="stevhliu/my_awesome_minds_model")
>>> classifier(audio_file)
[
{'score': 0.09766869246959686, 'label': 'cash_deposit'},
{'score': 0.07998877018690109, 'label': 'app_error'},
{'score': 0.0781070664525032, 'label': 'joint_account'},
{'score': 0.07667109370231628, 'label': 'pay_bill'},
{'score': 0.0755252093076706, 'label': 'balance'}
]
```

ÙŠÙ…ÙƒÙ†Ùƒ Ø£ÙŠØ¶Ù‹Ø§ Ù…Ø­Ø§ÙƒØ§Ø© Ù†ØªØ§Ø¦Ø¬ "pipeline" ÙŠØ¯ÙˆÙŠÙ‹Ø§ Ø¥Ø°Ø§ Ø£Ø±Ø¯Øª:

<frameworkcontent>
<pt>
Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù…Ø³ØªØ®Ø±Ø¬ Ù…ÙŠØ²Ø§Øª Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„Ù Ø§Ù„ØµÙˆØª ÙˆØ¥Ø±Ø¬Ø§Ø¹ "input" Ø¹Ù„Ù‰ Ø´ÙƒÙ„ Ù…ØµÙÙˆÙØ§Øª PyTorch:

```py
>>> from transformers import AutoFeatureExtractor

>>> feature_extractor = AutoFeatureExtractor.from_pretrained("stevhliu/my_awesome_minds_model")
>>> inputs = feature_extractor(dataset[0]["audio"]["array"], sampling_rate=sampling_rate, return_tensors="pt")
```

Ù…Ø±Ø± Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ£Ø¹Ø¯ Ø§Ù„Ù„ÙˆØºØ§Ø±ÙŠØªÙ…Ø§Øª:

```py
>>> from transformers import AutoModelForAudioClassification

>>> model = AutoModelForAudioClassification.from_pretrained("stevhliu/my_awesome_minds_model")
>>> with torch.no_grad():
...     logits = model(**inputs).logits
```

Ø§Ø­ØµÙ„ Ø¹Ù„Ù‰ Ø§Ù„ÙØ¦Ø© Ø°Ø§Øª Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø§Ù„Ø£Ø¹Ù„Ù‰ØŒ ÙˆØ§Ø³ØªØ®Ø¯Ù… Ø®Ø±ÙŠØ·Ø© "id2label" Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØ­ÙˆÙŠÙ„Ù‡Ø§ Ø¥Ù„Ù‰ ØªØ³Ù…ÙŠØ©:

```py
>>> import torch

>>> predicted_class_ids = torch.argmax(logits).item()
>>> predicted_label = model.config.id2label[predicted_class_ids]
>>> predicted_label
'cash_deposit'
```
</pt>
</frameworkcontent>