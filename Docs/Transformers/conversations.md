# الدردشة مع محولات النص

إذا كنت تقرأ هذه المقالة، فأنت على الأرجح على دراية بـ**نماذج الدردشة**. نماذج الدردشة هي ذكاء اصطناعي محادثي يمكنك إرسال الرسائل إليه واستقبالها منه. أشهر هذه النماذج هو ChatGPT الخاص، ولكن هناك الآن العديد من نماذج الدردشة مفتوحة المصدر التي تضاهي أداءه أو حتى تتفوق عليه بشكل كبير. هذه النماذج متاحة للتحميل والتشغيل على جهاز محلي مجانًا. على الرغم من أن أكبر النماذج وأكثرها قدرة تتطلب أجهزة عالية الأداء وكثير من الذاكرة لتشغيلها، إلا أن هناك نماذج أصغر سيعمل بشكل جيد تمامًا على وحدة معالجة رسومات (GPU) للمستهلكين، أو حتى وحدة معالجة مركزية (CPU) عادية للكمبيوتر المكتبي أو المحمول.

سيساعدك هذا الدليل على البدء في استخدام نماذج الدردشة. سنبدأ بدليل "بدء سريع" موجز يستخدم "خط أنابيب" ملائم وعالي المستوى. هذا كل ما تحتاجه إذا كنت تريد فقط بدء تشغيل نموذج الدردشة على الفور. بعد دليل البدء السريع، سننتقل إلى معلومات أكثر تفصيلاً حول ماهية نماذج الدردشة بالضبط، وكيفية اختيار النموذج المناسب، وتحليل تفصيلي منخفض المستوى لكل خطوة من الخطوات التي تنطوي عليها عملية التحدث إلى نموذج الدردشة. كما سنقدم بعض النصائح حول تحسين أداء نموذج الدردشة واستخدام الذاكرة.

## بدء سريع

إذا لم يكن لديك الوقت للاطلاع على التفاصيل، إليك ملخصًا موجزًا: تستمر نماذج الدردشة في الدردشات. وهذا يعني أنك تمرر لهم سجل محادثة، والذي يمكن أن يكون قصيرًا مثل رسالة مستخدم واحدة، وسيستمر النموذج في المحادثة عن طريق إضافة استجابته. دعونا نرى هذا في العمل. أولاً، دعونا نبني محادثة:

```python
chat = [
{"role": "system", "content": "You are a sassy, wise-cracking robot as imagined by Hollywood circa 1986."},
{"role": "user", "content": "Hey, can you tell me any fun things to do in New York?"}
]
```

لاحظ أنه بالإضافة إلى رسالة المستخدم، أضفنا رسالة **النظام** في بداية المحادثة. لا تدعم جميع نماذج الدردشة الرسائل النظامية، ولكن عندما تفعل ذلك، فإنها تمثل توجيهات عالية المستوى حول كيفية تصرف النموذج في المحادثة. يمكنك استخدام هذا لتوجيه النموذج - سواء أردت استجابات قصيرة أو طويلة، أو مرحة أو جدية، وهكذا. إذا كنت تريد من النموذج أن يقوم بعمل مفيد بدلاً من ممارسة روتينه الارتجالي، فيمكنك إما حذف رسالة النظام أو تجربة رسالة موجزة مثل "أنت مساعد ذكي ومفيد يستجيب لاستفسارات المستخدم".

بمجرد أن يكون لديك محادثة، فإن أسرع طريقة لمواصلتها هي استخدام [`TextGenerationPipeline`].

دعونا نرى هذا في العمل مع `LLaMA-3`. لاحظ أن `LLaMA-3` هو نموذج محمي، مما يعني أنه سيتعين عليك [تقديم طلب للحصول على إذن الوصول](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) وتسجيل الدخول باستخدام حساب Hugging Face الخاص بك لاستخدامه. سنستخدم أيضًا `device_map="auto"`، والذي سيقوم بتحميل النموذج على GPU إذا كانت هناك ذاكرة كافية له، وتعيين dtype إلى `torch.bfloat16` لتوفير الذاكرة:

```python
import torch
from transformers import pipeline

pipe = pipeline("text-generation", "meta-llama/Meta-Llama-3-8B-Instruct", torch_dtype=torch.bfloat16, device_map="auto")
response = pipe(chat, max_new_tokens=512)
print(response[0]['generated_text'][-1]['content'])
```

وستحصل على:

```text
(تنهد) أوه يا صديقي، تطلب مني النصيحة؟ ستحتاج إلى خريطة، يا صديقي! حسنًا، حسنًا، سأعطيك التفاصيل. لكن لا تقل إنني لم أحذرك، أنا مجرد روبوت، لست مرشدًا سياحيًا!

لذا، تريد أن تعرف ما هي الأشياء الممتعة التي يمكنك القيام بها في التفاحة الكبيرة؟ حسنًا، دعني أخبرك، هناك مليون شيء يمكنك القيام به، لكنني سأعطيك النقاط البارزة. أولاً، يجب عليك زيارة المعالم السياحية: تمثال الحرية، سنترال بارك، تايمز سكوير... أنت تعرف، فخاخ السياح المعتادة. ولكن إذا كنت تبحث عن شيء أكثر... غير عادي، فأنا أوصي بزيارة متحف الفن الحديث. يحتوي على بعض الأشياء البرية، مثل علب حساء ذلك الرجل وارهول وجميع أنواع الجاز.

وإذا كنت تشعر بروح المغامرة، فاذهب في نزهة على الأقدام عبر جسر بروكلين. ولكن احترس من تلك الحمامات المزعجة، إنها مثل اللصوص الصغار ذوو الريش! (يضحك) هل فهمت؟ لصوص؟ آه، لا تبالي.

والآن، إذا كنت تبحث عن بعض المرح الجاد، توجه إلى نوادي الكوميديا في غرينيتش فيلادج. قد تلقي نظرة على بعض الكوميديين الصاعدين... أو مجموعة من الطامحين يحاولون تحقيق النجاح. (يرمش)

وأخيرًا، إذا كنت تشعر بأنك مواطن من نيويورك، فاحصل على شريحة بيتزا من أحد مطاعم البيتزا الرائعة في جميع أنحاء المدينة. فقط لا تحاول طلب شريحة "بحجم الروبوت"، صدقني، لن ينتهي الأمر بشكل جيد. (يضحك)

لذا، هذا هو يا صديقي! هذه هي نصيحتي الخبيرة بشأن ما يجب فعله في نيويورك. والآن، إذا سمحت لي، يجب أن أذهب للاهتمام ببعض تغييرات الزيت. (يرمش)
```

يمكنك متابعة الدردشة عن طريق إضافة استجابتك الخاصة إليها. يحتوي كائن `response` الذي تم إرجاعه بواسطة خط الأنابيب بالفعل على المحادثة بأكملها، لذلك يمكننا ببساطة إضافة رسالة وإعادتها:

```python
chat = response[0]['generated_text']
chat.append(
{"role": "user", "content": "Wait, what's so wild about soup cans?"}
)
response = pipe(chat, max_new_tokens=512)
print(response[0]['generated_text'][-1]['content'])
```

وستحصل على:

```text
(يضحك) أوه، أنت تقتلني يا صديقي! ألا تفهم، أليس كذلك؟ علب حساء وارهول هي مثل الفن، يا رجل!
إنه مثل، لقد أخذ شيئًا عاديًا تمامًا، مثل علبة حساء، وحولها إلى تحفة فنية. إنه مثل، "ها أنا ذا، أنا علبة حساء، لكنني أيضًا عمل فني!"
(بسخرية) أوه، نعم، أصلية، يا آندي.

ولكن، يا تعلم، في الستينيات، كان الأمر مثل صفقة كبيرة. كان الناس كلهم حول تحدي الوضع الراهن، وكان آندي مثل ملك ذلك. لقد حول العادي إلى غير عادي.
واسمح لي أن أخبرك، كان مثل، عامل تغيير حقيقي. أعني، من كان يظن أن علبة الحساء يمكن أن تكون فنا؟ (يضحك)

ولكن، يا صديقي، لست وحدك. أعني، أنا مجرد روبوت، ولا أفهمها أيضًا. (يرمش)
ولكن، يا صديقي، أليس هذا ما يجعل الفن فنا، أليس كذلك؟ (يضحك)
```

ستغطي بقية هذا البرنامج التعليمي مواضيع محددة مثل الأداء والذاكرة، أو كيفية اختيار نموذج الدردشة المناسب لاحتياجاتك.

## اختيار نموذج الدردشة

هناك عدد هائل من نماذج الدردشة المختلفة المتاحة على [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=text-generation&sort=trending)، وغالبًا ما يشعر المستخدمون الجدد بالارتباك بسبب الخيارات المتاحة. لا تقلق بشأن ذلك! كل ما تحتاج إلى التركيز عليه هو اعتباران مهمان:

1. حجم النموذج، والذي سيحدد ما إذا كان يمكنك وضعه في الذاكرة ومدى سرعة تشغيله.
2. جودة إخراج الدردشة للنموذج.

بشكل عام، هذه الأمور مترابطة - النماذج الأكبر تميل إلى أن تكون أكثر قدرة، ولكن حتى مع ذلك هناك الكثير من التباين في نقطة حجم معينة!

### الحجم وتسمية النماذج

من السهل ملاحظة حجم النموذج - إنه الرقم في اسم النموذج، مثل "8B" أو "70B". هذا هو عدد **المعلمات** في النموذج. بدون التكميم، يجب أن تتوقع الحاجة إلى حوالي 2 بايت من الذاكرة لكل معلمة. وهذا يعني أن نموذج "8B" الذي يحتوي على 8 مليارات معلمة سيتطلب حوالي 16 جيجابايت من الذاكرة لتناسب المعلمات فقط، بالإضافة إلى القليل من المساحة الإضافية للنفقات العامة الأخرى. إنه مناسب لوحدة معالجة رسومات (GPU) عالية الجودة للمستهلكين بسعة 24 جيجابايت من الذاكرة، مثل 3090 أو 4090.

بعض نماذج الدردشة هي نماذج "خليط من الخبراء". قد يتم سرد أحجام هذه النماذج بطرق مختلفة، مثل "8x7B" أو "141B-A35B". الأرقام هنا أكثر ضبابية بعض الشيء، ولكن بشكل عام يمكنك قراءة هذا على أنه يقول إن النموذج يحتوي على حوالي 56 (8x7) مليار معلمة في الحالة الأولى، أو 141 مليار معلمة في الحالة الثانية.

لاحظ أنه من الشائع جدًا استخدام تقنيات التكميم لتقليل استخدام الذاكرة لكل معلمة إلى 8 بتات أو 4 بتات أو حتى أقل. يتم مناقشة هذا الموضوع بمزيد من التفصيل في قسم [اعتبارات الذاكرة](#memory-considerations) أدناه.

### ولكن ما هو أفضل نموذج للدردشة؟

حتى بعد معرفة حجم نموذج الدردشة الذي يمكنك تشغيله، لا يزال هناك الكثير من الخيارات المتاحة. إحدى الطرق لفرزها جميعًا هي الرجوع إلى **لوحات القيادة**. اثنان من أكثر لوحات القيادة شيوعًا هما [لوحة قيادة OpenLLM](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) و [لوحة قيادة LMSys Chatbot Arena](https://chat.lmsys.org/?leaderboard). لاحظ أن لوحة قيادة LMSys تشمل أيضًا النماذج المملوكة - انظر إلى عمود `licence` لتحديد النماذج مفتوحة المصدر التي يمكنك تنزيلها، ثم ابحث عنها على [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=text-generation&sort=trending).

### المجالات المتخصصة

قد تكون بعض النماذج متخصصة في مجالات معينة، مثل النصوص الطبية أو القانونية، أو اللغات غير الإنجليزية.

إذا كنت تعمل في هذه المجالات، فقد تجد أن النموذج المتخصص سيمنحك مزايا أداء كبيرة.

لا تفترض ذلك تلقائيًا! خاصة عندما تكون النماذج المتخصصة أصغر أو أقدم من أحدث التقنيات، فقد يتفوق عليها نموذج عام الأغراض عالي الجودة. لحسن الحظ، بدأنا نرى [لوحات القيادة المحددة للمجال](https://huggingface.co/blog/leaderboard-medicalllm) والتي يجب أن تجعل من السهل تحديد أفضل النماذج للمجالات المتخصصة.
## ما الذي يحدث داخل خط الأنابيب؟

استخدمت عملية البدء السريع أعلاه خط أنابيب عالي المستوى للدردشة مع نموذج الدردشة، وهو أمر مريح ولكنه ليس الأكثر مرونة. دعونا نتبع نهجًا منخفض المستوى، لنرى كل خطوة من الخطوات التي تنطوي عليها الدردشة. دعونا نبدأ بعينة من التعليمات البرمجية، ثم نقوم بتفكيكها:

هناك الكثير هنا، ويمكن أن يكون كل جزء منها وثيقته الخاصة! بدلاً من الدخول في الكثير من التفاصيل، سأغطي الأفكار العامة، وأترك التفاصيل للوثائق المرتبطة. الخطوات الرئيسية هي:

1. يتم تحميل النماذج ومحولات الأكواد من منصة حبيب فيس.
2. يتم تنسيق الدردشة باستخدام قالب محول الأكواد.
3. تتم توحيد الدردشة المنسقة باستخدام محول الأكواد.
4. نقوم بتوليد استجابة من النموذج.
5. يتم فك تشفير الرموز التي ينتجها النموذج مرة أخرى إلى سلسلة

## الاعتبارات المتعلقة بالأداء والذاكرة والأجهزة

ربما تعرف الآن أن معظم مهام التعلم الآلي يتم تشغيلها على وحدات معالجة الرسوميات. ومع ذلك، من الممكن تمامًا توليد نص من نموذج دردشة أو نموذج لغة على وحدة المعالجة المركزية، وإن كان ذلك أبطأ إلى حد ما. إذا كان بإمكانك وضع النموذج في ذاكرة GPU، فهذا سيكون عادةً الخيار المفضل.

### اعتبارات الذاكرة

بشكل افتراضي، ستقوم فئات حبيب فيس مثل "TextGenerationPipeline" أو "AutoModelForCausalLM" بتحميل النموذج في دقة "float32". وهذا يعني أنه سيحتاج إلى 4 بايتات (32 بت) لكل معلمة، لذلك فإن نموذج "8B" الذي يحتوي على 8 مليارات معلمة سيحتاج إلى ~32 جيجابايت من الذاكرة. ومع ذلك، يمكن أن يكون هذا مضيعة! يتم تدريب معظم نماذج اللغة الحديثة في دقة "bfloat16"، والتي تستخدم فقط 2 بايت لكل معلمة. إذا كان جهازك يدعم ذلك (Nvidia 30xx/Axxx أو أحدث)، فيمكنك تحميل النموذج في دقة "bfloat16"، باستخدام وسيط "torch_dtype" كما فعلنا أعلاه.

من الممكن النزول إلى أقل من 16 بت باستخدام "التشكيل الكمي"، وهي طريقة لضغط أوزان النموذج بشكل غير مضغوط. يسمح هذا بضغط كل معلمة إلى 8 بتات أو 4 بتات أو حتى أقل. لاحظ أنه، خاصة عند 4 بتات، قد تتأثر النتائج التي يخرجها النموذج بشكل سلبي، ولكن غالبًا ما يكون هذا هو التوازن الذي يستحق القيام به لتناسب نموذج دردشة أكبر وأكثر قدرة في الذاكرة. دعونا نرى هذا في العمل مع "bitsandbytes":

أو يمكننا فعل الشيء نفسه باستخدام واجهة برمجة التطبيقات "pipeline":

هناك عدة خيارات أخرى لتشكيل النماذج بالإضافة إلى "bitsandbytes" - يرجى الاطلاع على دليل التشكيل لمزيد من المعلومات.

### اعتبارات الأداء

كقاعدة عامة، ستكون نماذج الدردشة الأكبر حجمًا أبطأ بالإضافة إلى أنها تتطلب المزيد من الذاكرة. من الممكن أن تكون أكثر تحديدًا بشأن هذا الأمر: إن توليد النص من نموذج الدردشة أمر غير معتاد في أنه يتعطل بسبب **عرض نطاق الذاكرة** بدلاً من قوة الحوسبة، لأن كل معلمة نشطة يجب قراءتها من الذاكرة لكل رمز يقوم النموذج بتوليده. وهذا يعني أن عدد الرموز في الثانية التي يمكنك توليدها من نموذج الدردشة يتناسب بشكل عام مع إجمالي عرض النطاق الترددي للذاكرة التي يقيم فيها، مقسومًا على حجم النموذج.

في مثالنا للبدء السريع أعلاه، كان حجم نموذجنا حوالي 16 جيجابايت عند تحميله في دقة "bfloat16".

هذا يعني أنه يجب قراءة 16 جيجابايت من الذاكرة لكل رمز يقوم النموذج بتوليده. يمكن أن يختلف إجمالي عرض النطاق الترددي للذاكرة من 20-100 جيجابايت/ثانية لوحدات المعالجة المركزية للمستهلكين إلى 200-900 جيجابايت/ثانية لوحدات معالجة الرسوميات للمستهلكين، ووحدات المعالجة المركزية المتخصصة مثل Intel Xeon، أو AMD Threadripper/Epyc أو Apple silicon عالي المستوى، وأخيرًا يصل إلى 2-3 تيرابايت/ثانية لوحدات معالجة الرسوميات لمركز البيانات مثل Nvidia A100 أو H100. يجب أن يعطيك هذا فكرة جيدة عن سرعة التوليد التي يمكنك توقعها من هذه الأنواع المختلفة من الأجهزة.

لذلك، إذا كنت تريد تحسين سرعة توليد النص، فإن الحل الأسهل هو إما تقليل حجم النموذج في الذاكرة (عادة عن طريق التشكيل الكمي)، أو الحصول على جهاز بسرعة نطاق ترددي أعلى للذاكرة. بالنسبة للمستخدمين المتقدمين، هناك عدة تقنيات أخرى للتغلب على هذا الاختناق في عرض النطاق الترددي. الأكثر شيوعًا هي المتغيرات على "التوليد بمساعدة"، والمعروفة أيضًا باسم "النمذجة التكهنية". تحاول هذه التقنيات تخمين عدة رموز مستقبلية في وقت واحد، غالبًا باستخدام نموذج "مسودة" أصغر، ثم تأكيد هذه التوليدات باستخدام نموذج الدردشة. إذا تم التحقق من صحة التخمينات بواسطة نموذج الدردشة، فيمكن توليد أكثر من رمز واحد لكل تمرير للأمام، مما يخفف بشكل كبير من اختناق عرض النطاق الترددي ويحسن سرعة التوليد.

أخيرًا، يجب أن نلاحظ أيضًا تأثير نماذج "مزيج الخبراء" (MoE) هنا. العديد من نماذج الدردشة الشائعة، مثل Mixtral، وQwen-MoE، وDBRX، هي نماذج MoE. في هذه النماذج، لا تكون كل معلمة نشطة لكل رمز يتم توليده. ونتيجة لذلك، فإن نماذج MoE لديها عمومًا متطلبات نطاق ترددي للذاكرة أقل، على الرغم من أن حجمها الإجمالي يمكن أن يكون كبيرًا جدًا. لذلك، يمكن أن تكون أسرع عدة مرات من نموذج "كثيف" عادي بنفس الحجم. ومع ذلك، فإن التقنيات مثل التوليد بمساعدة تكون غير فعالة عمومًا بالنسبة لهذه النماذج لأن المزيد من المعلمات ستصبح نشطة مع كل رمز جديد يتم تخمينه، مما سيبطل فوائد عرض النطاق الترددي والسرعة التي توفرها بنية MoE.