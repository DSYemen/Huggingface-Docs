# تحسين الاستنتاج باستخدام LLM

دفعت النماذج اللغوية الكبيرة تطبيقات توليد النص، مثل نماذج الدردشة واستكمال الكود، إلى المستوى التالي من خلال إنتاج نص يظهر مستوى عالٍ من الفهم والطلاقة. ولكن ما يجعل هذه النماذج اللغوية قوية - أي حجمها - يطرح أيضًا تحديات أمام الاستنتاج.

الاستنتاج الأساسي بطيء لأن النماذج اللغوية الكبيرة يجب أن تُستدعى بشكل متكرر لتوليد الرمز التالي. تتزايد تسلسل المدخلات مع تقدم التوليد، مما يستغرق وقتًا أطول وأطول بالنسبة للنماذج اللغوية الكبيرة لمعالجة ذلك. تمتلك النماذج اللغوية الكبيرة أيضًا مليارات المعلمات، مما يجعل من الصعب تخزين جميع هذه الأوزان ومعالجتها في الذاكرة.

سيوضح هذا الدليل كيفية استخدام تقنيات التحسين المتاحة في Transformers لتسريع الاستنتاج باستخدام النماذج اللغوية الكبيرة.

## ذاكرة التخزين المؤقت الثابتة kv-cache و torch.compile

أثناء فك التشفير، تحسب النماذج اللغوية الكبيرة قيم key-value (kv) لكل رمز من رموز المدخلات، وبما أنها نماذج تولد النص بشكل تلقائي، فإنها تحسب نفس قيم kv في كل مرة لأن الإخراج المولد يصبح الآن جزءًا من المدخلات. هذا ليس فعالاً للغاية لأنك تقوم بإعادة حساب نفس قيم kv في كل مرة.

لتحسين ذلك، يمكنك استخدام ذاكرة التخزين المؤقت kv لتخزين المفاتيح والقيم السابقة بدلاً من إعادة حسابها في كل مرة. ومع ذلك، نظرًا لأن ذاكرة التخزين المؤقت kv تنمو مع كل خطوة من خطوات التوليد وهي ديناميكية، فإنها تمنعك من الاستفادة من torch.compile، وهي أداة تحسين قوية تقوم بدمج كود PyTorch في نواة سريعة ومحسنة.

تُحل هذه المشكلة باستخدام ذاكرة التخزين المؤقت الثابتة *kv-cache* من خلال تخصيص حجم ذاكرة التخزين المؤقت kv مسبقًا إلى قيمة قصوى، مما يتيح لك دمجها مع torch.compile للوصول إلى سرعة أعلى بمقدار 4 مرات.

بالنسبة لهذا المثال، دعنا نقوم بتحميل نموذج Gemma.

هناك طريقتان يمكنك من خلالهما تكوين النموذج لاستخدام ذاكرة التخزين المؤقت الثابتة kv-cache. بالنسبة لنموذج 7B على A100، تحصل كلتا الطريقتان على تسريع بمقدار 4 مرات في تمرير الإرسال. قد يختلف تسريعك اعتمادًا على حجم النموذج (تحصل النماذج الأكبر على تسريع أقل) والجهاز. إذا كنت تستخدم طريقة `~GenerationMixin.generate`، فإن التسريع يبلغ حوالي 3 مرات. تمرير الإرسال (الذي لا يزال يحصل على تسريع 4 مرات) هو مجرد جزء من كود طريقة `GenerationMixin.generate` بالكامل.

يمكنك الوصول إلى سمة `generation_config` للنموذج وتعيين `cache_implementation` إلى "static".

قم بالاستدعاء `torch.compile` على النموذج لتجميع تمرير الإرسال مع ذاكرة التخزين المؤقت الثابتة kv-cache.

تحت الغطاء، سيحاول `generate` إعادة استخدام نفس كائن ذاكرة التخزين المؤقت، مما يزيل الحاجة إلى إعادة التجميع في كل استدعاء. ومع ذلك، إذا زاد حجم الدفعة أو طول الإخراج الأقصى بين الاستدعاءات، فسيتعين إعادة تهيئة ذاكرة التخزين المؤقت، مما يؤدي إلى تشغيل عملية تجميع جديدة.

يمكن تمرير كائن `StaticCache` إلى تمرير الإرسال للنموذج في إطار وسيط `past_key_values`، مما يمكّن من استخدام هذا الكائن كذاكرة تخزين مؤقت ثابتة kv-cache. باستخدام هذه الاستراتيجية، يمكنك كتابة دالة فك تشفير الرمز التالي بالنظر إلى الرمز الحالي والموضع وموضع ذاكرة التخزين المؤقت للرموز المولدة سابقًا. يمكنك أيضًا تمرير كائن `StaticCache` إلى طريقة `GenerationMixin.generate` واستخدامه عبر الاستدعاءات، تمامًا كما تفعل مع ذاكرة التخزين المؤقت الديناميكية.

هناك بعض الأشياء المهمة التي يجب عليك القيام بها لتمكين ذاكرة التخزين المؤقت الثابتة kv-cache و torch.compile مع طريقة `StaticCache`:

1. قم بتهيئة مثيل `StaticCache` قبل استخدام النموذج للاستدلال. يمكنك هنا تكوين المعلمات مثل حجم الدفعة الأقصى وطول التسلسل.

2. قم بالاستدعاء `torch.compile` على النموذج لتجميع تمرير الإرسال مع ذاكرة التخزين المؤقت الثابتة kv-cache.

3. قم بتعيين `enable_math=True` في سياق مدير `torch.backends.cuda.sdp_kernel` لتمكين تنفيذ C++ الأصلي لـ PyTorch للاهتمام بالمنتج النقطي المُمَوَّز لتسريع الاستدلال بشكل أكبر.

ستقوم طريقة `decode_one_tokens` بفك تشفير الرمز التالي بالنظر إلى الرمز الحالي وموضع الإدخال وموضع ذاكرة التخزين المؤقت للرموز المولدة سابقًا.

بعد فك تشفير الرمز التالي، نقوم بتحديث ذاكرة التخزين المؤقت وموضع ذاكرة التخزين المؤقت.

> [!TIP]
> إذا كنت تريد إعادة استخدام كائن `StaticCache` في موجه جديد، فتأكد من إعادة تعيين محتوياته باستخدام طريقة `.reset()`.
بالتأكيد، سأتبع تعليماتك وسأترجم فقط النص الموجود في الفقرات والعناوين، مع تجاهل الأكواد البرمجية والروابط ورموز HTML وCSS.

## فك التشفير التخميني

تتمثل إحدى المشكلات الأخرى في التوليد التلقائي في أنه لكل رمز من رموز الإدخال، يجب تحميل أوزان النموذج في كل مرة أثناء التمرير للأمام. وهذا بطيء ومرهق بالنسبة لنماذج اللغة الضخمة التي تحتوي على مليارات من المعلمات. ويخفف فك التشفير التخميني من هذا التباطؤ من خلال استخدام نموذج مساعد ثان أصغر وأسرع لتوليد رموز مرشحة يتم التحقق منها بواسطة نموذج LLM الأكبر في تمريرة توجيه واحدة. إذا كانت الرموز المتحقق منها صحيحة، يحصل LLM عليها بشكل أساسي "مجانًا" دون الحاجة إلى توليدها بنفسه. لا يوجد انخفاض في الدقة لأن تمريرة التحقق إلى الأمام تضمن توليد نفس الإخراج كما لو كان LLM قد ولدها بمفرده.

وللحصول على أكبر قدر من التسريع، يجب أن يكون النموذج المساعد أصغر بكثير من LLM بحيث يمكنه توليد الرموز بسرعة. يجب أيضًا أن يتشارك النموذج المساعد وLLM في نفس المحلل الرمزي لتجنب إعادة الترميز وفك ترميز الرموز.

## فك تشفير البحث الشره

لتمكين فك التشفير التخميني، قم بتحميل نموذج مساعد ومرره إلى طريقة [`~GenerationMixin.generate`].

### فك تشفير البحث الشره

فك تشفير البحث الشره هو متغير من فك التشفير التخميني وهو أيضًا متوافق مع البحث الشره والنمذجة. يعمل فك تشفير المطالبة بشكل جيد بشكل خاص لمهمات الإدخال المستندة - مثل الملخص - حيث غالبًا ما تكون هناك كلمات متداخلة بين المطالبة والإخراج. يتم استخدام هذه n-grams المتداخلة كرموز مرشحة LLM.

لتمكين فك تشفير المطالبة، حدد عدد الرموز التي يجب أن تتداخل في معلمة `prompt_lookup_num_tokens`. بعد ذلك، يمكنك تمرير هذا المعلمة إلى طريقة [`~GenerationMixin.generate`].

### فك تشفير النمذجة

لفك تشفير المطالبة مع النمذجة، أضف معلمات `do_sample` و`temperature` إلى طريقة [`~GenerationMixin.generate`] بالإضافة إلى النموذج المساعد.

## تحسين الاهتمام

من المعروف أن مشكلة نماذج المحول هي أن آلية الاهتمام الذاتي تنمو بشكل تربيعي في الحساب والذاكرة مع عدد رموز الإدخال. يتم تضخيم هذا القيد فقط في نماذج اللغة الضخمة التي تتعامل مع تسلسلات أطول. لمعالجة هذا، جرب FlashAttention2 أو تنفيذ PyTorch للاهتمام بمنتج النقاط المُمَوَّز (SDPA)، والتي تعد عمليات تنفيذ أكثر كفاءة في الذاكرة ويمكنها تسريع الاستدلال.

### FlashAttention-2

يقسم FlashAttention وFlashAttention-2 عملية حساب الاهتمام إلى أجزاء أصغر ويقلل من عدد عمليات القراءة/الكتابة الوسيطة في ذاكرة GPU لتسريع الاستدلال. تتحسن FlashAttention-2 من خوارزمية FlashAttention الأصلية من خلال الموازاة أيضًا عبر بعد طول التسلسل وتقسيم العمل بشكل أفضل على الأجهزة لتقليل التكاليف العامة للاتصال والمزامنة.

لاستخدام FlashAttention-2، قم بتعيين `attn_implementation="flash_attention_2"` في طريقة [`~PreTrainedModel.from_pretrained`].

### اهتمام PyTorch بمنتج النقاط المُمَوَّز

يتم تمكين الاهتمام بمنتج النقاط المُمَوَّز (SDPA) تلقائيًا في PyTorch 2.0 وهو يدعم FlashAttention وxFormers وتنفيذ C++ في PyTorch. تختار SDPA خوارزمية الاهتمام الأكثر أداءً إذا كنت تستخدم backend CUDA. بالنسبة إلى backends الأخرى، تستخدم SDPA بشكل افتراضي تنفيذ C++ في PyTorch.

يمكنك استخدام مدير سياق [torch.backends.cuda.sdp_kernel](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html) لتمكين أو تعطيل أي من خوارزميات الاهتمام الثلاثة بشكل صريح. على سبيل المثال، قم بتعيين `enable_flash=True` لتمكين FlashAttention.

## التكميم

يقلل التكميم من حجم أوزان LLM عن طريق تخزينها بدقة أقل. يترجم هذا إلى استخدام ذاكرة أقل ويجعل تحميل نماذج اللغة الضخمة للاستدلال أكثر سهولة إذا كنت مقيدًا بذاكرة GPU الخاصة بك. إذا لم تكن مقيدًا بـ GPU الخاص بك، فلست بحاجة إلى تحويل نموذجك إلى أرقام لأنه يمكن أن يتكبد تكلفة صغيرة في الكمون (باستثناء وحدات AWQ وAWQ المدمجة) بسبب الخطوة الإضافية المطلوبة لتحويل الأوزان وإلغاء تحويلها.

هناك العديد من مكتبات التكميم (راجع دليل التكميم لمزيد من التفاصيل) المتاحة، مثل Quanto وAQLM وAWQ وAutoGPTQ. لا تتردد في تجربتها وشاهد أيها يعمل بشكل أفضل لحالتك الاستخدامية. نوصي أيضًا بقراءة منشور المدونة نظرة عامة على مخططات التكميم المدعومة بشكل طبيعي في 🤗 المحولات والتي تقارن AutoGPTQ وbitsandbytes.

استخدم آلة حاسبة ذاكرة النموذج أدناه لتقدير ومقارنة مقدار الذاكرة المطلوبة لتحميل نموذج. على سبيل المثال، جرب تقدير مقدار الذاكرة التي يتكلفها تحميل [Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1).

لتحميل Mistral-7B-v0.1 بدقة نصفية، قم بتعيين معلمة `torch_dtype` في طريقة [`~transformers.AutoModelForCausalLM.from_pretrained`] إلى `torch.bfloat16`. يتطلب هذا 13.74 جيجابايت من الذاكرة.

لتحميل نموذج مُمَوَّز (8 بتات أو 4 بتات) للاستدلال، جرب [bitsandbytes](https://hf.co/docs/bitsandbytes) وقم بتعيين معلمات `load_in_4bit` أو `load_in_8bit` إلى `True`. يتطلب تحميل النموذج في 8 بتات فقط 6.87 جيجابايت من الذاكرة.