# آليات الانتباه 

تستخدم معظم نماذج المحول الكامل الانتباه بمعنى أن مصفوفة الانتباه تكون مربعة. يمكن أن يمثل ذلك عنق زجاجة حسابيًا كبيرًا عندما يكون لديك نصوص طويلة. Longformer وReformer هما نموذجان يحاولان أن يكونا أكثر كفاءة ويستخدمان نسخة مبعثرة من مصفوفة الانتباه لتسريع التدريب.

## الاهتمام LSH 

يستخدم [Reformer](model_doc/reformer) اهتمام LSH. في softmax(QK^t)، فإن أكبر العناصر فقط (في بعد softmax) من المصفوفة QK^t هي التي ستعطي مساهمات مفيدة. لذلك، لكل استعلام q في Q، يمكننا أن نأخذ في الاعتبار فقط المفاتيح k في K القريبة من q. وتُستخدم دالة تجزئة لتحديد ما إذا كان q وk قريبين. يتم تعديل قناع الانتباه لإخفاء الرمز الحالي (باستثناء الموضع الأول)، لأنه سيعطي استعلامًا ومفتاحًا متساويين (لذلك متشابهين جدًا). نظرًا لأن التجزئة يمكن أن تكون عشوائية بعض الشيء، يتم استخدام عدة دوال تجزئة في الممارسة العملية (يحددها معلمة n_rounds) ثم يتم حساب متوسطها معًا.

## الاهتمام المحلي 

يستخدم [Longformer](model_doc/longformer) الانتباه المحلي: غالبًا ما يكون السياق المحلي (على سبيل المثال، ما هما الرمزان الموجودان على اليسار واليمين؟) كافيًا لاتخاذ إجراء بالنسبة للرمز المعطى. أيضًا، عن طريق تكديس طبقات الانتباه التي لها نافذة صغيرة، سيكون للطبقة الأخيرة مجال استقبال أكبر من مجرد الرموز الموجودة في النافذة، مما يسمح لها ببناء تمثيل للجملة بأكملها.

كما يتم منح بعض الرموز المدخلة مسبقًا اهتمامًا عالميًا: بالنسبة لهذه الرموز القليلة، يمكن لمصفوفة الانتباه الوصول إلى جميع الرموز وتكون هذه العملية متماثلة: جميع الرموز الأخرى لديها القدرة على الوصول إلى تلك الرموز المحددة (بالإضافة إلى تلك الموجودة في نافذتهم المحلية). وهذا موضح في الشكل 2د من الورقة، انظر أدناه لعينة قناع الانتباه:

يستخدم النموذج مصفوفات الانتباه هذه مع عدد أقل من المعلمات، مما يسمح له بإدخالات ذات طول تسلسل أكبر.

## الحيل الأخرى 

### الترميزات الموضعية المحورية 

يستخدم [Reformer](model_doc/reformer) ترميزات موضعية محورية: في نماذج المحول التقليدية، يكون الترميز الموضعي E مصفوفة بحجم \\(l\\) بواسطة \\(d\\)، حيث \\(l\\) هو طول التسلسل و\\(d\\) هو بُعد الحالة المخفية. إذا كان لديك نصوص طويلة جدًا، فقد تكون هذه المصفوفة ضخمة وتستهلك الكثير من المساحة على وحدة معالجة الرسومات (GPU). للتخفيف من ذلك، تتكون الترميزات الموضعية المحورية من تحليل تلك المصفوفة الكبيرة E إلى مصفوفتين أصغر E1 وE2، بأبعاد \\(l_{1} \times d_{1}\\) و\\(l_{2} \times d_{2}\\)، بحيث \\(l_{1} \times l_{2} = l\\) و\\(d_{1} + d_{2} = d\\) (مع المنتج لأطوال، ينتهي الأمر بكونه أصغر بكثير). يتم الحصول على الترميز للخطوة الزمنية \\(j\\) في E عن طريق دمج الترميزات لخطوة الوقت \\(j \% l1\\) في E1 و\\(j // l1\\) في E2.