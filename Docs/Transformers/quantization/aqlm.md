# AQLM

تمثل إضافة الضغط الكمي للغة النماذج (AQLM) طريقة لضغط نماذج اللغة الكبيرة. فهو يقوم بضغط العديد من الأوزان معًا ويستفيد من أوجه الترابط بينها. يمثل AQLM مجموعات من 8-16 وزنًا على أنها مجموع رموز متعددة.

يتم تحقيق دعم الاستدلال لـ AQLM في مكتبة `aqlm`. تأكد من تثبيته لتشغيل النماذج (ملاحظة: تعمل aqlm فقط مع python>=3.10):

يوفر المكتبة نوى فعالة لكل من GPU وCPU الاستدلال والتدريب.

يمكن العثور على التعليمات حول كيفية ضغط النماذج بنفسك، بالإضافة إلى جميع التعليمات البرمجية ذات الصلة، في مستودع GitHub المقابل. لتشغيل نماذج AQLM، قم ببساطة بتحميل نموذج تمت ضغطه باستخدام AQLM:

## PEFT

بدءًا من الإصدار `aqlm 1.0.2`، يدعم AQLM الضبط الدقيق الفعال للبارامترات على شكل [LoRA](https://huggingface.co/docs/peft/package_reference/lora) المدمج في مكتبة [PEFT](https://huggingface.co/blog/peft).

## تكوينات AQLM

تختلف إعدادات ضغط AQLM بشكل أساسي في عدد كتيبات الرموز المستخدمة بالإضافة إلى أحجام كتيبات الرموز بالبتات. فيما يلي أكثر الإعدادات شيوعًا، بالإضافة إلى نوى الاستدلال التي تدعمها:

| النواة | عدد كتيبات الرموز | حجم كتيبة الرموز، بت | التدوين | الدقة | تسريع | استدلال GPU سريع | استدلال CPU سريع |
| --- | ---------------- | ------------------- | -------- | -------- | -------- | ------------------ | ------------------ |
| Triton | K | N | KxN | - | حتى ~0.7x | ✅ | ❌ |
| CUDA | 1 | 16 | 1x16 | الأفضل | حتى ~1.3x | ✅ | ❌ |
| CUDA | 2 | 8 | 2x8 | جيد | حتى ~3.0x | ✅ | ❌ |
| Numba | K | 8 | Kx8 | جيد جدًا | حتى ~4.0x | ❌ | ✅ |