# تسريع الاستنتاج

هناك عدة طرق لتحسين أداء "Diffusers" من أجل زيادة سرعة الاستنتاج، مثل تقليل العبء الحسابي من خلال خفض دقة البيانات أو استخدام نموذج خفيف مُقطَّر. هناك أيضًا تطبيقات فعالة للذاكرة لآلية الانتباه، مثل [xFormers](xformers) و [scaled dot product attention](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html) في PyTorch 2.0، والتي تقلل من استخدام الذاكرة، مما يؤدي بشكل غير مباشر إلى تسريع الاستنتاج. يمكن تكديس تحسينات السرعة المختلفة معًا للحصول على أوقات استنتاج أسرع.

> [!TIP]
> يمكن أن يؤدي التحسين من أجل زيادة سرعة الاستنتاج أو تقليل استخدام الذاكرة إلى تحسين الأداء في الفئة الأخرى، لذلك يجب عليك محاولة التحسين لكليهما كلما أمكن ذلك. يركز هذا الدليل على سرعة الاستنتاج، ولكن يمكنك معرفة المزيد حول تقليل استخدام الذاكرة في دليل [تقليل استخدام الذاكرة](memory).

تم الحصول على أوقات الاستنتاج أدناه من خلال توليد صورة واحدة بحجم 512x512 بناءً على مُطال "a photo of an astronaut riding a horse on mars" ("صورة لرائد فضاء يمتطي حصانًا على كوكب المريخ") باستخدام 50 خطوة من خوارزمية DDIM على معالج رسومات NVIDIA A100.

| الإعداد | زمن التأخير | زيادة السرعة |
|----------|---------|----------|
| baseline | 5.27s   | x1       |
| tf32     | 4.14s   | x1.27    |
| fp16     | 3.51s   | x1.50    |
| combined | 3.41s   | x1.54    |

## TensorFloat-32

في معمارية Ampere وأجهزة CUDA الأحدث، يمكن لعمليات الضرب المصفوفي والعمليات التحويلية استخدام وضع [TensorFloat-32 (tf32)](https://blogs.nvidia.com/blog/2020/05/14/tensorfloat-32-precision-format/) لإجراء عمليات حسابية أسرع، ولكن أقل دقة بشكل طفيف. بشكل افتراضي، يقوم PyTorch بتمكين وضع tf32 للعمليات التحويلية ولكن ليس لعمليات الضرب المصفوفي. ما لم تتطلب شبكتك الدقة الكاملة لـ float32، نوصي بتمكين tf32 لعمليات الضرب المصفوفي. يمكن أن يؤدي ذلك إلى تسريع العمليات الحسابية بشكل كبير مع خسارة طفيفة في الدقة العددية.

```python
import torch

torch.backends.cuda.matmul.allow_tf32 = True
```

تعرف على المزيد حول tf32 في دليل [التدريب باستخدام الدقة المختلطة](https://huggingface.co/docs/transformers/en/perf_train_gpu_one#tf32).

## أوزان نصف الدقة

للحفاظ على ذاكرة GPU وزيادة السرعة، قم بتعيين `torch_dtype=torch.float16` لتحميل وتشغيل أوزان النموذج مباشرة بأوزان نصف الدقة.

```Python
import torch
from diffusers import DiffusionPipeline

pipe = DiffusionPipeline.from_pretrained(
"runwayml/stable-diffusion-v1-5",
torch_dtype=torch.float16,
use_safetensors=True,
)
pipe = pipe.to("cuda")
```

> [!WARNING]
> لا تستخدم [torch.autocast](https://pytorch.org/docs/stable/amp.html#torch.autocast) في أي من خطوط الأنابيب لأنه قد يؤدي إلى ظهور صور سوداء وهو دائمًا أبطأ من دقة float16 النقية.

## النموذج المقطَّر

يمكنك أيضًا استخدام نموذج مُقطَّر من Stable Diffusion ومُرمِّز فك التشفير لتسريع الاستنتاج. أثناء التقطير، يتم التخلص من العديد من الكتل الاحتياطية وكتل الانتباه في شبكة UNet لتخفيض حجم النموذج بنسبة 51% وتحسين زمن التأخير على وحدة المعالجة المركزية/وحدة معالجة الرسومات بنسبة 43%. النموذج المقطَّر أسرع ويستخدم ذاكرة أقل أثناء توليد صور ذات جودة مماثلة للنموذج الكامل من Stable Diffusion.

> [!TIP]
> اقرأ منشور المدونة [Open-sourcing Knowledge Distillation Code and Weights of SD-Small and SD-Tiny](https://huggingface.co/blog/sd_distillation) ("مشاركة كود وأوزان تقطير المعرفة لـ SD-Small و SD-Tiny") لمعرفة المزيد حول كيفية عمل التدريب بالتقطير لإنتاج نموذج تنبئي أسرع وأصغر وأقل تكلفة.

تم الحصول على أوقات الاستنتاج أدناه من خلال توليد 4 صور بناءً على مُطال "a photo of an astronaut riding a horse on mars" ("صورة لرائد فضاء يمتطي حصانًا على كوكب المريخ") باستخدام 25 خطوة من خوارزمية PNDM على معالج رسومات NVIDIA A100. تم تكرار كل عملية توليد 3 مرات باستخدام النموذج المقطَّر Stable Diffusion v1.4 من [Nota AI](https://hf.co/nota-ai).

| الإعداد | زمن التأخير | زيادة السرعة |
|------------------------------|---------|----------|
| baseline                     | 6.37s   | x1       |
| distilled                    | 4.18s   | x1.52    |
| distilled + tiny autoencoder | 3.83s   | x1.66    |

دعنا نحمل النموذج المقطَّر من Stable Diffusion ونقارنه بالنموذج الأصلي من Stable Diffusion.

```py
from diffusers import StableDiffusionPipeline
import torch

distilled = StableDiffusionPipeline.from_pretrained(
"nota-ai/bk-sdm-small", torch_dtype=torch.float16, use_safetensors=True,
).to("cuda")
prompt = "a golden vase with different flowers"
generator = torch.manual_seed(2023)
image = distilled("a golden vase with different flowers", num_inference_steps=25, generator=generator).images[0]
image
```

<div class="flex gap-4">
<div>
<img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/original_sd.png"/>
<figcaption class="mt-2 text-center text-sm text-gray-500">النموذج الأصلي من Stable Diffusion</figcaption>
</div>
<div>
<img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/distilled_sd.png"/>
<figcaption class="mt-2 text-center text-sm text-gray-500">النموذج المقطَّر من Stable Diffusion</figcaption>
</div>
</div>

### مُرمِّز فك التشفير الصغير

لتسريع الاستنتاج بشكل أكبر، استبدل مُرمِّز فك التشفير بإصدار مُقطَّر منه [distilled version](https://huggingface.co/sayakpaul/taesdxl-diffusers) ("الإصدار المقطَّر").

```py
import torch
from diffusers import AutoencoderTiny, StableDiffusionPipeline

distilled = StableDiffusionPipeline.from_pretrained(
"nota-ai/bk-sdm-small", torch_dtype=torch.float16, use_safetensors=True,
).to("cuda")
distilled.vae = AutoencoderTiny.from_pretrained(
"sayakpaul/taesd-diffusers", torch_dtype=torch.float16, use_safetensors=True,
).to("cuda")

prompt = "a golden vase with different flowers"
generator = torch.manual_seed(2023)
image = distilled("a golden vase with different flowers", num_inference_steps=25, generator=generator).images[0]
image
```

<div class="flex justify-center">
<div>
<img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/distilled_sd_vae.png" />
<figcaption class="mt-2 text-center text-sm text-gray-500">النموذج المقطَّر من Stable Diffusion + مُرمِّز فك التشفير الصغير</figcaption>
</div>
</div>