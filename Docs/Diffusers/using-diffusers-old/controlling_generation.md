بالتأكيد! فيما يلي ترجمة للجزء المطلوب من النص:

# التحكم في التوليد

لطالما سعى المجتمع إلى التحكم في المخرجات التي تولدها نماذج الانتشار، وأصبح هذا الموضوع الآن أحد مجالات البحث النشطة. في العديد من نماذج الانتشار الشائعة، يمكن للتغييرات الطفيفة في المدخلات، سواء الصور أو النصوص، أن تغير المخرجات بشكل كبير. في عالم مثالي، نريد أن نتمكن من التحكم في كيفية الحفاظ على المعاني وتغييرها.

تتعلق معظم الأمثلة على الحفاظ على المعاني بالقدرة على رسم خريطة دقيقة لتغيير في المدخلات إلى تغيير في المخرجات. أي أن إضافة صفة إلى موضوع في موجه يحافظ على الصورة بأكملها، ولا يعدل سوى الموضوع الذي تم تغييره. أو، تباين الصورة لموضوع معين يحافظ على وضع الموضوع.

بالإضافة إلى ذلك، هناك خصائص للصور المولدة التي نود أن نؤثر عليها إلى ما بعد الحفاظ على المعنى. أي بشكل عام، نود أن تكون مخرجاتنا ذات جودة جيدة، وتلتزم بأسلوب معين، أو تكون واقعية.

سنقوم بتوثيق بعض التقنيات التي تدعمها "Diffusers" للتحكم في توليد نماذج الانتشار. الكثير من ذلك هو أحدث ما توصل إليه البحث ويمكن أن يكون دقيقًا جدًا. إذا كان هناك شيء يحتاج إلى توضيح أو لديك اقتراح، فلا تتردد في فتح مناقشة على [المنتدى](https://discuss.huggingface.co/c/discussion-related-to-httpsgithubcomhuggingfacediffusers/63) أو [قضية GitHub](https://github.com/huggingface/diffusers/issues).

نقدم شرحًا عالي المستوى لكيفية التحكم في التوليد، بالإضافة إلى مقتطف من الجوانب التقنية. للحصول على تفسيرات أكثر تعمقًا للجوانب التقنية، فإن الأوراق الأصلية المرتبطة بأنابيب هي دائمًا أفضل الموارد.

اعتمادًا على حالة الاستخدام، يجب اختيار تقنية وفقًا لذلك. في كثير من الحالات، يمكن الجمع بين هذه التقنيات. على سبيل المثال، يمكنك الجمع بين الانقلاب النصي مع SEGA لتوفير المزيد من الإرشادات الدلالية للمخرجات التي تم إنشاؤها باستخدام الانقلاب النصي.

ما لم يذكر خلاف ذلك، فإن هذه التقنيات تعمل مع النماذج الموجودة ولا تتطلب أوزانها الخاصة.

1. [InstructPix2Pix](#instruct-pix2pix)
2. [Pix2Pix Zero](#pix2pix-zero)
3. [Attend and Excite](#attend-and-excite)
4. [الإرشاد الدلالي (SEGA)](#semantic-guidance-sega)
5. [إرشادات الانتباه الذاتي (SAG)](#self-attention-guidance-sag)
6. [Depth2Image](#depth2image)
7. [MultiDiffusion Panorama](#multidiffusion-panorama)
8. [DreamBooth](#dreambooth)
9. [الانقلاب النصي](#textual-inversion)
10. [ControlNet](#controlnet)
11. [ترجيح الموجه](#prompt-weighting)
12. [الانتشار المخصص](#custom-diffusion)
13. [تحرير النماذج](#model-editing)
14. [DiffEdit](#diffedit)
15. [T2I-Adapter](#t2i-adapter)
16. [FABRIC](#fabric)

للراحة، نقدم جدولًا للإشارة إلى الطرق التي تعمل بالاستنتاج فقط والطرق التي تتطلب الضبط الدقيق/التدريب.

| الأسلوب | الاستدلال فقط | يتطلب التدريب/الضبط الدقيق | التعليقات |
| :----: | :----------: | :--------------------: | :----: |
|        |              |                        |        |
|        |              |                        |        |
|        |              |                        |        |
|        |              |                        |        |
|        |              |                        |        |
|        |              |                        |        |
|        |              |                        |        |
|        |              |                        |        |
|        |              |                        |        |
|        |              |                        |        |
|        |              |                        |        |
|        |              |                        |        |
|        |              |                        |        |
|        |              |                        |        |
|        |              |                        |        |
 ## InstructPix2Pix 

[ورقة](https://arxiv.org/abs/2211.09800) 

[InstructPix2Pix](../api/pipelines/pix2pix) تمت معايرته الدقيقة من Stable Diffusion لدعم تحرير الصور المدخلة. إنه يأخذ كمدخلات صورة وموجه يصف عملية التحرير، وينتج الصورة المعدلة.

تم تدريب InstructPix2Pix بشكل صريح للعمل بشكل جيد مع موجهات تشبه [InstructGPT](https://openai.com/blog/instruction-following/) .

 ## Pix2Pix Zero 

[ورقة](https://arxiv.org/abs/2302.03027) 

يسمح [Pix2Pix Zero](../api/pipelines/pix2pix_zero) بتعديل صورة بحيث يتم ترجمة مفهوم أو موضوع واحد إلى آخر مع الحفاظ على دلالة الصورة العامة.

تتم توجيه عملية إزالة الضوضاء من تضمين مفهومي واحد إلى آخر. يتم تحسين الوسائط الخفية المتوسطة أثناء عملية إزالة التشويش لدفع خرائط الاهتمام نحو خرائط الاهتمام المرجعية. خرائط الاهتمام المرجعية هي من عملية إزالة ضوضاء الصورة المدخلة ويتم استخدامها لتشجيع الحفاظ على المعنى.

يمكن استخدام Pix2Pix Zero لتحرير الصور الاصطناعية وكذلك الصور الحقيقية.

- لتحرير الصور الاصطناعية، قم أولاً بتوليد صورة بناءً على عنوان. بعد ذلك، نقوم بتوليد عناوين للصور للمفهوم الذي سيتم تحريره وللمفهوم المستهدف الجديد. يمكننا استخدام نموذج مثل [Flan-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5) لهذا الغرض. بعد ذلك، يتم إنشاء تضمينات "متوسطة" للموجه لكل من المفهوم المصدر والمفهوم المستهدف عبر مشفر النص. أخيرًا، يتم استخدام خوارزمية pix2pix-zero لتحرير الصورة الاصطناعية.
- لتحرير صورة حقيقية، قم أولاً بتوليد عنوان صورة باستخدام نموذج مثل [BLIP](https://huggingface.co/docs/transformers/model_doc/blip). بعد ذلك، قم بتطبيق عكس DDIM على الموجه والصورة لتوليد وسائط خفية "عكسية". مثلما كان من قبل، يتم إنشاء تضمينات "متوسطة" للموجه لكل من المفهوم المصدر والمفهوم المستهدف، وأخيرًا يتم استخدام خوارزمية pix2pix-zero بالاقتران مع الوسائط الخفية "العكسية" لتحرير الصورة.

<Tip> 

Pix2Pix Zero هو أول نموذج يسمح بتحرير الصور "بدون أي بيانات تدريب". وهذا يعني أن النموذج يمكنه تحرير صورة في أقل من دقيقة على وحدة معالجة رسومات (GPU) للمستهلك كما هو موضح [هنا](../api/pipelines/pix2pix_zero#usage-example). 

</Tip> 

كما ذكرنا سابقًا، يتضمن Pix2Pix Zero تحسين الوسائط الخفية (وليس أي من UNet أو VAE أو مشفر النص) لتوجيه التوليد نحو مفهوم محدد. وهذا يعني أن الأنبوب بأكمله قد يتطلب ذاكرة أكبر من [StableDiffusionPipeline](../api/pipelines/stable_diffusion/text2img) قياسي.

<Tip> 

هناك تمييز مهم بين الطرق مثل InstructPix2Pix و Pix2Pix Zero وهو أن الأول يتضمن ضبط دقيق للأوزان المسبقة التدريب بينما لا يفعل الأخير. وهذا يعني أنه يمكنك تطبيق Pix2Pix Zero على أي من نماذج Stable Diffusion المتاحة. 

</Tip>
## Attend and Excite
تسمح [Attend and Excite](../api/pipelines/attend_and_excite) بتمثيل الموضوعات في المحث بشكل أمين في الصورة النهائية.
تُعطى مجموعة من مؤشرات الرموز كمدخلات، والتي تتوافق مع الموضوعات في المحث التي يجب أن تكون موجودة في الصورة. أثناء إزالة التشويش، يُضمن لكل مؤشر رمز عتبة اهتمام دنيا لرقعة واحدة على الأقل من الصورة. يتم تحسين المخفيات الوسيطة بشكل تكراري أثناء عملية إزالة التشويش لتعزيز اهتمام رمز الموضوع الأكثر إهمالاً حتى يتم تجاوز عتبة الاهتمام لجميع رموز الموضوع.
مثل Pix2Pix Zero، تتضمن Attend and Excite أيضًا حلقة تحسين مصغرة (مع ترك الأوزان المُدربة مسبقًا دون تغيير) في خط أنابيبها وقد تتطلب ذاكرة أكبر من [StableDiffusionPipeline](../api/pipelines/stable_diffusion/text2img) المعتادة.

## Semantic Guidance (SEGA)
تسمح [SEGA](../api/pipelines/semantic_stable_diffusion) بإضافة أو إزالة مفهوم واحد أو أكثر من صورة. يمكن أيضًا التحكم في قوة المفهوم. أي يمكن استخدام مفهوم الابتسامة لزيادة أو تقليل ابتسامة صورة شخصية بشكل تدريجي.
على غرار كيفية تقديم الإرشادات الخالية من التصنيفات عبر إدخالات المحث الفارغة، توفر SEGA الإرشادات للمحثات المفاهيمية. يمكن تطبيق العديد من هذه المحثات المفاهيمية في نفس الوقت. يمكن لكل محث مفاهيمي إضافة مفهومه أو إزالته اعتمادًا على ما إذا كان الإرشاد مطبقًا بشكل إيجابي أو سلبي.
على عكس Pix2Pix Zero أو Attend and Excite، تتفاعل SEGA مباشرة مع عملية الانتشار بدلاً من إجراء أي تحسين صريح قائم على التدرجات.

## Self-attention Guidance (SAG)
[Self-attention Guidance](../api/pipelines/self_attention_guidance) يحسن الجودة العامة للصور.
يوفر SAG الإرشادات من التنبؤات غير المشروطة على تفاصيل التردد العالي للصور المشروطة بالكامل. يتم استخراج تفاصيل التردد العالي من خرائط الاهتمام الذاتي لشبكة UNet.

## Depth2Image
[Depth2Image](../api/pipelines/stable_diffusion/depth2img) تمت معايرته بدقة من Stable Diffusion للحفاظ بشكل أفضل على الدلالات للتباين الموجه بالنص الصورة.
فهي تفرض شرطًا على تقدير العمق الأحادي للصورة الأصلية.

## MultiDiffusion Panorama
يعرّف [MultiDiffusion Panorama](../api/pipelines/panorama) عملية توليد جديدة عبر نموذج انتشار مُدرب مسبقًا. تربط هذه العملية بين طرق التوليد المتعددة للانتشار التي يمكن تطبيقها بسهولة لتوليد صور عالية الجودة ومتنوعة. تلتزم النتائج بالضوابط التي يوفرها المستخدم، مثل نسبة العرض إلى الارتفاع المرغوبة (مثل الصور البانورامية)، وإشارات التوجيه المكاني، والتي تتراوح من أقنعة التجزئة المحكمة إلى صناديق الإحاطة.
يتيح MultiDiffusion Panorama إمكانية إنشاء صور عالية الجودة بنسب عرض إلى ارتفاع عشوائية (مثل الصور البانورامية).

## ضبط النماذج الخاصة بك
بالإضافة إلى النماذج المُدربة مسبقًا، تحتوي Diffusers على نصوص تدريب لضبط دقة النماذج على البيانات التي يوفرها المستخدم.

## DreamBooth
[DreamBooth](../training/dreambooth) يضبط دقة نموذج لتعليمه حول موضوع جديد. أي يمكن استخدام بضع صور لشخص ما لتوليد صور لهذا الشخص بأنماط مختلفة.

## Textual Inversion
[Textual Inversion](../training/text_inversion) يضبط دقة نموذج لتعليمه حول مفهوم جديد. أي يمكن استخدام بضع صور لأسلوب فني لتوليد صور بهذا الأسلوب.

## ControlNet
[ControlNet](../api/pipelines/controlnet) عبارة عن شبكة مساعدة تضيف شرطًا إضافيًا.
هناك 8 شبكات ControlNet أساسية مُدربة مسبقًا على عمليات ضبط مختلفة مثل اكتشاف الحواف، والخطوط العشوائية، وخرائط العمق، والتجزئات الدلالية.

## Prompt Weighting
[Prompt weighting](../using-diffusers/weighted_prompts) هي تقنية بسيطة تضع وزن اهتمام أكبر على أجزاء معينة من إدخال النص.

## Custom Diffusion
[Custom Diffusion](../training/custom_diffusion) يضبط دقة خرائط الاهتمام المتقاطع لنموذج انتشار مُدرب مسبقًا للصور النصية فقط. كما يسمح أيضًا بإجراء الانقلاب النصي الإضافي. فهو يدعم التدريب متعدد المفاهيم بشكل افتراضي. مثل DreamBooth وTextual Inversion، يستخدم Custom Diffusion أيضًا لتعليم نموذج انتشار مُدرب مسبقًا للصور النصية حول مفاهيم جديدة لتوليد مخرجات تتضمن مفهوم (مفاهيم) الاهتمام.

## Model Editing
تساعدك [خط أنابيب تحرير النماذج](../api/pipelines/model_editing) على التخفيف من بعض الافتراضات الضمنية غير الصحيحة التي قد يتخذها نموذج انتشار مُدرب مسبقًا للصور النصية حول الموضوعات الموجودة في محث الإدخال. على سبيل المثال، إذا قمت بمحث Stable Diffusion لتوليد صور لـ "حزمة من الورود"، فمن المرجح أن تكون الورود في الصور المولدة حمراء. تساعدك هذه الخط أنابيب على تغيير هذا الافتراض.

## DiffEdit
يسمح [DiffEdit](../api/pipelines/diffedit) بالتحرير الدلالي لصور الإدخال إلى جانب محثات الإدخال مع الحفاظ على صور الإدخال الأصلية قدر الإمكان.

## T2I-Adapter
[T2I-Adapter](../api/pipelines/stable_diffusion/adapter) عبارة عن شبكة مساعدة تضيف شرطًا إضافيًا.
هناك 8 وحدات تكييف أساسية مُدربة مسبقًا على عمليات ضبط مختلفة مثل اكتشاف الحواف، والرسومات، وخرائط العمق، والتجزئات الدلالية.

## Fabric
[Fabric](https://github.com/huggingface/diffusers/tree/442017ccc877279bcf24fbe92f92d3d0def191b6/examples/community#stable-diffusion-fabric-pipeline) هو نهج خالٍ من التدريب قابل للتطبيق على مجموعة واسعة من نماذج الانتشار الشائعة، والتي تستغل طبقة الاهتمام الذاتي الموجودة في أكثر الهندسات المعمارية استخدامًا لفرض عملية الانتشار على مجموعة من صور التعليقات.