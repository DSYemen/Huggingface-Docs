# ControlNet

ControlNet ูู ููุน ูู ุงูููุงุฐุฌ ุงููุณุชุฎุฏูุฉ ููุชุญูู ูู ููุงุฐุฌ ุงูุชุดุงุฑ ุงูุตูุฑ ูู ุฎูุงู ุชูููุฑ ุฏุฎู ุฅุถุงูู ููุตูุฑุฉ. ููุงู ุงูุนุฏูุฏ ูู ุฃููุงุน ุงููุฏุฎูุงุช ุงููุดุฑูุทุฉ (Canny edgeุ ุฑุณู ุงููุณุชุฎุฏูุ ูุถุน ุงูุฅูุณุงูุ ุงูุนููุ ูุงููุฒูุฏ) ุงูุชู ููููู ุงุณุชุฎุฏุงููุง ููุชุญูู ูู ูููุฐุฌ ุงูุงูุชุดุงุฑ. ููุฐุง ูููุฏ ููุบุงูุฉ ูุฃูู ูููุญู ุชุญูููุง ุฃูุจุฑ ูู ุฅูุดุงุก ุงูุตูุฑุ ููุง ูุณูู ุฅูุดุงุก ุตูุฑ ูุญุฏุฏุฉ ุฏูู ุงูุญุงุฌุฉ ุฅูู ุชุฌุฑุจุฉ ูุทุงูุจุงุช ูุตูุฉ ุฃู ููู ุฅุฒุงูุฉ ุงูุถูุถุงุก ุงููุฎุชููุฉ.

<Tip>
ุงุทูุน ุนูู ุงููุณู 3.5 ูู ูุฑูุฉ [ControlNet](https://huggingface.co/papers/2302.05543) ุงูุฅุตุฏุงุฑ 1 ููุญุตูู ุนูู ูุงุฆูุฉ ุจุชูููุฐุงุช ControlNet ููุฎุชูู ุงููุฏุฎูุงุช ุงููุดุฑูุทุฉ. ููููู ุงูุนุซูุฑ ุนูู ููุงุฐุฌ ControlNet ุงููุดุฑูุทุฉ ุงูุฑุณููุฉ ูุงููุณุชูุฑุฉ ุนูู ููู ุชุนุฑูู [lllyasviel](https://huggingface.co/lllyasviel) Hubุ ูุงููุฒูุฏ ูู ุงูููุงุฐุฌ [ุงููุฏุฑุจุฉ ูู ูุจู ุงููุฌุชูุน](https://huggingface.co/models?other=stable-diffusion&other=controlnet) ุนูู Hub.

ุจุงููุณุจุฉ ูููุงุฐุฌ ControlNet SDXL (Stable Diffusion XL)ุ ููููู ุงูุนุซูุฑ ุนูููุง ูู ููุธูุฉ ๐ค [Diffusers](https://huggingface.co/diffusers) Hubุ ุฃู ููููู ุชุตูุญ ุงูููุงุฐุฌ [ุงููุฏุฑุจุฉ ูู ูุจู ุงููุฌุชูุน](https://huggingface.co/modelsุother=stable-diffusion-xl&other=controlnet) ุนูู Hub.
</Tip>

ูุญุชูู ูููุฐุฌ ControlNet ุนูู ูุฌููุนุชูู ูู ุงูุฃูุฒุงู (ุฃู ุงููุชู) ูุชุตูุฉ ุจุทุจูุฉ ุงูุชุตููุฉ ุงูุตูุฑูุฉ:

- *ูุณุฎุฉ ูุญููุฉ* ุชุญุงูุธ ุนูู ูู ูุง ุชุนููู ูููุฐุฌ ุงูุงูุชุดุงุฑ ุงููุณุจู ุงููุจูุฑ
- *ูุณุฎุฉ ูุงุจูุฉ ููุชุฏุฑูุจ* ูุชู ุชุฏุฑูุจูุง ุนูู ุฅุฏุฎุงู ุงูุดุฑุท ุงูุฅุถุงูู

ูุธุฑูุง ูุฃู ุงููุณุฎุฉ ุงููุญููุฉ ุชุญุงูุธ ุนูู ุงููููุฐุฌ ุงููุณุจู ุงูุชุฏุฑูุจุ ูุฅู ุชุฏุฑูุจ ูุชูููุฐ ControlNet ุนูู ุฅุฏุฎุงู ุดุฑุท ุฌุฏูุฏ ุณุฑูุน ูุซู ุถุจุท ูููุฐุฌ ุขุฎุฑ ูุฃูู ูุง ุชุฏุฑุจ ุงููููุฐุฌ ูู ุงูุตูุฑ.

ุณููุถุญ ูุฐุง ุงูุฏููู ููููุฉ ุงุณุชุฎุฏุงู ControlNet ููุชุญููู ูู ูุต ุฅูู ุตูุฑุฉุ ููู ุตูุฑุฉ ุฅูู ุตูุฑุฉุ ูุงูุทูุงุก ุงูุชููุงุฆูุ ูุงููุฒูุฏ! ููุงู ุงูุนุฏูุฏ ูู ุฃููุงุน ูุฏุฎูุงุช ControlNet ููุงุฎุชูุงุฑ ูู ุจูููุงุ ูููู ูู ูุฐุง ุงูุฏูููุ ุณูุฑูุฒ ููุท ุนูู ุจุนุถ ูููุง. ูุง ุชุชุฑุฏุฏ ูู ุชุฌุฑุจุฉ ูุฏุฎูุงุช ุงูุดุฑุท ุงูุฃุฎุฑู!

ูุจู ุงูุจุฏุกุ ุชุฃูุฏ ูู ุชุซุจูุช ุงูููุชุจุงุช ุงูุชุงููุฉ:

```py
# ูู ุจุฅูุบุงุก ุงูุชุนููู ูุชุซุจูุช ุงูููุชุจุงุช ุงูุถุฑูุฑูุฉ ูู Colab
#! pip install -q diffusers transformers accelerate opencv-python
```

## ูู ูุต ุฅูู ุตูุฑุฉ

ุจุงููุณุจุฉ ููุชุญููู ูู ูุต ุฅูู ุตูุฑุฉุ ุนุงุฏุฉู ูุง ูุชู ุชูุฑูุฑ ูุทุงูุจุฉ ูุตูุฉ ุฅูู ุงููููุฐุฌ. ูููู ูุน ControlNetุ ููููู ุชุญุฏูุฏ ุฅุฏุฎุงู ุดุฑุท ุฅุถุงูู. ุฏุนููุง ูุดุชุฑุท ุงููููุฐุฌ ูุน ุตูุฑุฉ Cannyุ ููู ูุฎุทุท ุฃุจูุถ ูุตูุฑุฉ ุนูู ุฎูููุฉ ุณูุฏุงุก. ุจูุฐู ุงูุทุฑููุฉุ ูููู ูู ControlNet ุงุณุชุฎุฏุงู ุตูุฑุฉ Canny ูุนูุตุฑ ุชุญูู ูุชูุฌูู ุงููููุฐุฌ ูุฅูุดุงุก ุตูุฑุฉ ุจููุณ ุงููุฎุทุท.

ูู ุจุชุญููู ุตูุฑุฉ ูุงุณุชุฎุฏู ููุชุจุฉ [opencv-python](https://github.com/opencv/opencv-python) ูุงุณุชุฎุฑุงุฌ ุตูุฑุฉ Canny:

```py
from diffusers.utils import load_image, make_image_grid
from PIL import Image
import cv2
import numpy as np

original_image = load_image(
"https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png"
)

image = np.array(original_image)

low_threshold = 100
high_threshold = 200

image = cv2.Canny(image, low_threshold, high_threshold)
image = image[:, :, None]
image = np.concatenate([image, image, image], axis=2)
canny_image = Image.fromarray(image)
```

<div class="flex gap-4">
<div>
<img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png"/>
<figcaption class="mt-2 text-center text-sm text-gray-500">ุงูุตูุฑุฉ ุงูุฃุตููุฉ</figcaption>
</div>
<div>
<img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/vermeer_canny_edged.png"/>
<figcaption class="mt-2 text-center text-sm text-gray-500">ุตูุฑุฉ Canny</figcaption>
</div>
</div>

ุจุนุฏ ุฐููุ ูู ุจุชุญููู ูููุฐุฌ ControlNet ุงููุดุฑูุท ุนูู ุงูุชุดุงู ุญุงูุฉ Canny ููุฑุฑู ุฅูู [`StableDiffusionControlNetPipeline`]. ุงุณุชุฎุฏู [`UniPCMultistepScheduler`] ุงูุฃุณุฑุน ููู ุจุชูููู ููู ุงููููุฐุฌ ุฅูู ูุญุฏุฉ ุงููุนุงูุฌุฉ ุงููุฑูุฒูุฉ ููุชุณุฑูุน ุงูุงุณุชุฏูุงู ูุชูููู ุงุณุชุฎุฏุงู ุงูุฐุงูุฑุฉ.

```py
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler
import torch

controlnet = ControlNetModel.from_pretrained("lllyasviel/sd-controlnet-canny", torch_dtype=torch.float16, use_safetensors=True)
pipe = StableDiffusionControlNetPipeline.from_pretrained(
"runwayml/stable-diffusion-v1-5"ุ controlnet=controlnetุ torch_dtype=torch.float16ุ use_safetensors=True
)

pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
pipe.enable_model_cpu_offload()
```

ุงูุขู ูู ุจุชูุฑูุฑ ูุทุงูุจุชู ูุตูุฑุฉ Canny ุฅูู ุงูุฃูุจูุจ:

```py
output = pipe(
"the mona lisa"ุ image=canny_image
).images[0]
make_image_grid([original_image, canny_image, output], rows=1, cols=3)
```

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet-text2img.png"/>
</div>

## ูู ุตูุฑุฉ ุฅูู ุตูุฑุฉ

ุจุงููุณุจุฉ ููุชุญููู ูู ุตูุฑุฉ ุฅูู ุตูุฑุฉุ ุนุงุฏุฉู ูุง ูุชู ุชูุฑูุฑ ุตูุฑุฉ ุฃูููุฉ ูุทูุจ ุฅูู ุงูุฃูุจูุจ ูุฅูุดุงุก ุตูุฑุฉ ุฌุฏูุฏุฉ. ูุน ControlNetุ ููููู ุชูุฑูุฑ ุฅุฏุฎุงู ุดุฑุท ุฅุถุงูู ูุชูุฌูู ุงููููุฐุฌ. ุฏุนููุง ูุดุชุฑุท ุงููููุฐุฌ ุจุงุณุชุฎุฏุงู ุฎุฑูุทุฉ ุงูุนููุ ููู ุตูุฑุฉ ุชุญุชูู ุนูู ูุนูููุงุช ููุงููุฉ. ุจูุฐู ุงูุทุฑููุฉุ ูููู ูู ControlNet ุงุณุชุฎุฏุงู ุฎุฑูุทุฉ ุงูุนูู ูุนูุตุฑ ุชุญูู ูุชูุฌูู ุงููููุฐุฌ ูุฅูุดุงุก ุตูุฑุฉ ุชุญุงูุธ ุนูู ุงููุนูููุงุช ุงูููุงููุฉ.

ุณุชุณุชุฎุฏู [`StableDiffusionControlNetImg2ImgPipeline`] ููุฐู ุงููููุฉุ ูุงูุชู ุชุฎุชูู ุนู [`StableDiffusionControlNetPipeline`] ูุฃููุง ุชุณูุญ ูู ุจุชูุฑูุฑ ุตูุฑุฉ ุฃูููุฉ ูููุทุฉ ุจุฏุงูุฉ ูุนูููุฉ ุฅูุดุงุก ุงูุตูุฑุฉ.

ูู ุจุชุญููู ุตูุฑุฉ ูุงุณุชุฎุฏู ุฎุท ุฃูุงุจูุจ `depth-estimation` [`~transformers.Pipeline`] ูู ๐ค Transformers ูุงุณุชุฎุฑุงุฌ ุฎุฑูุทุฉ ุนูู ุงูุตูุฑุฉ:

```py
import torch
import numpy as np

from transformers import pipeline
from diffusers.utils import load_image, make_image_grid

image = load_image(
"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet-img2img.jpg"
)

def get_depth_map(image, depth_estimator):
image = depth_estimator(image)["depth"]
image = np.array(image)
image = image[:, :, None]
image = np.concatenate([image, image, image], axis=2)
detected_map = torch.from_numpy(image).float() / 255.0
depth_map = detected_map.permute(2, 0, 1)
return depth_map

depth_estimator = pipeline("depth-estimation")
depth_map = get_depth_map(image, depth_estimator).unsqueeze(0).half().to("cuda")
```

ุจุนุฏ ุฐููุ ูู ุจุชุญููู ูููุฐุฌ ControlNet ุงููุดุฑูุท ุนูู ุฎุฑุงุฆุท ุงูุนูู ููุฑุฑู ุฅูู [`StableDiffusionControlNetImg2ImgPipeline`]. ุงุณุชุฎุฏู [`UniPCMultistepScheduler`] ุงูุฃุณุฑุน ููู ุจุชูููู ููู ุงููููุฐุฌ ุฅูู ูุญุฏุฉ ุงููุนุงูุฌุฉ ุงููุฑูุฒูุฉ ููุชุณุฑูุน ุงูุงุณุชุฏูุงู ูุชูููู ุงุณุชุฎุฏุงู ุงูุฐุงูุฑุฉ.

```py
from diffusers import StableDiffusionControlNetImg2ImgPipeline, ControlNetModel, UniPCMultistepScheduler
import torch

controlnet = ControlNetModel.from_pretrained("lllyasviel/control_v11f1p_sd15_depth"ุ torch_dtype=torch.float16ุ use_safetensors=True)
pipe = StableDiffusionControlNetImg2ImgPipeline.from_pretrained(
"runwayml/stable-diffusion-v1-5"ุ controlnet=controlnetุ torch_dtype=torch.float16ุ use_safetensors=True
)

pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
pipe.enable_model_cpu_offload()
```

ุงูุขู ูู ุจุชูุฑูุฑ ูุทุงูุจุชูุ ูุงูุตูุฑุฉ ุงูุฃูููุฉุ ูุฎุฑูุทุฉ ุงูุนูู ุฅูู ุงูุฃูุจูุจ:

```py
output = pipe(
"lego batman and robin"ุ image=image, control_image=depth_map,
).images[0]
make_image_grid([image, output], rows=1, cols=2)
```

<div class="flex gap-4">
<div>
<img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet-img2img.jpg"/>
<figcaption class="mt-2 text-center text-sm text-gray-500">ุงูุตูุฑุฉ ุงูุฃุตููุฉ</figcaption>
</div>
<div>
<img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet-img2img-2.png"/>
<figcaption class="mt-2 text-center text-sm text-gray-500">ุงูุตูุฑุฉ ุงููููุฏุฉ</figcaption>
</div>
</div>
## Inpainting

ุจุงููุณุจุฉ ูุชูููุฉ Inpaintingุ ูุฃูุช ุจุญุงุฌุฉ ุฅูู ุตูุฑุฉ ุฃูููุฉุ ูุตูุฑุฉ ููุงุนุ ููุตู ูุญุฏุฏ ูุง ูุฌุจ ุงุณุชุจุฏุงู ุงูููุงุน ุจู. ุชุณูุญ ููุงุฐุฌ ControlNet ุจุฅุถุงูุฉ ุตูุฑุฉ ุชุญูู ุฃุฎุฑู ูุชููุฆุฉ ุงููููุฐุฌ. ุฏุนูุง ููู ุจุชููุฆุฉ ุงููููุฐุฌ ุจุงุณุชุฎุฏุงู ููุงุน Inpainting. ุจูุฐู ุงูุทุฑููุฉุ ูููู ูู ControlNet ุงุณุชุฎุฏุงู ููุงุน Inpainting ููุณููุฉ ุชุญูู ูุชูุฌูู ุงููููุฐุฌ ูุชูููุฏ ุตูุฑุฉ ุฏุงุฎู ููุทูุฉ ุงูููุงุน.

ูู ุจุชุญููู ุตูุฑุฉ ุฃูููุฉ ูุตูุฑุฉ ููุงุน:

```py
from diffusers.utils import load_image, make_image_grid

init_image = load_image(
"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet-inpaint.jpg"
)
init_image = init_image.resize((512, 512))

mask_image = load_image(
"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet-inpaint-mask.jpg"
)
mask_image = mask_image.resize((512, 512))
make_image_grid([init_image, mask_image], rows=1, cols=2)
```

ูู ุจุฅูุดุงุก ุฏุงูุฉ ูุฅุนุฏุงุฏ ุตูุฑุฉ ุงูุชุญูู ูู ุงูุตูุฑุฉ ุงูุฃูููุฉ ูุตูุฑุฉ ุงูููุงุน. ุณูุคุฏู ูุฐุง ุฅูู ุฅูุดุงุก ูุตูููุฉ ูุชุญุฏูุฏ ุงูุจูุณูุงุช ูู `init_image` ูุจูุณูุงุช ูููุนุฉ ุฅุฐุง ูุงู ุงูุจูุณู ุงูููุงุจู ูู `mask_image` ุฃุนูู ูู ุนุชุจุฉ ูุนููุฉ.

```py
import numpy as np
import torch

def make_inpaint_condition(image, image_mask):
    image = np.array(image.convert("RGB")).astype(np.float32) / 255.0
    image_mask = np.array(image_mask.convert("L")).astype(np.float32) / 255.0

    assert image.shape[0:1] == image_mask.shape[0:1]
    image[image_mask > 0.5] = -1.0 # set as masked pixel
    image = np.expand_dims(image, 0).transpose(0, 3, 1, 2)
    image = torch.from_numpy(image)
    return image

control_image = make_inpaint_condition(init_image, mask_image)
```

<div class="flex gap-4">
<div>
<img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet-inpaint.jpg"/>
<figcaption class="mt-2 text-center text-sm text-gray-500">ุงูุตูุฑุฉ ุงูุฃุตููุฉ</figcaption>
</div>
<div>
<img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet-inpaint-mask.jpg"/>
<figcaption class="mt-2 text-center text-sm text-gray-500">ุตูุฑุฉ ุงูููุงุน</figcaption>
</div>
</div>

ูู ุจุชุญููู ูููุฐุฌ ControlNet ุงููุดุฑูุท ุจู Inpainting ููุฑุฑู ุฅูู [`StableDiffusionControlNetInpaintPipeline`]. ุงุณุชุฎุฏู [`UniPCMultistepScheduler`] ุงูุฃุณุฑุน ููู ุจุชูููู ุชูุฑูุบ ุงููููุฐุฌ ูุชุณุฑูุน ุงูุงุณุชูุชุงุฌ ูุชูููู ุงุณุชุฎุฏุงู ุงูุฐุงูุฑุฉ.

```py
from diffusers import StableDiffusionControlNetInpaintPipeline, ControlNetModel, UniPCMultistepScheduler

controlnet = ControlNetModel.from_pretrained("lllyasviel/control_v11p_sd15_inpaint", torch_dtype=torch.float16, use_safetensors=True)
pipe = StableDiffusionControlNetInpaintPipeline.from_pretrained(
"runwayml/stable-diffusion-v1-5", controlnet=controlnet, torch_dtype=torch.float16, use_safetensors=True
)

pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
pipe.enable_model_cpu_offload()
```

ุงูุขู ูู ุจุชูุฑูุฑ ูุตููุ ูุงูุตูุฑุฉ ุงูุฃูููุฉุ ูุตูุฑุฉ ุงูููุงุนุ ูุตูุฑุฉ ุงูุชุญูู ุฅูู ุฎุท ุงูุฃูุงุจูุจ:

```py
output = pipe(
"corgi face with large ears, detailed, pixar, animated, disney",
num_inference_steps=20,
eta=1.0,
image=init_image,
mask_image=mask_image,
control_image=control_image,
).images[0]
make_image_grid([init_image, mask_image, output], rows=1, cols=3)
```

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet-inpaint-result.png"/>
</div>

## ูุถุน ุงูุชุฎููู

[ูุถุน ุงูุชุฎููู](https://github.com/lllyasviel/ControlNet/discussions/188) ูุง ูุชุทูุจ ุชูููุฑ ูุตู ูุดุจูุฉ ุงูุชุญูู ุนูู ุงูุฅุทูุงู! ููุฐุง ูุฌุจุฑ ูุดูุฑ ControlNet ุนูู ุจุฐู ูุตุงุฑู ุฌูุฏู ูู "ุชุฎููู" ูุญุชููุงุช ุฎุฑูุทุฉ ุงูุชุญูู ุงููุฏุฎูุฉ (ุฎุฑูุทุฉ ุงูุนููุ ุชูุฏูุฑ ุงููุถุนุ Canny edgeุ ุฅูุฎ).

ูููู ูุถุน ุงูุชุฎููู ุจุชุนุฏูู ูููุงุณ ุงููุฎููุงุช ุงููุงุชุฌุฉ ุนู ControlNet ููููุง ููุณุจุฉ ุซุงุจุชุฉ ุชุนุชูุฏ ุนูู ุนูู ุงููุชูุฉ. ููุงุจู ุงููุชูุฉ ุงูุฃุนูู `DownBlock` 0.1ุ ููุน ุฒูุงุฏุฉ ุนูู ุงููุชูุ ูุฒูุฏ ุงููููุงุณ ุจุดูู ุฃุณูู ุจุญูุซ ูุตุจุญ ูููุงุณ ุฅุฎุฑุงุฌ `MidBlock` 1.0.

<Tip>
ูุง ูุคุซุฑ ูุถุน ุงูุชุฎููู ุนูู ุชููุฆุฉ ุงููุตู ูููููู ูุง ุชุฒุงู ุชูููุฑ ูุตู ุฅุฐุง ุฃุฑุฏุช ุฐูู.
</Tip>

ูู ุจุชุนููู `guess_mode=True` ูู ุฎุท ุงูุฃูุงุจูุจุ ููู [ุงููุณุชุญุณู](https://github.com/lllyasviel/ControlNet#guess-mode--non-prompt-mode) ุชุนููู ูููุฉ `guidance_scale` ุจูู 3.0 ู5.0.

```py
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel
from diffusers.utils import load_image, make_image_grid
import numpy as np
import torch
from PIL import Image
import cv2

controlnet = ControlNetModel.from_pretrained("lllyasviel/sd-controlnet-canny", use_safetensors=True)
pipe = StableDiffusionControlNetPipeline.from_pretrained("runwayml/stable-diffusion-v1-5", controlnet=controlnet, use_safetensors=True).to("cuda")

original_image = load_image("https://huggingface.co/takuma104/controlnet_dev/resolve/main/bird_512x512.png")

image = np.array(original_image)

low_threshold = 100
high_threshold = 200

image = cv2.Canny(image, low_threshold, high_threshold)
image = image[:, :, None]
image = np.concatenate([image, image, image], axis=2)
canny_image = Image.fromarray(image)

image = pipe("", image=canny_image, guess_mode=True, guidance_scale=3.0).images[0]
make_image_grid([original_image, canny_image, image], rows=1, cols=3)
```

<div class="flex gap-4">
<div>
<img class="rounded-xl" src="https://huggingface.co/takuma104/controlnet_dev/resolve/main/gen_compare_guess_mode/output_images/diffusers/output_bird_canny_0.png"/>
<figcaption class="mt-2 text-center text-sm text-gray-500">ุงููุถุน ุงูุนุงุฏู ูุน ุงููุตู</figcaption>
</div>
<div>
<img class="rounded-xl" src="https://huggingface.co/takuma104/controlnet_dev/resolve/main/gen_compare_guess_mode/output_images/diffusers/output_bird_canny_0_gm.png"/>
<figcaption class="mt-2 text-center text-sm text-gray-500">ูุถุน ุงูุชุฎููู ุจุฏูู ูุตู</figcaption>
</div>
</div>
## ControlNet with Stable Diffusion XL

ูู ุงูููุช ุงูุญุงููุ ูุง ููุฌุฏ ุงููุซูุฑ ูู ููุงุฐุฌ ControlNet ุงููุชูุงููุฉ ูุน Stable Diffusion XL (SDXL)ุ ูููููุง ูููุง ุจุชุฏุฑูุจ ูููุฐุฌูู ูุงูููู ูู ููุงุฐุฌ ControlNet ุงููุชูุงููุฉ ูุน SDXL ุงููุดุฑูุทุฉ ุนูู ูุดู ุญูุงู ูุงูู (Canny edge detection) ูุฎุฑุงุฆุท ุงูุนูู (depth maps). ููุง ูุฌุฑู ุชุฌุงุฑุจ ูุฅูุดุงุก ุฅุตุฏุงุฑุงุช ุฃุตุบุฑ ูู ููุงุฐุฌ ControlNet ุงููุชูุงููุฉ ูุน SDXL ูุชุณููู ุชุดุบูููุง ุนูู ุงูุฃุฌูุฒุฉ ุงููุญุฏูุฏุฉ ุงูููุงุฑุฏ. ููููู ุงูุนุซูุฑ ุนูู ูุฐู ุงูููุงุท ุงููุฑุฌุนูุฉ ุนูู [ููุธูุฉ ๐ค Diffusers Hub](https://huggingface.co/diffusers)!

ููุณุชุฎุฏู ูููุฐุฌ SDXL ControlNet ุงููุดุฑูุท ุนูู ุตูุฑ ูุงูู ูุชูููุฏ ุตูุฑุฉ. ุงุจุฏุฃ ุจุชุญููู ุตูุฑุฉ ูุฅุนุฏุงุฏ ุตูุฑุฉ ูุงูู:

```py
from diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel, AutoencoderKL
from diffusers.utils import load_image, make_image_grid
from PIL import Image
import cv2
import numpy as np
import torch

original_image = load_image(
"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd_controlnet/hf-logo.png"
)

image = np.array(original_image)

low_threshold = 100
high_threshold = 200

image = cv2.Canny(image, low_threshold, high_threshold)
image = image[:, :, None]
image = np.concatenate([image, image, image], axis=2)
canny_image = Image.fromarray(image)
make_image_grid([original_image, canny_image], rows=1, cols=2)
```

<div class="flex gap-4">
<div>
<img class="rounded-xl" src="https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd_controlnet/hf-logo.png"/>
<figcaption class="mt-2 text-center text-sm text-gray-500">ุงูุตูุฑุฉ ุงูุฃุตููุฉ</figcaption>
</div>
<div>
<img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/hf-logo-canny.png"/>
<figcaption class="mt-2 text-center text-sm text-gray-500">ุตูุฑุฉ ูุงูู</figcaption>
</div>
</div>

ูู ุจุชุญููู ูููุฐุฌ SDXL ControlNet ุงููุดุฑูุท ุนูู ูุดู ุญูุงู ูุงูู ููุฑุฑู ุฅูู [`StableDiffusionXLControlNetPipeline`]. ููููู ุฃูุถูุง ุชูููู ููู ุงููููุฐุฌ ุฅูู ูุญุฏุฉ ุงููุนุงูุฌุฉ ุงููุฑูุฒูุฉ (CPU) ูุชูููู ุงุณุชุฎุฏุงู ุงูุฐุงูุฑุฉ.

```py
controlnet = ControlNetModel.from_pretrained(
"diffusers/controlnet-canny-sdxl-1.0",
torch_dtype=torch.float16,
use_safetensors=True
)
vae = AutoencoderKL.from_pretrained("madebyollin/sdxl-vae-fp16-fix", torch_dtype=torch.float16, use_safetensors=True)
pipe = StableDiffusionXLControlNetPipeline.from_pretrained(
"stabilityai/stable-diffusion-xl-base-1.0",
controlnet=controlnet,
vae=vae,
torch_dtype=torch.float16,
use_safetensors=True
)
pipe.enable_model_cpu_offload()
```

ุงูุขูุ ูู ุจุชูุฑูุฑ ุงููุญุซ (prompt) (ูุงููุญุซ ุงูุณูุจู ุฅู ููุช ุชุณุชุฎุฏูู) ูุตูุฑุฉ ูุงูู ุฅูู ุฎุท ุงูุฃูุงุจูุจ (pipeline):

<Tip>

ูุญุฏุฏ ูุนุงูู [`controlnet_conditioning_scale`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/controlnet#diffusers.StableDiffusionControlNetPipeline.__call__.controlnet_conditioning_scale) ููุฏุงุฑ ุงููุฒู ุงููุฎุตุต ููุฏุฎูุงุช ุงูุชูููู. ุงููููุฉ ุงูููุตู ุจูุง ูู 0.5 ูุชุญููู ุชุนููู ุฌูุฏุ ูููู ููููู ุชุฌุฑุจุฉ ุฃุฑูุงู ุฃุฎุฑู!

</Tip>

```py
prompt = "aerial view, a futuristic research complex in a bright foggy jungle, hard lighting"
negative_prompt = 'low quality, bad quality, sketches'

image = pipe(
prompt,
negative_prompt=negative_prompt,
image=canny_image,
controlnet_conditioning_scale=0.5,
).images[0]
make_image_grid([original_image, canny_image, image], rows=1, cols=3)
```

<div class="flex justify-center">
<img class="rounded-xl" src="https://huggingface.co/diffusers/controlnet-canny-sdxl-1.0/resolve/main/out_hug_lab_7.png"/>
</div>

ููููู ุฃูุถูุง ุงุณุชุฎุฏุงู [`StableDiffusionXLControlNetPipeline`] ูู ูุถุน ุงูุชุฎููู ุนู ุทุฑูู ุชุนููู ุงููุนุงูู ุฅูู `True`:

```py
from diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel, AutoencoderKL
from diffusers.utils import load_image, make_image_grid
import numpy as np
import torch
import cv2
from PIL import Image

prompt = "aerial view, a futuristic research complex in a bright foggy jungle, hard lighting"
negative_prompt = "low quality, bad quality, sketches"

original_image = load_image(
"https://hf.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd_controlnet/hf-logo.png"
)

controlnet = ControlNetModel.from_pretrained(
"diffusers/controlnet-canny-sdxl-1.0", torch_dtype=torch.float16, use_safetensors=True
)
vae = AutoencoderKL.from_pretrained("madebyollin/sdxl-vae-fp16-fix", torch_dtype=torch.float16, use_safetensors=True)
pipe = StableDiffusionXLControlNetPipeline.from_pretrained(
"stabilityai/stable-diffusion-xl-base-1.0", controlnet=controlnet, vae=vae, torch_dtype=torch.float16, use_safetensors=True
)
pipe.enable_model_cpu_offload()

image = np.array(original_image)
image = cv2.Canny(image, 100, 200)
image = image[:, :, None]
image = np.concatenate([image, image, image], axis=2)
canny_image = Image.fromarray(image)

image = pipe(
prompt, negative_prompt=negative_prompt, controlnet_conditioning_scale=0.5, image=canny_image, guess_mode=True,
).images[0]
make_image_grid([original_image, canny_image, image], rows=1, cols=3)
```

<Tip>

ููููู ุงุณุชุฎุฏุงู ูููุฐุฌ ุชุญุณูู (refiner model) ูุน `StableDiffusionXLControlNetPipeline` ูุชุญุณูู ุฌูุฏุฉ ุงูุตูุฑุฉุ ุชูุงููุง ููุง ุชูุนู ูุน `StableDiffusionXLPipeline` ุงูุนุงุฏู.

ุฑุงุฌุน ูุณู [ุชุญุณูู ุฌูุฏุฉ ุงูุตูุฑุฉ](./sdxl#refine-image-quality) ููุนุฑูุฉ ููููุฉ ุงุณุชุฎุฏุงู ูููุฐุฌ ุงูุชุญุณูู.

ุชุฃูุฏ ูู ุงุณุชุฎุฏุงู `StableDiffusionXLControlNetPipeline` ููุฑุฑ `image` ู`controlnet_conditioning_scale`.

```py
base = StableDiffusionXLControlNetPipeline(...)
image = base(
prompt=prompt,
controlnet_conditioning_scale=0.5,
image=canny_image,
num_inference_steps=40,
denoising_end=0.8,
output_type="latent",
).images
# ุงูุจุงูู ููุง ูู ุชูุงููุง ูุน StableDiffusionXLPipeline
```

</Tip>
## MultiControlNet 

ููููู ุชูููู ุงูุนุฏูุฏ ูู ุนูููุงุช ุถุจุท ControlNet ูู ูุฏุฎูุงุช ุงูุตูุฑ ุงููุฎุชููุฉ ูุฅูุดุงุก *MultiControlNet*. ูููุญุตูู ุนูู ูุชุงุฆุฌ ุฃูุถูุ ูู ุงููููุฏ ุบุงูุจูุง:

1. ููุงุน ุงูุถุจุท ุจุญูุซ ูุง ุชุชุฏุงุฎู (ุนูู ุณุจูู ุงููุซุงูุ ููุงุน ููุทูุฉ ุตูุฑุฉ Canny ุญูุซ ููุน ุถุจุท ุงููุถุน)
2. ุชุฌุฑุจุฉ ูุน ูุนููุฉ [`controlnet_conditioning_scale`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/controlnet#diffusers.StableDiffusionControlNetPipeline.__call__.controlnet_conditioning_scale) ูุชุญุฏูุฏ ููุฏุงุฑ ุงููุฒู ุงูุฐู ูุฌุจ ุชุนูููู ููู ุฅุฏุฎุงู ุถุจุท

ูู ูุฐุง ุงููุซุงูุ ุณุชุฌูุน ุจูู ุตูุฑุฉ Canny ูุตูุฑุฉ ุชูุฏูุฑ ูุถุน ุงูุฅูุณุงู ูุฅูุดุงุก ุตูุฑุฉ ุฌุฏูุฏุฉ.

ูู ุจุฅุนุฏุงุฏ ุถุจุท ุตูุฑุฉ Canny:

```py
from diffusers.utils import load_image, make_image_grid
from PIL import Image
import numpy as np
import cv2

original_image = load_image(
"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/landscape.png"
)
image = np.array(original_image)

low_threshold = 100
high_threshold = 200

image = cv2.Canny(image, low_threshold, high_threshold)

# zero out middle columns of image where pose will be overlaid
zero_start = image.shape[1] // 4
zero_end = zero_start + image.shape[1] // 2
image[:, zero_start:zero_end] = 0

image = image[:, :, None]
image = np.concatenate([image, image, image], axis=2)
canny_image = Image.fromarray(image)
make_image_grid([original_image, canny_image], rows=1, cols=2)
```

<div class="flex gap-4">
<div>
<img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/landscape.png"/>
<figcaption class="mt-2 text-center text-sm text-gray-500">ุงูุตูุฑุฉ ุงูุฃุตููุฉ</figcaption>
</div>
<div>
<img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/controlnet/landscape_canny_masked.png"/>
<figcaption class="mt-ompi text-center text-sm text-gray-500">ุตูุฑุฉ Canny</figcaption>
</div>
</div>

ุจุงููุณุจุฉ ูุชูุฏูุฑ ุงููุถุน ุงูุจุดุฑูุ ูู ุจุชุซุจูุช [controlnet_aux](https://github.com/patrickvonplaten/controlnet_aux):

```py
# uncomment to install the necessary library in Colab
#! pip install -q controlnet-aux
```

ูู ุจุฅุนุฏุงุฏ ุถุจุท ุชูุฏูุฑ ุงููุถุน ุงูุจุดุฑู:

```py
from controlnet_aux import OpenposeDetector

openpose = OpenposeDetector.from_pretrained("lllyasviel/ControlNet")
original_image = load_image(
"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/person.png"
)
openpose_image = openpose(original_image)
make_image_grid([original_image, openpose_image], rows=1, cols=2)
```

<div class="flex gap-4">
<div>
<img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/person.png"/>
<figcaption class="mt-2 text-center text-sm text-gray-500">ุงูุตูุฑุฉ ุงูุฃุตููุฉ</figcaption>
</div>
<div>
<img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/controlnet/person_pose.png"/>
<figcaption class="mt-2 text-center text-sm text-gray-500">ุตูุฑุฉ ุงููุถุน ุงูุจุดุฑู</figcaption>
</div>
</div>

ูู ุจุชุญููู ูุงุฆูุฉ ููุงุฐุฌ ControlNet ุงูุชู ุชุชูุงูู ูุน ูู ุถุจุทุ ููุฑุฑูุง ุฅูู [`StableDiffusionXLControlNetPipeline`]. ุงุณุชุฎุฏู [`UniPCMultistepScheduler`] ุงูุฃุณุฑุน ููู ุจุชูููู ุชูุฑูุบ ุงููููุฐุฌ ูุชูููู ุงุณุชุฎุฏุงู ุงูุฐุงูุฑุฉ.

```py
from diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel, AutoencoderKL, UniPCMultistepScheduler
import torch

controlnets = [
ControlNetModel.from_pretrained(
"thibaud/controlnet-openpose-sdxl-1.0", torch_dtype=torch.float16
),
ControlNetModel.from_pretrained(
"diffusers/controlnet-canny-sdxl-1.0", torch_dtype=torch.float16, use_safetensors=True
),
]

vae = AutoencoderKL.from_pretrained("madebyollin/sdxl-vae-fp16-fix", torch_dtype=torch.float16, use_safetensors=True)
pipe = StableDiffusionXLControlNetPipeline.from_pretrained(
"stabilityai/stable-diffusion-xl-base-1.0", controlnet=controlnets, vae=vae, torch_dtype=torch.float16, use_safetensors=True
)
pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
pipe.enable_model_cpu_offload()
```

ุงูุขู ููููู ุชูุฑูุฑ ูุทุงูุจุชู (ูุทุงูุจุฉ ุณูุจูุฉ ุฅุฐุง ููุช ุชุณุชุฎุฏู ูุงุญุฏุฉ)ุ ูุตูุฑุฉ Cannyุ ูุตูุฑุฉ ุงููุถุน ุฅูู ุงูุฃูุจูุจ:

```py
prompt = "a giant standing in a fantasy landscape, best quality"
negative_prompt = "monochrome, lowres, bad anatomy, worst quality, low quality"

generator = torch.manual_seed(1)

images = [openpose_image.resize((1024, 1024)), canny_image.resize((1024, 1024))]

images = pipe(
prompt,
image=images,
num_inference_steps=25,
generator=generator,
negative_prompt=negative_prompt,
num_images_per_prompt=3,
controlnet_conditioning_scale=[1.0, 0.8],
).images
make_image_grid([original_image, canny_image, openpose_image,
images[0].resize((512, 512)), images[1].resize((512, 512)), images[2].resize((512, 512))], rows=2, cols=3)
```

<div class="flex justify-center">
<img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/multicontrolnet.png"/>
</div>